{
  "meta": {
    "schemaVersion": 1,
    "generatedAt": "2026-02-21T08:49:03.431Z",
    "sources": [
      {
        "id": "openai_news",
        "name": "OpenAI — News",
        "type": "rss",
        "url": "https://openai.com/news/rss.xml"
      },
      {
        "id": "deepmind_blog",
        "name": "Google DeepMind — Blog",
        "type": "rss",
        "url": "https://deepmind.google/blog/feed/basic"
      },
      {
        "id": "msr_blog",
        "name": "Microsoft Research — Blog",
        "type": "rss",
        "url": "https://www.microsoft.com/en-us/research/blog/feed/"
      },
      {
        "id": "hf_blog",
        "name": "Hugging Face — Blog",
        "type": "rss",
        "url": "https://huggingface.co/blog/feed.xml"
      },
      {
        "id": "nvidia_dev_blog",
        "name": "NVIDIA Developer — Blog",
        "type": "rss",
        "url": "https://developer.nvidia.com/blog/feed/"
      },
      {
        "id": "arxiv_cs_ai",
        "name": "arXiv — cs.AI",
        "type": "rss",
        "url": "https://rss.arxiv.org/rss/cs.AI"
      },
      {
        "id": "arxiv_cs_cl",
        "name": "arXiv — cs.CL",
        "type": "rss",
        "url": "https://rss.arxiv.org/rss/cs.CL"
      }
    ]
  },
  "items": [
    {
      "id": "ce66dc286d3287b232af4bd369f45bf83ed8520de4c35fdaa08a5b346a2a4c37",
      "title": "Our First Proof submissions",
      "url": "https://openai.com/index/first-proof-submissions",
      "sourceId": "openai_news",
      "sourceName": "OpenAI — News",
      "publishedAt": "2026-02-20T14:30:00.000Z",
      "summary": "We share our AI model’s proof attempts for the First Proof math challenge, testing research-grade reasoning on expert-level problems.",
      "tags": [
        "primary",
        "release",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://openai.com/index/first-proof-submissions"
        },
        {
          "label": "Feed",
          "url": "https://openai.com/news/rss.xml"
        }
      ]
    },
    {
      "id": "217e73110b478566655f2f5881ed89d1f9a9217e699aa28e60fd68b321196e21",
      "title": "AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment",
      "url": "https://arxiv.org/abs/2602.16714",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16714v1 Announce Type: new Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16714"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f96c898fec181a8a63109dc06dd6affedfc2376baf342b25fe404ba7886a61b6",
      "title": "Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems",
      "url": "https://arxiv.org/abs/2602.16715",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16715v1 Announce Type: new Abstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16715"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5ff11bf9d72afc4123913303cd0a038adea31a7a1a9a05d5342d7dbc601287dc",
      "title": "Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence",
      "url": "https://arxiv.org/abs/2602.16716",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16716v1 Announce Type: new Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16716"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "37469d77e061f36d275dd9e72b10d1f9e6d02ee1d98c4f058bc469dbc3a657ed",
      "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation",
      "url": "https://arxiv.org/abs/2602.16727",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16727v1 Announce Type: new Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16727"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f5af5f1b5bc622c92540c9bbe0020c57e12bfd5ffcb55a717b4013897ddd8bf6",
      "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation",
      "url": "https://arxiv.org/abs/2602.16763",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16763v1 Announce Type: new Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16763"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "20ab97bd5269b0a352fe0251ea766acd8022e1bb0c7485168468e09e4e3858c2",
      "title": "Simple Baselines are Competitive with Code Evolution",
      "url": "https://arxiv.org/abs/2602.16805",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16805v1 Announce Type: new Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "compute",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16805"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8db71bd97af7c1bbca8911cc7370855fc808203390a7ede277b66924012d3c5e",
      "title": "Improved Upper Bounds for Slicing the Hypercube",
      "url": "https://arxiv.org/abs/2602.16807",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16807v1 Announce Type: new Abstract: A collection of hyperplanes $\\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\\{-1,1\\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \\leq \\lceil \\frac{4n}{5} \\rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \\leq \\frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \\leq \\lceil\\frac{5n}{6} \\rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.",
      "tags": [
        "papers",
        "reasoning"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16807"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d32cbdd3781fcd6064732056c9612cff88f5e8ee676375601b75077c985ac11a",
      "title": "NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography",
      "url": "https://arxiv.org/abs/2602.16812",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16812v1 Announce Type: new Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16812"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "af4e86834f57f088354695e6fdd1e3803eb5741934e845d0b2b76cac3c741d52",
      "title": "Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI",
      "url": "https://arxiv.org/abs/2602.16814",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16814v1 Announce Type: new Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective",
      "tags": [
        "papers",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16814"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5752a2f1babae325e7bbe5f86c557ced26a797184e8c341c4cdd6d8894dc4d20",
      "title": "An order-oriented approach to scoring hesitant fuzzy elements",
      "url": "https://arxiv.org/abs/2602.16827",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16827v1 Announce Type: new Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the G\\\"ardenfors condition. Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16827"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d4659fc1f9904b3793aea886a112836075a38cd65c58dad24f791cfdb7dd56af",
      "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages",
      "url": "https://arxiv.org/abs/2602.16832",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16832v1 Announce Type: new Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \\textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks. IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16832"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3444c28ac57bd87b159e2252ca72232632294e07bc173b0ac16ee29d417ebf85",
      "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents",
      "url": "https://arxiv.org/abs/2602.16855",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16855v1 Announce Type: new Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16855"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8fd4f03ee07f14ba3de2e1f49c76a06a7513eeb76e7b220ac97adac9eecaacf1",
      "title": "OpenSage: Self-programming Agent Generation Engine",
      "url": "https://arxiv.org/abs/2602.16891",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16891v1 Announce Type: new Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16891"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f4f59e978697be30b2327a589078ba1752ec4e1eaa206464ef3ac279ed0e2a0c",
      "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
      "url": "https://arxiv.org/abs/2602.16901",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16901v1 Announce Type: new Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.",
      "tags": [
        "papers",
        "agents",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16901"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "765d052b4f60c1f1c4bbff3b1e495cee7ce473628e8f9488ed9ed16d921785d2",
      "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
      "url": "https://arxiv.org/abs/2602.16902",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16902v1 Announce Type: new Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16902"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3837845688c85d879c10203cd7f69d57644d47d1899e72fa1e07a63f6da10840",
      "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
      "url": "https://arxiv.org/abs/2602.16931",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16931v1 Announce Type: new Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \\pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \\pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16931"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "fb6fc8ccf7f3ce9fb79760ae7a1a966a5d4d6c6441524608eed3cc8324f88f85",
      "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
      "url": "https://arxiv.org/abs/2602.16935",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16935v1 Announce Type: new Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "compute",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16935"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6dcce69c1958531c86e512d3679308ad3d576e6332a2776642474764f451f2a4",
      "title": "SourceBench: Can AI Answers Reference Quality Web Sources?",
      "url": "https://arxiv.org/abs/2602.16942",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16942v1 Announce Type: new Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16942"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "239539774ec3e43ddf2ebe496baad910971898dd069fc66b623f893b735f6fc6",
      "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
      "url": "https://arxiv.org/abs/2602.16943",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16943v1 Announce Type: new Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "safety",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16943"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "623eeff97ecb11505fbb6011ed6cf2033b0ab1985b1444ecebc2d55641b6c5d4",
      "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
      "url": "https://arxiv.org/abs/2602.16953",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16953v1 Announce Type: new Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16953"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "97fe2381af0e10d2a72eb699045f44cd07127260a6ccb7f40bc6f6d02a633915",
      "title": "Automating Agent Hijacking via Structural Template Injection",
      "url": "https://arxiv.org/abs/2602.16958",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16958v1 Announce Type: new Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16958"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bd11218a300d3d9d801e93faec45073ccef6cfb0686174d9eda4bb0e32c17d7e",
      "title": "HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing",
      "url": "https://arxiv.org/abs/2602.16976",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16976v1 Announce Type: new Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed: Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16976"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4eba73cb32ff9e2cce0bf060fb32a3be01ef38ae052bc9e6be70b2179a838def",
      "title": "Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning",
      "url": "https://arxiv.org/abs/2602.16984",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16984v1 Announce Type: new Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16984"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6907a8e36a72d1107f32df5877c0b5707afaf6d51bb499cb7ed006a3e4284d81",
      "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
      "url": "https://arxiv.org/abs/2602.16990",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16990v1 Announce Type: new Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16990"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0fea6506045cde2068cf1e65fca158ad6db2579d2e8b199f86913b1f3fcf3a8b",
      "title": "Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases",
      "url": "https://arxiv.org/abs/2602.17001",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17001v1 Announce Type: new Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17001"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8572f0a1bc191d7aabb3f2f574157e9600b221b4103113bd705480f44cf8408c",
      "title": "Cinder: A fast and fair matchmaking system",
      "url": "https://arxiv.org/abs/2602.17015",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17015v1 Announce Type: new Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the \"non-outlier\" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a \"Sanction Score.\" We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17015"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "56334ce9c8630319fdfca4a8619465ac87377e89189a566aeb884f601854e2c7",
      "title": "M2F: Automated Formalization of Mathematical Literature at Scale",
      "url": "https://arxiv.org/abs/2602.17016",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17016v1 Announce Type: new Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\\%$ proof success (vs.\\ $80\\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.",
      "tags": [
        "papers",
        "agents"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17016"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "894fded98f1bb175507691b52704069a94c654b324732274baaeb45a0b316f11",
      "title": "Sales Research Agent and Sales Research Bench",
      "url": "https://arxiv.org/abs/2602.17017",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17017v1 Announce Type: new Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17017"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8b093b7b7d43c64cbccac7b93c409922e875dfc1212043b09ad34e99c84e098d",
      "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.17038",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17038v1 Announce Type: new Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \\emph{single} policy network, causing \\emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \\textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \\emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.",
      "tags": [
        "papers",
        "agents",
        "policy"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17038"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0e933c15a51803c08ee57805092eb16ef2cc550f3556deb79f710c7dd4726fcb",
      "title": "Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs",
      "url": "https://arxiv.org/abs/2602.17046",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17046v1 Announce Type: new Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17046"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7bf842a19d55653c0a12732781257481d03500be4a5cd400f66515ad9d514bd8",
      "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
      "url": "https://arxiv.org/abs/2602.17049",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17049v1 Announce Type: new Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "compute"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17049"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "77caac40a7c0fa3905676edecb1e7969698092833447d545f1da26a7d13e2dfd",
      "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.17053",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17053v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17053"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e6971160cb25597b5e974b916b1e0778633deb5b344aab615bcc666eacfb68a4",
      "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.17062",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17062v1 Announce Type: new Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "policy",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17062"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1a432cb62100dc705e2293da2bff96694c73efdfeb06f629b3890c5e782ea956",
      "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization",
      "url": "https://arxiv.org/abs/2602.17066",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17066v1 Announce Type: new Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17066"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f2e305e389e850643faa4118f880b29dd420cafb6bfbef63af3fa4dd719cc0d1",
      "title": "How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses",
      "url": "https://arxiv.org/abs/2602.17084",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17084v1 Announce Type: new Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.",
      "tags": [
        "papers",
        "agents",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17084"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "24a10df331c21c1e3d44d3474414e85ed6a71e099390e3b5c8318c5f0f576fc1",
      "title": "Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence",
      "url": "https://arxiv.org/abs/2602.17096",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17096v1 Announce Type: new Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications. Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions. Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17096"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ae184948deb5cf5f1ce3e3cc6c820b10a46d0700bbce8e1594f2f5b8275586be",
      "title": "Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction",
      "url": "https://arxiv.org/abs/2602.17106",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17106v1 Announce Type: new Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.",
      "tags": [
        "papers",
        "eval",
        "policy",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17106"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "18cb51584c8da5022e949cf56f4f803306cea0f890ed6ac0840e717b25cf52cf",
      "title": "Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)",
      "url": "https://arxiv.org/abs/2602.17107",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17107v1 Announce Type: new Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.",
      "tags": [
        "papers",
        "safety",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17107"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3c2ea3ac88fb90ababc60ebeadf466a59de9426afbdc0c79d9a63f7bfcc7ca6a",
      "title": "Instructor-Aligned Knowledge Graphs for Personalized Learning",
      "url": "https://arxiv.org/abs/2602.17111",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17111v1 Announce Type: new Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like \"Algorithms\" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., \"part-of\" or \"depends-on\" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., \"recursion\" is taught before \"mergesort\"; \"recursion\" is mentioned in the definition of \"merge sort\") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17111"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6cba8eb9cb13774797878feed140f64abf5fb13e0e344642e1d4d12993b00996",
      "title": "Epistemology of Generative AI: The Geometry of Knowing",
      "url": "https://arxiv.org/abs/2602.17116",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17116v1 Announce Type: new Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17116"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "163d19f1dd9a9ab42265583d1fd6326576b322770de09f1cc466173049fe7cf2",
      "title": "Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances",
      "url": "https://arxiv.org/abs/2602.17130",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17130v1 Announce Type: new Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.",
      "tags": [
        "papers",
        "compute"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17130"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "52a8d27bd050ac0e6f70f74471b75c543281399e282cec1461ce7c3b1c50231a",
      "title": "Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning",
      "url": "https://arxiv.org/abs/2602.17145",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17145v1 Announce Type: new Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\\%.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17145"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f885d36dbd2a0e2cf2111c948ae7df7684548cc8c660988a5019e1286e64c36f",
      "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
      "url": "https://arxiv.org/abs/2602.17162",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17162v1 Announce Type: new Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17162"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "56a076447b7874bb09b7ae4113071b0690c80d08e8eed7486b4ad826c13a2ea2",
      "title": "Texo: Formula Recognition within 20M Parameters",
      "url": "https://arxiv.org/abs/2602.17189",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17189v1 Announce Type: new Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.",
      "tags": [
        "papers",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17189"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "41dfcf27af4f9f8991cb0e468067412123de961e57cf86b58cd9cdb7ffb6dfaa",
      "title": "Continual learning and refinement of causal models through dynamic predicate invention",
      "url": "https://arxiv.org/abs/2602.17217",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17217v1 Announce Type: new Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.",
      "tags": [
        "papers",
        "agents",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17217"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e50c4126cc32e60fa681c763db32274297e3608feaa1250bbc0ebf285cde2edb",
      "title": "From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences",
      "url": "https://arxiv.org/abs/2602.17221",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17221v1 Announce Type: new Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a \"methodological experiment,\" this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology. This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A). This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.",
      "tags": [
        "papers",
        "agents",
        "reasoning"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17221"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7469076e25202cf7dfc00807c31bb63099d645a86f1f7f26e957630bdd403065",
      "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight",
      "url": "https://arxiv.org/abs/2602.17222",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17222v1 Announce Type: new Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17222"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e6c83a493c6d923ab1942bd8d6394aadafaae69932fee55b1997b5e7ee5e2324",
      "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
      "url": "https://arxiv.org/abs/2602.17229",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17229v1 Announce Type: new Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17229"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "fcd589af67b84bcd84347d4f4dda33dd417d9804e7c2ef6633ca1c50e1324465",
      "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
      "url": "https://arxiv.org/abs/2602.17234",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17234v1 Announce Type: new Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \\textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \\textbf{Shapley}-weighted \\textbf{D}ecision-\\textbf{C}ritical \\textbf{L}eakage \\textbf{R}ate (\\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \\textbf{Time}-\\textbf{S}upervised \\textbf{P}rediction with \\textbf{E}xtracted \\textbf{C}laims (\\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17234"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ce527d6978ab5adf7bbabc9bbfa69ddb31d683317821f1be52e4417c016db8cd",
      "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
      "url": "https://arxiv.org/abs/2602.17245",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17245v1 Announce Type: new Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \\textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \\textbf{reliability} by providing stable interfaces, \\textbf{efficiency} by reducing dozens of steps into a few function calls, and \\textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.",
      "tags": [
        "papers",
        "agents",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17245"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6dd99456871bbad52803113f30db1644e8a960d8a67cb2057fc0e36b61841faf",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
      "url": "https://arxiv.org/abs/2602.17288",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17288v1 Announce Type: new Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.",
      "tags": [
        "papers",
        "reasoning",
        "compute",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17288"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "14d28d21a7736ad410218e7823a0b2710019f7e64ccfb71dd4c9eecb77668c4b",
      "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions",
      "url": "https://arxiv.org/abs/2602.17308",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17308v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17308"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2953d5b7cf0e7d520da6152f625e9791ca485b1bc1741aac38e2680ea9561b77",
      "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature",
      "url": "https://arxiv.org/abs/2602.17385",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17385v1 Announce Type: new Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17385"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a788ec0c599816af3464ad457ee9fb094e863bbbd6b9bed54dac0773bc97974a",
      "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval",
      "url": "https://arxiv.org/abs/2602.17386",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17386v1 Announce Type: new Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.",
      "tags": [
        "papers",
        "reasoning",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17386"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6b7c6f34ccbf1342a9e620b993c1d7928d0f2cc5204ef582c2d47bb7ff53b18a",
      "title": "A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities",
      "url": "https://arxiv.org/abs/2602.17402",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17402v1 Announce Type: new Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17402"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6997800facb15f5d7f7443fd8847c420070e69f894235c07e6153a073cf70dad",
      "title": "A Privacy by Design Framework for Large Language Model-Based Applications for Children",
      "url": "https://arxiv.org/abs/2602.17418",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17418v1 Announce Type: new Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.",
      "tags": [
        "papers",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17418"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7aaf49366adb62a1c4041fce7022b387b65c0277bb8a529fe29e1f807162ffde",
      "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation",
      "url": "https://arxiv.org/abs/2602.17442",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17442v1 Announce Type: new Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/",
      "tags": [
        "papers",
        "agents",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17442"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c32414c9589636f274a9974b1fff1e3a255ee47626cbb5b0cf304f36202db2d8",
      "title": "Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems",
      "url": "https://arxiv.org/abs/2602.17508",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17508v1 Announce Type: new Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17508"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4be4819ed509b4371f9145c4f48464d64f75914713688043c4bda1bf0ddc794f",
      "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.17529",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17529v1 Announce Type: new Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17529"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bd1fa6345d4acb6b42bffb767e1447e978b322a68609c176314fbbe67c3d0d99",
      "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
      "url": "https://arxiv.org/abs/2602.17544",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17544v1 Announce Type: new Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17544"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2d056660ac538f61272a47297e2eb5f0579504e9540f6f06edabb7500ca260e1",
      "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
      "url": "https://arxiv.org/abs/2602.17547",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17547v1 Announce Type: new Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17547"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6e137c7eb803690a1bef854d29e62bb8a410a8fba7c689a9090dfda83fd9e0d0",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "url": "https://arxiv.org/abs/2602.17560",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17560v1 Announce Type: new Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \\textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17560"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "518aba97b70a51616585b743ac0a1338ff5b8bb8b59ccc3e3de504ae29384d68",
      "title": "A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN",
      "url": "https://arxiv.org/abs/2602.17566",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17566v1 Announce Type: new Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17566"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "42bcc010505cb472492a44a8eb8fdcbdf999b0292e2806f37efc6672eaa0b414",
      "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
      "url": "https://arxiv.org/abs/2602.17594",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17594v1 Announce Type: new Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17594"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9dbd4234aead157f9a0ebc8c8bbfaca3b0def55b86b390b66df4aaf97517de2d",
      "title": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models",
      "url": "https://arxiv.org/abs/2602.17602",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17602v1 Announce Type: new Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17602"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ae86964128e33d5355df84ab840febde1fd3b359724b9b37ea3e2e2012cb6d9a",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
      "url": "https://arxiv.org/abs/2602.17607",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17607v1 Announce Type: new Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \\texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17607"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ec376c6f6ba3bc38632ad89fb1a690667ed31030a30e8258c7fecd7a9c59fade",
      "title": "CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts",
      "url": "https://arxiv.org/abs/2602.17663",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17663v1 Announce Type: new Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (\"Has the person ever been at this place?\") and $isAt$ (\"Is the person located at this place around publication time?\") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.",
      "tags": [
        "papers",
        "reasoning",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17663"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "10e384067d302ade4cc1d562a095401a3552a30fe96fd66e452893ebc30cf6c7",
      "title": "GPU-Accelerated Algorithms for Graph Vector Search: Taxonomy, Empirical Study, and Research Directions",
      "url": "https://arxiv.org/abs/2602.16719",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16719v1 Announce Type: cross Abstract: Approximate Nearest Neighbor Search (ANNS) underpins many large-scale data mining and machine learning applications, with efficient retrieval increasingly hinging on GPU acceleration as dataset sizes grow. Although graph-based approaches represent the state of the art in approximate nearest neighbor search, there is a lack of systematic understanding regarding their optimization for modern GPU architectures and their end-to-end effectiveness in practical scenarios. In this work, we present a comprehensive survey and experimental study of GPU-accelerated graph-based vector search algorithms. We establish a detailed taxonomy of GPU optimization strategies and clarify the mapping between algorithmic tasks and hardware execution units within GPUs. Through a thorough evaluation of six leading algorithms on eight large-scale benchmark datasets, we assess both graph index construction and query search performance. Our analysis reveals that distance computation remains the primary computational bottleneck, while data transfer between the host CPU and GPU emerges as the dominant factor influencing real-world latency at large scale. We also highlight key trade-offs in scalability and memory usage across different system designs. Our findings offer clear guidelines for designing scalable and robust GPU-powered approximate nearest neighbor search systems, and provide a comprehensive benchmark for the knowledge discovery and data mining community.",
      "tags": [
        "papers",
        "eval",
        "compute",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16719"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "128429cee11d20ba6483ba0eed5fb8912e16ae0dfcf1f2df118f8a5699a8193d",
      "title": "APEX-SQL: Talking to the data via Agentic Exploration for Text-to-SQL",
      "url": "https://arxiv.org/abs/2602.16720",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16720v1 Announce Type: cross Abstract: Text-to-SQL systems powered by Large Language Models have excelled on academic benchmarks but struggle in complex enterprise environments. The primary limitation lies in their reliance on static schema representations, which fails to resolve semantic ambiguity and scale effectively to large, complex databases. To address this, we propose APEX-SQL, an Agentic Text-to-SQL Framework that shifts the paradigm from passive translation to agentic exploration. Our framework employs a hypothesis-verification loop to ground model reasoning in real data. In the schema linking phase, we use logical planning to verbalize hypotheses, dual-pathway pruning to reduce the search space, and parallel data profiling to validate column roles against real data, followed by global synthesis to ensure topological connectivity. For SQL generation, we introduce a deterministic mechanism to retrieve exploration directives, allowing the agent to effectively explore data distributions, refine hypotheses, and generate semantically accurate SQLs. Experiments on BIRD (70.65% execution accuracy) and Spider 2.0-Snow (51.01% execution accuracy) demonstrate that APEX-SQL outperforms competitive baselines with reduced token consumption. Further analysis reveals that agentic exploration acts as a performance multiplier, unlocking the latent reasoning potential of foundation models in enterprise settings. Ablation studies confirm the critical contributions of each component in ensuring robust and accurate data analysis.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16720"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6e6f5f1519bc9c50f5c14a34e3b77e8bc58e49508af0d709e4d881e7246a4acf",
      "title": "Is Mamba Reliable for Medical Imaging?",
      "url": "https://arxiv.org/abs/2602.16723",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16723v1 Announce Type: cross Abstract: State-space models like Mamba offer linear-time sequence processing and low memory, making them attractive for medical imaging. However, their robustness under realistic software and hardware threat models remains underexplored. This paper evaluates Mamba on multiple MedM-NIST classification benchmarks under input-level attacks, including white-box adversarial perturbations (FGSM/PGD), occlusion-based PatchDrop, and common acquisition corruptions (Gaussian noise and defocus blur) as well as hardware-inspired fault attacks emulated in software via targeted and random bit-flip injections into weights and activations. We profile vulnerabilities and quantify impacts on accuracy indicating that defenses are needed for deployment.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16723"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f8ff77671a426f564fef8b79ffd994f66ba9ae410e96cd01110c2a320234d031",
      "title": "Intent Laundering: AI Safety Datasets Are Not What They Seem",
      "url": "https://arxiv.org/abs/2602.16729",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16729v1 Announce Type: cross Abstract: We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world attacks based on three key properties: driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on \"triggering cues\": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce \"intent laundering\": a procedure that abstracts away triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world attacks due to their overreliance on triggering cues. In fact, once these cues are removed, all previously evaluated \"reasonably safe\" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated and how real-world adversaries behave.",
      "tags": [
        "papers",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16729"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a7b11f6ca73225633aaacf8b03316143b1b9171ab56220401d2ddeb4f0c1f52d",
      "title": "The Compute ICE-AGE: Invariant Compute Envelope under Addressable Graph Evolution",
      "url": "https://arxiv.org/abs/2602.16736",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16736v1 Announce Type: cross Abstract: This paper presents empirical results from a production-grade C++ implementation of a deterministic semantic state substrate derived from prior formal work on Bounded Local Generator Classes (Martin, 2026). The system was mathematically specified prior to implementation and realized as a CPU-resident graph engine operating under bounded local state evolution. Contemporary inference-driven AI architectures reconstruct semantic state through probabilistic recomposition, producing compute cost that scales with token volume and context horizon. In contrast, the substrate described here represents semantic continuity as a persistent, addressable memory graph evolved under a time-modulated local operator g(t). Work is bounded by local semantic change Delta s, independent of total memory cardinality M. Empirical measurements on Apple M2-class silicon demonstrate invariant traversal latency (approximately 0.25 to 0.32 ms), stable CPU utilization (approximately 17.2 percent baseline with Delta CPU approximately 0 to 0.2 percent), and no scale-correlated thermal signature across 1M to 25M node regimes under sustained operation. Measured per-node density ranges from approximately 1.3 KB (Float64 baseline) to approximately 687 bytes (compressed Float32 accounting). Under binary memory accounting, this yields a 1.6 billion node capacity projection within a 1 TiB envelope. These results indicate an empirically invariant thermodynamic regime in which scaling is governed by memory capacity rather than inference-bound recomposition. The Compute ICE-AGE is defined as the Invariant Compute Envelope under Addressable Graph Evolution, and the empirical evidence presented demonstrates this regime up to 25M nodes.",
      "tags": [
        "papers",
        "compute",
        "inference"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16736"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ac2893edf777d0caa08e0b538b5b6af2313924d700b1fc95c7be2a0c45582e9e",
      "title": "Quantifying LLM Attention-Head Stability: Implications for Circuit Universality",
      "url": "https://arxiv.org/abs/2602.16740",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16740v1 Announce Type: cross Abstract: In mechanistic interpretability, recent work scrutinizes transformer \"circuits\" - sparse, mono or multi layer sub computations, that may reflect human understandable functions. Yet, these network circuits are rarely acid-tested for their stability across different instances of the same deep learning architecture. Without this, it remains unclear whether reported circuits emerge universally across labs or turn out to be idiosyncratic to a particular estimation instance, potentially limiting confidence in safety-critical settings. Here, we systematically study stability across-refits in increasingly complex transformer language models of various sizes. We quantify, layer by layer, how similarly attention heads learn representations across independently initialized training runs. Our rigorous experiments show that (1) middle-layer heads are the least stable yet the most representationally distinct; (2) deeper models exhibit stronger mid-depth divergence; (3) unstable heads in deeper layers become more functionally important than their peers from the same layer; (4) applying weight decay optimization substantially improves attention-head stability across random model initializations; and (5) the residual stream is comparatively stable. Our findings establish the cross-instance robustness of circuits as an essential yet underappreciated prerequisite for scalable oversight, drawing contours around possible white-box monitorability of AI systems.",
      "tags": [
        "papers",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16740"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "219bff80285ecb1d071e26682ca9f95f8f8b97e9b756cd49e2e637065f4d7b0b",
      "title": "Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis",
      "url": "https://arxiv.org/abs/2602.16741",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16741v1 Announce Type: cross Abstract: AI-assisted code review is widely used to detect vulnerabilities before production release. Prior work shows that adversarial prompt manipulation can degrade large language model (LLM) performance in code generation. We test whether similar comment-based manipulation misleads LLMs during vulnerability detection. We build a 100-sample benchmark across Python, JavaScript, and Java, each paired with eight comment variants ranging from no comments to adversarial strategies such as authority spoofing and technical deception. Eight frontier models, five commercial and three open-source, are evaluated in 9,366 trials. Adversarial comments produce small, statistically non-significant effects on detection accuracy (McNemar exact p > 0.21; all 95 percent confidence intervals include zero). This holds for commercial models with 89 to 96 percent baseline detection and open-source models with 53 to 72 percent, despite large absolute performance gaps. Unlike generation settings where comment manipulation achieves high attack success, detection performance does not meaningfully degrade. More complex adversarial strategies offer no advantage over simple manipulative comments. We test four automated defenses across 4,646 additional trials (14,012 total). Static analysis cross-referencing performs best at 96.9 percent detection and recovers 47 percent of baseline misses. Comment stripping reduces detection for weaker models by removing helpful context. Failures concentrate on inherently difficult vulnerability classes, including race conditions, timing side channels, and complex authorization logic, rather than on adversarial comments.",
      "tags": [
        "papers",
        "eval",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16741"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a24ccf9a0d0b3ed7e292dad0e348b3a800a341cb51570a6a37101c976d3401f5",
      "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.16742",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16742v1 Announce Type: cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \\textbf{DeepVision-103K}, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: \\href{https://huggingface.co/datasets/skylenage/DeepVision-103K}{this url}.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16742"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ed05176fa430e43443fdaac39a494547e363e6222e898dd6519583e32f245fec",
      "title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency",
      "url": "https://arxiv.org/abs/2602.16745",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16745v1 Announce Type: cross Abstract: Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16745"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d67a7e338b489f500463831ff3484005a51edef34166383f48f92477fe203f3d",
      "title": "Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking",
      "url": "https://arxiv.org/abs/2602.16746",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16746v1 Announce Type: cross Abstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.",
      "tags": [
        "papers",
        "safety",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16746"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e14a71c190f8c368eab521d9bb563dd17f18d411c110d7d0b8f49bcec01f1f81",
      "title": "LiveClin: A Live Clinical Benchmark without Leakage",
      "url": "https://arxiv.org/abs/2602.16747",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16747v1 Announce Type: cross Abstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy, followed closely by Attending Physicians, with both surpassing most models. LiveClin thus provides a continuously evolving, clinically grounded framework to guide the development of medical LLMs towards closing this gap and achieving greater reliability and real-world utility. Our data and code are publicly available at https://github.com/AQ-MedAI/LiveClin.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16747"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "35220e681544fd93487f1d0e375f9441b5dded90692f88ff92d26dd7e3e5d86b",
      "title": "PREFER: An Ontology for the PREcision FERmentation Community",
      "url": "https://arxiv.org/abs/2602.16755",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16755v1 Announce Type: cross Abstract: Precision fermentation relies on microbial cell factories to produce sustainable food, pharmaceuticals, chemicals, and biofuels. Specialized laboratories such as biofoundries are advancing these processes using high-throughput bioreactor platforms, which generate vast datasets. However, the lack of community standards limits data accessibility and interoperability, preventing integration across platforms. In order to address this, we introduce PREFER, an open-source ontology designed to establish a unified standard for bioprocess data. Built in alignment with the widely adopted Basic Formal Ontology (BFO) and connecting with several other community ontologies, PREFER ensures consistency and cross-domain compatibility and covers the whole precision fermentation process. Integrating PREFER into high-throughput bioprocess development workflows enables structured metadata that supports automated cross-platform execution and high-fidelity data capture. Furthermore, PREFER's standardization has the potential to bridge disparate data silos, generating machine-actionable datasets critical for training predictive, robust machine learning models in synthetic biology. This work provides the foundation for scalable, interoperable bioprocess systems and supports the transition toward more data-driven bioproduction.",
      "tags": [
        "papers",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16755"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "40da287090d822812d47af034bd1765ab08c1b19fe6c7919c16b3dae65e8b147",
      "title": "Attending to Routers Aids Indoor Wireless Localization",
      "url": "https://arxiv.org/abs/2602.16762",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16762v1 Announce Type: cross Abstract: Modern machine learning-based wireless localization using Wi-Fi signals continues to face significant challenges in achieving groundbreaking performance across diverse environments. A major limitation is that most existing algorithms do not appropriately weight the information from different routers during aggregation, resulting in suboptimal convergence and reduced accuracy. Motivated by traditional weighted triangulation methods, this paper introduces the concept of attention to routers, ensuring that each router's contribution is weighted differently when aggregating information from multiple routers for triangulation. We demonstrate, by incorporating attention layers into a standard machine learning localization architecture, that emphasizing the relevance of each router can substantially improve overall performance. We have also shown through evaluation over the open-sourced datasets and demonstrate that Attention to Routers outperforms the benchmark architecture by over 30% in accuracy.",
      "tags": [
        "papers",
        "eval",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16762"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9f48dbd2ce5e9d4e813b430441fe8b39a6aac7eff6351cee7192a920a94c1d03",
      "title": "Large-scale online deanonymization with LLMs",
      "url": "https://arxiv.org/abs/2602.16800",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16800v1 Announce Type: cross Abstract: We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that individual, we implement a scalable attack pipeline that uses LLMs to: (1) extract identity-relevant features, (2) search for candidate matches via semantic embeddings, and (3) reason over top candidates to verify matches and reduce false positives. Compared to prior deanonymization work (e.g., on the Netflix prize) that required structured data or manual feature engineering, our approach works directly on raw user content across arbitrary platforms. We construct three datasets with known ground-truth data to evaluate our attacks. The first links Hacker News to LinkedIn profiles, using cross-platform references that appear in the profiles. Our second dataset matches users across Reddit movie discussion communities; and the third splits a single user's Reddit history in time to create two pseudonymous profiles to be matched. In each setting, LLM-based methods substantially outperform classical baselines, achieving up to 68% recall at 90% precision compared to near 0% for the best non-LLM method. Our results show that the practical obscurity protecting pseudonymous users online no longer holds and that threat models for online privacy need to be reconsidered.",
      "tags": [
        "papers",
        "agents",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16800"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4c9726028dbe7bdb9dd4114237d322a2a6ea3c1a32dd389079922af813e852d3",
      "title": "References Improve LLM Alignment in Non-Verifiable Domains",
      "url": "https://arxiv.org/abs/2602.16802",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16802v1 Announce Type: cross Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16802"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b79a8528f999568cad7484d626c93c773d8396404f8f66e2b2168e4fe062336b",
      "title": "Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark",
      "url": "https://arxiv.org/abs/2602.16811",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16811v1 Announce Type: cross Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16811"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7a469d2c4d3ddff760ac2b5c7bcb225b34c0b8294a864a543c6dcfa83a58fb2e",
      "title": "One-step Language Modeling via Continuous Denoising",
      "url": "https://arxiv.org/abs/2602.16813",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16813v1 Announce Type: cross Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16813"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4290490969f7c8d9cc3009b5a91070597caa61431fa84cbababd92f7690dc52f",
      "title": "AI-Mediated Feedback Improves Student Revisions: A Randomized Trial with FeedbackWriter in a Large Undergraduate Course",
      "url": "https://arxiv.org/abs/2602.16820",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16820v1 Announce Type: cross Abstract: Despite growing interest in using LLMs to generate feedback on students' writing, little is known about how students respond to AI-mediated versus human-provided feedback. We address this gap through a randomized controlled trial in a large introductory economics course (N=354), where we introduce and deploy FeedbackWriter - a system that generates AI suggestions to teaching assistants (TAs) while they provide feedback on students' knowledge-intensive essays. TAs have the full capacity to adopt, edit, or dismiss the suggestions. Students were randomly assigned to receive either handwritten feedback from TAs (baseline) or AI-mediated feedback where TAs received suggestions from FeedbackWriter. Students revise their drafts based on the feedback, which is further graded. In total, 1,366 essays were graded using the system. We found that students receiving AI-mediated feedback produced significantly higher-quality revisions, with gains increasing as TAs adopted more AI suggestions. TAs found the AI suggestions useful for spotting gaps and clarifying rubrics.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16820"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c5e52466432e385ef55073c809cfc0331c3a13cde768c6e78ae503ed3a6ed0a4",
      "title": "HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind",
      "url": "https://arxiv.org/abs/2602.16826",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16826v1 Announce Type: cross Abstract: Theory of mind (ToM) enables AI systems to infer agents' hidden goals and mental states, but existing approaches focus mainly on small human understandable gridworld spaces. We introduce HiVAE, a hierarchical variational architecture that scales ToM reasoning to realistic spatiotemporal domains. Inspired by the belief-desire-intention structure of human cognition, our three-level VAE hierarchy achieves substantial performance improvements on a 3,185-node campus navigation task. However, we identify a critical limitation: while our hierarchical structure improves prediction, learned latent representations lack explicit grounding to actual mental states. We propose self-supervised alignment strategies and present this work to solicit community feedback on grounding approaches.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "safety"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16826"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "be80369d5fa57ed1bbea8913aaba79f19d3448524ab5166ee03b8d2d081a8eb4",
      "title": "Learning under noisy supervision is governed by a feedback-truth gap",
      "url": "https://arxiv.org/abs/2602.16829",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16829v1 Announce Type: cross Abstract: When feedback is absorbed faster than task structure can be evaluated, the learner will favor feedback over truth. A two-timescale model shows this feedback-truth gap is inevitable whenever the two rates differ and vanishes only when they match. We test this prediction across neural networks trained with noisy labels (30 datasets, 2,700 runs), human probabilistic reversal learning (N = 292), and human reward/punishment learning with concurrent EEG (N = 25). In each system, truth is defined operationally: held-out labels, the objectively correct option, or the participant's pre-feedback expectation - the only non-circular reference decodable from post-feedback EEG. The gap appeared universally but was regulated differently: dense networks accumulated it as memorization; sparse-residual scaffolding suppressed it; humans generated transient over-commitment that was actively recovered. Neural over-commitment (~0.04-0.10) was amplified tenfold into behavioral commitment (d = 3.3-3.9). The gap is a fundamental constraint on learning under noisy supervision; its consequences depend on the regulation each system employs.",
      "tags": [
        "papers",
        "policy",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16829"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "04e7561328291d9545838613344aa066a3d0a71a13a5c38a516804c72a85b051",
      "title": "VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study",
      "url": "https://arxiv.org/abs/2602.16833",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16833v1 Announce Type: cross Abstract: Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16833"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d1999c231da106be6e539a9d9276728e37add6eca0eb5f835d55d1fb00e74dbb",
      "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities",
      "url": "https://arxiv.org/abs/2602.16844",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16844v1 Announce Type: cross Abstract: To enable human oversight, agentic AI systems often provide a trace of reasoning and action steps. Designing traces to have an informative, but not overwhelming, level of detail remains a critical challenge. In three user studies on a Computer User Agent, we investigate the utility of basic action traces for verification, explore three alternatives via design probes, and test a novel interface's impact on error finding in question-answering tasks. As expected, we find that current practices are cumbersome, limiting their efficacy. Conversely, our proposed design reduced the time participants spent finding errors. However, although participants reported higher levels of confidence in their decisions, their final accuracy was not meaningfully improved. To this end, our study surfaces challenges for human verification of agentic systems, including managing built-in assumptions, users' subjective and changing correctness criteria, and the shortcomings, yet importance, of communicating the agent's process.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "compute"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16844"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "72bc1bc7c7119cafc7651cc53e83233d8401ab8021e3cf818ee6fd4a667322cc",
      "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation",
      "url": "https://arxiv.org/abs/2602.16863",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16863v1 Announce Type: cross Abstract: The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.",
      "tags": [
        "papers",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16863"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ffa8a87991e665e4924b795ec9b760076ce6a65837fd935354805b3da3bb6c32",
      "title": "Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling",
      "url": "https://arxiv.org/abs/2602.16864",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16864v1 Announce Type: cross Abstract: Time series (TS) modeling has come a long way from early statistical, mainly linear, approaches to the current trend in TS foundation models. With a lot of hype and industrial demand in this field, it is not always clear how much progress there really is. To advance TS forecasting and analysis to the next level, here we argue that the field needs a dynamical systems (DS) perspective. TS of observations from natural or engineered systems almost always originate from some underlying DS, and arguably access to its governing equations would yield theoretically optimal forecasts. This is the promise of DS reconstruction (DSR), a class of ML/AI approaches that aim to infer surrogate models of the underlying DS from data. But models based on DS principles offer other profound advantages: Beyond short-term forecasts, they enable to predict the long-term statistics of an observed system, which in many practical scenarios may be the more relevant quantities. DS theory furthermore provides domain-independent theoretical insight into mechanisms underlying TS generation, and thereby will inform us, e.g., about upper bounds on performance of any TS model, generalization into unseen regimes as in tipping points, or potential control strategies. After reviewing some of the central concepts, methods, measures, and models in DS theory and DSR, we will discuss how insights from this field can advance TS modeling in crucial ways, enabling better forecasting with much lower computational and memory footprints. We conclude with a number of specific suggestions for translating insights from DSR into TS modeling.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16864"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "beb89b2554f315ff765b366112830d5201e1eb91466327773e2b2a4ae362e472",
      "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
      "url": "https://arxiv.org/abs/2602.16873",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16873v1 Announce Type: cross Abstract: As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dynamically selects among four canonical topologies (parallel, sequential, hierarchical, and hybrid) based on task dependency graphs and empirically derived domain characteristics. Our framework introduces three key contributions: (1) a Performance Convergence Scaling Law, formalizing conditions under which orchestration selection outweighs model selection; (2) a Topology Routing Algorithm that maps task decomposition DAGs to optimal orchestration patterns in O(|V| + |E|) time; and (3) an Adaptive Synthesis Protocol with provable termination guarantees and heuristic consistency scoring for parallel agent outputs. We validate AdaptOrch across coding (SWE-bench), reasoning (GPQA), and retrieval-augmented generation tasks, demonstrating that topology-aware orchestration achieves 12-23% improvement over static single-topology baselines, even when using identical underlying models. Our results establish orchestration design as a first-class optimization target independent of model scaling.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16873"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "96577774ce9b856fc4ac95b6e5f68bb63009b3c11057d16f6d74652b5da79945",
      "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
      "url": "https://arxiv.org/abs/2602.16898",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16898v1 Announce Type: cross Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16898"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "933cfdb69a314687a5a3a4e3c7eee2d48151b29335e89ae9489af2794449059c",
      "title": "A Reversible Semantics for Janus",
      "url": "https://arxiv.org/abs/2602.16913",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16913v1 Announce Type: cross Abstract: Janus is a paradigmatic example of reversible programming language. Indeed, Janus programs can be executed backwards as well as forwards. However, its small-step semantics (useful, e.g., for debugging or as a basis for extensions with concurrency primitives) is not reversible, since it loses information while computing forwards. E.g., it does not satisfy the Loop Lemma, stating that any reduction has an inverse, a main property of reversibility in process calculi, where small-step semantics is commonly used. We present here a novel small-step semantics which is actually reversible, while remaining equivalent to the previous one. It involves the non-trivial challenge of defining a semantics based on a \"program counter\" for a high-level programming language.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16913"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9b37739a381c8a2bc97519922be4d23c74c74a89da795cdfa8bd1a4254096a26",
      "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data",
      "url": "https://arxiv.org/abs/2602.16918",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16918v1 Announce Type: cross Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16918"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "94747fd1ac054c24c85cc08713d3d0c86954126fe93e7c0bffadf01d970a8c81",
      "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
      "url": "https://arxiv.org/abs/2602.16928",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16928v1 Announce Type: cross Abstract: Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16928"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "fa5520eb7e0d5b95b42a4b90885f662128b922c574b53c8030ac0b2d69b67287",
      "title": "Say It My Way: Exploring Control in Conversational Visual Question Answering with Blind Users",
      "url": "https://arxiv.org/abs/2602.16930",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16930v1 Announce Type: cross Abstract: Prompting and steering techniques are well established in general-purpose generative AI, yet assistive visual question answering (VQA) tools for blind users still follow rigid interaction patterns with limited opportunities for customization. User control can be helpful when system responses are misaligned with their goals and contexts, a gap that becomes especially consequential for blind users that may rely on these systems for access. We invite 11 blind users to customize their interactions with a real-world conversational VQA system. Drawing on 418 interactions, reflections, and post-study interviews, we analyze prompting-based techniques participants adopted, including those introduced in the study and those developed independently in real-world settings. VQA interactions were often lengthy: participants averaged 3 turns, sometimes up to 21, with input text typically tenfold shorter than the responses they heard. Built on state-of-the-art LLMs, the system lacked verbosity controls, was limited in estimating distance in space and time, relied on inaccessible image framing, and offered little to no camera guidance. We discuss how customization techniques such as prompt engineering can help participants work around these limitations. Alongside a new publicly available dataset, we offer insights for interaction design at both query and system levels.",
      "tags": [
        "papers",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16930"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f8c8c10895bf77f50356d8e8ef26651741d0ed4cc3b9af187ab876ad90dcc640",
      "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
      "url": "https://arxiv.org/abs/2602.16932",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16932v1 Announce Type: cross Abstract: Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16932"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "343088800eefa0958855a3a240a2388cb7e633245af9779cf253bfbb45f636be",
      "title": "Beyond Message Passing: A Symbolic Alternative for Expressive and Interpretable Graph Learning",
      "url": "https://arxiv.org/abs/2602.16947",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16947v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have become essential in high-stakes domains such as drug discovery, yet their black-box nature remains a significant barrier to trustworthiness. While self-explainable GNNs attempt to bridge this gap, they often rely on standard message-passing backbones that inherit fundamental limitations, including the 1-Weisfeiler-Lehman (1-WL) expressivity barrier and a lack of fine-grained interpretability. To address these challenges, we propose SymGraph, a symbolic framework designed to transcend these constraints. By replacing continuous message passing with discrete structural hashing and topological role-based aggregation, our architecture theoretically surpasses the 1-WL barrier, achieving superior expressiveness without the overhead of differentiable optimization. Extensive empirical evaluations demonstrate that SymGraph achieves state-of-the-art performance, outperforming existing self-explainable GNNs. Notably, SymGraph delivers 10x to 100x speedups in training time using only CPU execution. Furthermore, SymGraph generates rules with superior semantic granularity compared to existing rule-based methods, offering great potential for scientific discovery and explainable AI.",
      "tags": [
        "papers",
        "eval",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16947"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f24dcf4b6047ada356358d337957058b68edda39482fcc20fb745a2c7af83249",
      "title": "When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English",
      "url": "https://arxiv.org/abs/2602.16957",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16957v1 Announce Type: cross Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.",
      "tags": [
        "papers",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16957"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "663a3c62c424791d37f0168185146ab43e675b62fb69300f01fcdb4aca331b00",
      "title": "Eigenmood Space: Uncertainty-Aware Spectral Graph Analysis of Psychological Patterns in Classical Persian Poetry",
      "url": "https://arxiv.org/abs/2602.16959",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16959v1 Announce Type: cross Abstract: Classical Persian poetry is a historically sustained archive in which affective life is expressed through metaphor, intertextual convention, and rhetorical indirection. These properties make close reading indispensable while limiting reproducible comparison at scale. We present an uncertainty-aware computational framework for poet-level psychological analysis based on large-scale automatic multi-label annotation. Each verse is associated with a set of psychological concepts, per-label confidence scores, and an abstention flag that signals insufficient evidence. We aggregate confidence-weighted evidence into a Poet $\\times$ Concept matrix, interpret each poet as a probability distribution over concepts, and quantify poetic individuality as divergence from a corpus baseline using Jensen--Shannon divergence and Kullback--Leibler divergence. To capture relational structure beyond marginals, we build a confidence-weighted co-occurrence graph over concepts and define an Eigenmood embedding through Laplacian spectral decomposition. On a corpus of 61{,}573 verses across 10 poets, 22.2\\% of verses are abstained, underscoring the analytical importance of uncertainty. We further report sensitivity analysis under confidence thresholding, selection-bias diagnostics that treat abstention as a category, and a distant-to-close workflow that retrieves verse-level exemplars along Eigenmood axes. The resulting framework supports scalable, auditable digital-humanities analysis while preserving interpretive caution by propagating uncertainty from verse-level evidence to poet-level inference.",
      "tags": [
        "papers",
        "inference"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16959"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "30222810bcaaa0744bbdbd50fdcceecdd2cae51860e4ad771c91f4b777dbf261",
      "title": "A Unified Framework for Locality in Scalable MARL",
      "url": "https://arxiv.org/abs/2602.16966",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16966v1 Announce Type: cross Abstract: Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \\emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^\\pi$, which decouples the environment's sensitivity to state ($E^{\\mathrm{s}}$) and action ($E^{\\mathrm{a}}$) from the policy's sensitivity to state ($\\Pi(\\pi)$). This decomposition reveals that locality can be induced by a smooth policy (small $\\Pi(\\pi)$) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition $\\rho(E^{\\mathrm{s}}+E^{\\mathrm{a}}\\Pi(\\pi)) < 1$ for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.",
      "tags": [
        "papers",
        "agents",
        "policy"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16966"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a78ff2df9cbfd8af25df1c2a48083871a43c4b8492f25fa189e0085e1ff23fad",
      "title": "Early-Warning Signals of Grokking via Loss-Landscape Geometry",
      "url": "https://arxiv.org/abs/2602.16967",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16967v1 Announce Type: cross Abstract: Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly 50% on Dyck), while suppressing orthogonal gradient flow delays or prevents it. The three task families form a spectrum of causal sensitivity -- modular arithmetic is rigid, Dyck is responsive, SCAN is intermediate -- yet suppression delays or prevents grokking in all cases, establishing necessity as a universal finding. These results identify the commutator defect as a robust, architecture-agnostic, causally implicated early-warning signal for delayed generalization in transformers.",
      "tags": [
        "papers",
        "eval",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16967"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6a28f57fb911132e1ccbe5d84bc5e4592f958e28c46403c874b59f1d17b20731",
      "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
      "url": "https://arxiv.org/abs/2602.16968",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16968v1 Announce Type: cross Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\\times$ and $3.2\\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.",
      "tags": [
        "papers",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16968"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2c87387cfb097fee52ea06d7352f0bc4da4f45f4d022ba97c71fe62ada828bba",
      "title": "Exploring LLMs for User Story Extraction from Mockups",
      "url": "https://arxiv.org/abs/2602.16997",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16997v1 Announce Type: cross Abstract: User stories are one of the most widely used artifacts in the software industry to define functional requirements. In parallel, the use of high-fidelity mockups facilitates end-user participation in defining their needs. In this work, we explore how combining these techniques with large language models (LLMs) enables agile and automated generation of user stories from mockups. To this end, we present a case study that analyzes the ability of LLMs to extract user stories from high-fidelity mockups, both with and without the inclusion of a glossary of the Language Extended Lexicon (LEL) in the prompts. Our results demonstrate that incorporating the LEL significantly enhances the accuracy and suitability of the generated user stories. This approach represents a step forward in the integration of AI into requirements engineering, with the potential to improve communication between users and developers.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16997"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8135c5e9cdd11ae36f74e6577731284863e3b468cf66b1bc6ffff6ab829bfe11",
      "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
      "url": "https://arxiv.org/abs/2602.17003",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17003v1 Announce Type: cross Abstract: Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables fine-grained assessment of personalization. We conduct extensive experiments across various agent architectures, backbone models, history access schemes, and queries with varying ambiguity levels, revealing key challenges in personalized web agent behavior. For reproducibility, our codes and datasets are publicly available at https://anonymous.4open.science/r/Persona2Web-73E8.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17003"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c003b0b3acfc7d479635d0d91b65790f8b46d3b77867df3c1eea779c3c3e3df5",
      "title": "ReIn: Conversational Error Recovery with Reasoning Inception",
      "url": "https://arxiv.org/abs/2602.17022",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17022v1 Announce Type: cross Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17022"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "10e8b12a0f42d51e5c18a62622f731a0bb562a477f4fe2324e96911087a8fc38",
      "title": "Transforming Behavioral Neuroscience Discovery with In-Context Learning and AI-Enhanced Tensor Methods",
      "url": "https://arxiv.org/abs/2602.17027",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17027v1 Announce Type: cross Abstract: Scientific discovery pipelines typically involve complex, rigid, and time-consuming processes, from data preparation to analyzing and interpreting findings. Recent advances in AI have the potential to transform such pipelines in a way that domain experts can focus on interpreting and understanding findings, rather than debugging rigid pipelines or manually annotating data. As part of an active collaboration between data science/AI researchers and behavioral neuroscientists, we showcase an example AI-enhanced pipeline, specifically designed to transform and accelerate the way that the domain experts in the team are able to gain insights out of experimental data. The application at hand is in the domain of behavioral neuroscience, studying fear generalization in mice, an important problem whose progress can advance our understanding of clinically significant and often debilitating conditions such as PTSD (Post-Traumatic Stress Disorder). We identify the emerging paradigm of \"In-Context Learning\" (ICL) as a suitable interface for domain experts to automate parts of their pipeline without the need for or familiarity with AI model training and fine-tuning, and showcase its remarkable efficacy in data preparation and pattern interpretation. Also, we introduce novel AI-enhancements to tensor decomposition model, which allows for more seamless pattern discovery from the heterogeneous data in our application. We thoroughly evaluate our proposed pipeline experimentally, showcasing its superior performance compared to what is standard practice in the domain, as well as against reasonable ML baselines that do not fall under the ICL paradigm, to ensure that we are not compromising performance in our quest for a seamless and easy-to-use interface for domain experts. Finally, we demonstrate effective discovery, with results validated by the domain experts in the team.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17027"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "083b5e09e09df28827bf383de448785e278100434b8cd0e967810251f2c9644c",
      "title": "Forecasting Anomaly Precursors via Uncertainty-Aware Time-Series Ensembles",
      "url": "https://arxiv.org/abs/2602.17028",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17028v1 Announce Type: cross Abstract: Detecting anomalies in time-series data is critical in domains such as industrial operations, finance, and cybersecurity, where early identification of abnormal patterns is essential for ensuring system reliability and enabling preventive maintenance. However, most existing methods are reactive: they detect anomalies only after they occur and lack the capability to provide proactive early warning signals. In this paper, we propose FATE (Forecasting Anomalies with Time-series Ensembles), a novel unsupervised framework for detecting Precursors-of-Anomaly (PoA) by quantifying predictive uncertainty from a diverse ensemble of time-series forecasting models. Unlike prior approaches that rely on reconstruction errors or require ground-truth labels, FATE anticipates future values and leverages ensemble disagreement to signal early signs of potential anomalies without access to target values at inference time. To rigorously evaluate PoA detection, we introduce Precursor Time-series Aware Precision and Recall (PTaPR), a new metric that extends the traditional Time-series Aware Precision and Recall (TaPR) by jointly assessing segment-level accuracy, within-segment coverage, and temporal promptness of early predictions. This enables a more holistic assessment of early warning capabilities that existing metrics overlook. Experiments on five real-world benchmark datasets show that FATE achieves an average improvement of 19.9 percentage points in PTaPR AUC and 20.02 percentage points in early detection F1 score, outperforming baselines while requiring no anomaly labels. These results demonstrate the effectiveness and practicality of FATE for real-time unsupervised early warning in complex time-series environments.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17028"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4172f257fcb36d5542a68b37d998f0ac186c3612ef47e7659405611736441293",
      "title": "Wink: Recovering from Misbehaviors in Coding Agents",
      "url": "https://arxiv.org/abs/2602.17037",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17037v1 Announce Type: cross Abstract: Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories. To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17037"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "10d2ba1538ba82179b1ad744446bc5174c8372d550a4b30075b244eb3d4d4241",
      "title": "Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data",
      "url": "https://arxiv.org/abs/2602.17051",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17051v1 Announce Type: cross Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17051"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b835cb88b12ab61bcef5f13801ab9381eb35213cc415b80471c5a75886b8bac2",
      "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning",
      "url": "https://arxiv.org/abs/2602.17054",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17054v1 Announce Type: cross Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17054"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "379b398bfe8b019ed93853b78662c6cf7971fa9202cb21ff30ba1c49a021ca5f",
      "title": "Sign Lock-In: Randomly Initialized Weight Signs Persist and Bottleneck Sub-Bit Model Compression",
      "url": "https://arxiv.org/abs/2602.17063",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17063v1 Announce Type: cross Abstract: Sub-bit model compression seeks storage below one bit per weight; as magnitudes are aggressively compressed, the sign bit becomes a fixed-cost bottleneck. Across Transformers, CNNs, and MLPs, learned sign matrices resist low-rank approximation and are spectrally indistinguishable from an i.i.d. Rademacher baseline. Despite this apparent randomness, most weights retain their initialization signs; flips primarily occur via rare near-zero boundary crossings, suggesting that sign-pattern randomness is largely inherited from initialization. We formalize this behavior with sign lock-in theory, a stopping-time analysis of sign flips under SGD noise. Under bounded updates and a rare re-entry condition into a small neighborhood around zero, the number of effective sign flips exhibits a geometric tail. Building on this mechanism, we introduce a gap-based initialization and a lightweight outward-drift regularizer, reducing the effective flip rate to approximately $10^{-3}$ with only about a one-point increase in perplexity.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17063"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2ce7c58ab224d873afd238bbdc31306c56f6cebaf854e0274b04e049d0b5c0ba",
      "title": "General sample size analysis for probabilities of causation: a delta method approach",
      "url": "https://arxiv.org/abs/2602.17070",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17070v1 Announce Type: cross Abstract: Probabilities of causation (PoCs), such as the probability of necessity and sufficiency (PNS), are important tools for decision making but are generally not point identifiable. Existing work has derived bounds for these quantities using combinations of experimental and observational data. However, there is very limited research on sample size analysis, namely, how many experimental and observational samples are required to achieve a desired margin of error. In this paper, we propose a general sample size framework based on the delta method. Our approach applies to settings in which the target bounds of PoCs can be expressed as finite minima or maxima of linear combinations of experimental and observational probabilities. Through simulation studies, we demonstrate that the proposed sample size calculations lead to stable estimation of these bounds.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17070"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f0df4fa78569ed91e3b853ce8fbb97ce1320d56f3bf74c801762e46b77dace44",
      "title": "AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation",
      "url": "https://arxiv.org/abs/2602.17071",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17071v1 Announce Type: cross Abstract: Graph neural networks frequently encounter significant performance degradation when confronted with structural noise or non-homophilous topologies. To address these systemic vulnerabilities, we present AdvSynGNN, a comprehensive architecture designed for resilient node-level representation learning. The proposed framework orchestrates multi-resolution structural synthesis alongside contrastive objectives to establish geometry-sensitive initializations. We develop a transformer backbone that adaptively accommodates heterophily by modulating attention mechanisms through learned topological signals. Central to our contribution is an integrated adversarial propagation engine, where a generative component identifies potential connectivity alterations while a discriminator enforces global coherence. Furthermore, label refinement is achieved through a residual correction scheme guided by per-node confidence metrics, which facilitates precise control over iterative stability. Empirical evaluations demonstrate that this synergistic approach effectively optimizes predictive accuracy across diverse graph distributions while maintaining computational efficiency. The study concludes with practical implementation protocols to ensure the robust deployment of the AdvSynGNN system in large-scale environments.",
      "tags": [
        "papers",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17071"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "df655f1f887652fda56c77c3c9dc174353619e02bb70db97550b19458ab05271",
      "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
      "url": "https://arxiv.org/abs/2602.17095",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17095v1 Announce Type: cross Abstract: Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\\times$.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17095"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c85a31d78b3eeb756c3b638b834491e81f9891e75b5379edc515cb8eba454643",
      "title": "Deep Reinforcement Learning for Optimal Portfolio Allocation: A Comparative Study with Mean-Variance Optimization",
      "url": "https://arxiv.org/abs/2602.17098",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17098v1 Announce Type: cross Abstract: Portfolio Management is the process of overseeing a group of investments, referred to as a portfolio, with the objective of achieving predetermined investment goals. Portfolio optimization is a key component that involves allocating the portfolio assets so as to maximize returns while minimizing risk taken. It is typically carried out by financial professionals who use a combination of quantitative techniques and investment expertise to make decisions about the portfolio allocation. Recent applications of Deep Reinforcement Learning (DRL) have shown promising results when used to optimize portfolio allocation by training model-free agents on historical market data. Many of these methods compare their results against basic benchmarks or other state-of-the-art DRL agents but often fail to compare their performance against traditional methods used by financial professionals in practical settings. One of the most commonly used methods for this task is Mean-Variance Portfolio Optimization (MVO), which uses historical time series information to estimate expected asset returns and covariances, which are then used to optimize for an investment objective. Our work is a thorough comparison between model-free DRL and MVO for optimal portfolio allocation. We detail the specifics of how to make DRL for portfolio optimization work in practice, also noting the adjustments needed for MVO. Backtest results demonstrate strong performance of the DRL agent across many metrics, including Sharpe ratio, maximum drawdowns, and absolute returns.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17098"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "aefdd3c361023972ab7db79129c12355451c088951e66c1d91639d6e8603819a",
      "title": "TIFO: Time-Invariant Frequency Operator for Stationarity-Aware Representation Learning in Time Series",
      "url": "https://arxiv.org/abs/2602.17122",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17122v1 Announce Type: cross Abstract: Nonstationary time series forecasting suffers from the distribution shift issue due to the different distributions that produce the training and test data. Existing methods attempt to alleviate the dependence by, e.g., removing low-order moments from each individual sample. These solutions fail to capture the underlying time-evolving structure across samples and do not model the complex time structure. In this paper, we aim to address the distribution shift in the frequency space by considering all possible time structures. To this end, we propose a Time-Invariant Frequency Operator (TIFO), which learns stationarity-aware weights over the frequency spectrum across the entire dataset. The weight representation highlights stationary frequency components while suppressing non-stationary ones, thereby mitigating the distribution shift issue in time series. To justify our method, we show that the Fourier transform of time series data implicitly induces eigen-decomposition in the frequency space. TIFO is a plug-and-play approach that can be seamlessly integrated into various forecasting models. Experiments demonstrate our method achieves 18 top-1 and 6 top-2 results out of 28 forecasting settings. Notably, it yields 33.3% and 55.3% improvements in average MSE on the ETTm2 dataset. In addition, TIFO reduces computational costs by 60% -70% compared to baseline methods, demonstrating strong scalability across diverse forecasting models.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17122"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ac5959cb20d5a1e68e76d647f06445dff81bb2a8f9576daf8ce90062f298b0e8",
      "title": "3D Scene Rendering with Multimodal Gaussian Splatting",
      "url": "https://arxiv.org/abs/2602.17124",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17124v1 Announce Type: cross Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.",
      "tags": [
        "papers",
        "compute"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17124"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c214615cac711c1e83b53fee8ea87945459b5f49be2c4d79990d65ad6a077229",
      "title": "VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation",
      "url": "https://arxiv.org/abs/2602.17133",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17133v1 Announce Type: cross Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and \"codebook collapse\" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the neural network's viewpoint, performing quantization primarily manifests as injecting a structured perturbation in latent space. Accordingly, VP-VAE replaces the non-differentiable quantizer with distribution-consistent and scale-adaptive latent perturbations generated via Metropolis--Hastings sampling. This design enables stable training without a codebook while making the model robust to inference-time quantization error. Moreover, under the assumption of approximately uniform latent variables, we derive FSP (Finite Scalar Perturbation), a lightweight variant of VP-VAE that provides a unified theoretical explanation and a practical improvement for FSQ-style fixed quantizers. Extensive experiments on image and audio benchmarks demonstrate that VP-VAE and FSP improve reconstruction fidelity and achieve substantially more balanced token usage, while avoiding the instability inherent to coupled codebook training.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17133"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "edcdf3e41ae48666e169a858349aa6a38d6ff1e679d0dd0dafa52fc75a06d035",
      "title": "TimeOmni-VL: Unified Models for Time Series Understanding and Generation",
      "url": "https://arxiv.org/abs/2602.17149",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17149v1 Announce Type: cross Abstract: Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17149"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "68185d5c90d3ea911db5565bbeb1104632eeed73668de0f64bec83dff9e6de20",
      "title": "In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks",
      "url": "https://arxiv.org/abs/2602.17171",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17171v1 Announce Type: cross Abstract: Recent work has demonstrated that transformers and linear attention models can perform in-context learning (ICL) on simple function classes, such as linear regression. In this paper, we empirically study how these two attention mechanisms differ in their ICL behavior on the canonical linear-regression task of Garg et al. We evaluate learning quality (MSE), convergence, and generalization behavior of each architecture. We also analyze how increasing model depth affects ICL performance. Our results illustrate both the similarities and limitations of linear attention relative to quadratic attention in this setting.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17171"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "28a8c01a8a75e38f9ef00aad5fd7e373e28dea243bb7f75ed710a6f64cfa1283",
      "title": "Continual uncertainty learning",
      "url": "https://arxiv.org/abs/2602.17174",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17174v1 Announce Type: cross Abstract: Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting. To ensure learning efficiency, we jointly incorporate a model-based controller (MBC), which guarantees a shared baseline performance across the plant sets, into the learning process to accelerate the convergence. This residual learning scheme facilitates task-specific optimization of the DRL agent for each uncertainty, thereby enhancing sample efficiency. As a practical industrial application, this study applies the proposed method to designing an active vibration controller for automotive powertrains. We verified that the resulting controller is robust against structural nonlinearities and dynamic variations, realizing successful sim-to-real transfer.",
      "tags": [
        "papers",
        "agents",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17174"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3d63bdbbd11a3657f5cf02bfed741959fc79cdbdbd3b9f3915bc15d66321fafa",
      "title": "Universal Fine-Grained Symmetry Inference and Enforcement for Rigorous Crystal Structure Prediction",
      "url": "https://arxiv.org/abs/2602.17176",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17176v1 Announce Type: cross Abstract: Crystal structure prediction (CSP), which aims to predict the three-dimensional atomic arrangement of a crystal from its composition, is central to materials discovery and mechanistic understanding. Existing deep learning models often treat crystallographic symmetry only as a soft heuristic or rely on space group and Wyckoff templates retrieved from known structures, which limits both physical fidelity and the ability to discover genuinely new material structures. In contrast to retrieval-based methods, our approach leverages large language models to encode chemical semantics and directly generate fine-grained Wyckoff patterns from composition, effectively circumventing the limitations inherent to database lookups. Crucially, we incorporate domain knowledge into the generative process through an efficient constrained-optimization search that rigorously enforces algebraic consistency between site multiplicities and atomic stoichiometry. By integrating this symmetry-consistent template into a diffusion backbone, our approach constrains the stochastic generative trajectory to a physically valid geometric manifold. This framework achieves state-of-the-art performance across stability, uniqueness, and novelty (SUN) benchmarks, alongside superior matching performance, thereby establishing a new paradigm for the rigorous exploration of targeted crystallographic space. This framework enables efficient expansion into previously uncharted materials space, eliminating reliance on existing databases or a priori structural knowledge.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17176"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "92793154e8f09172971490c013c9a860095b8cdf189dccb74d02500208f66aca",
      "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
      "url": "https://arxiv.org/abs/2602.17183",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17183v1 Announce Type: cross Abstract: Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17183"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "dd183ce481a2944d15f7e0039837af8dbe0bdfaef25a9ef50d607b00fe4bb4be",
      "title": "The Bots of Persuasion: Examining How Conversational Agents' Linguistic Expressions of Personality Affect User Perceptions and Decisions",
      "url": "https://arxiv.org/abs/2602.17185",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17185v1 Announce Type: cross Abstract: Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessimistic), authority (authoritative/submissive), and reasoning (emotional/rational). While the CA's composite personality did not affect participants' decisions, it did affect their perceptions and emotional responses. Particularly, participants interacting with pessimistic CAs felt lower emotional state and lower affinity towards the cause, perceived the CA as less trustworthy and less competent, and yet tended to donate more toward the charity. Perceptions of trust, competence, and situational empathy significantly predicted donation decisions. Our findings emphasize the risks CAs pose as instruments of manipulation, subtly influencing user perceptions and decisions.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17185"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b99fd5d17cc314d1628163cbcbed2f28d354d71d50e374300bdc7fdd141a88ff",
      "title": "Deeper detection limits in astronomical imaging using self-supervised spatiotemporal denoising",
      "url": "https://arxiv.org/abs/2602.17205",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17205v1 Announce Type: cross Abstract: The detection limit of astronomical imaging observations is limited by several noise sources. Some of that noise is correlated between neighbouring image pixels and exposures, so in principle could be learned and corrected. We present an astronomical self-supervised transformer-based denoising algorithm (ASTERIS), that integrates spatiotemporal information across multiple exposures. Benchmarking on mock data indicates that ASTERIS improves detection limits by 1.0 magnitude at 90% completeness and purity, while preserving the point spread function and photometric accuracy. Observational validation using data from the James Webb Space Telescope (JWST) and Subaru telescope identifies previously undetectable features, including low-surface-brightness galaxy structures and gravitationally-lensed arcs. Applied to deep JWST images, ASTERIS identifies three times more redshift > 9 galaxy candidates, with rest-frame ultraviolet luminosity 1.0 magnitude fainter, than previous methods.",
      "tags": [
        "papers",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17205"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "698ebb28337c48ce5051129e2ee96f142c0a0bee1e319aa2739f7d3cd2b329df",
      "title": "Extending quantum theory with AI-assisted deterministic game theory",
      "url": "https://arxiv.org/abs/2602.17213",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17213v1 Announce Type: cross Abstract: We present an AI-assisted framework for predicting individual runs of complex quantum experiments, including contextuality and causality (adaptive measurements), within our long-term programme of discovering a local hidden-variable theory that extends quantum theory. In order to circumvent impossibility theorems, we replace the assumption of free choice (measurement independence and parameter independence) with a weaker, compatibilistic version called contingent free choice. Our framework is based on interpreting complex quantum experiments as a Chess-like game between observers and the universe, which is seen as an economic agent minimizing action. The game structures corresponding to generic experiments such as fixed-causal-order process matrices or causal contextuality scenarios, together with a deterministic non-Nashian resolution algorithm that abandons unilateral deviation assumptions (free choice) and assumes Perfect Prediction instead, were described in previous work. In this new research, we learn the reward functions of the game, which contain a hidden variable, using neural networks. The cost function is the Kullback-Leibler divergence between the frequency histograms obtained through many deterministic runs of the game and the predictions of the extended Born rule. Using our framework on the specific case of the EPR 2-2-2 experiment acts as a proof-of-concept and a toy local-realist hidden-variable model that non-Nashian quantum theory is a promising avenue towards a local hidden-variable theory. Our framework constitutes a solid foundation, which can be further expanded in order to fully discover a complete quantum theory.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17213"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f522138bdf0b650df2fabfe26ef994c303f77be08042c0128efa894db2edadee",
      "title": "TAPO-Structured Description Logic for Information Behavior: Procedural and Oracle-Based Extensions",
      "url": "https://arxiv.org/abs/2602.17242",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17242v1 Announce Type: cross Abstract: We introduce \\emph{TAPO-Structured Description Logic} (TAPO--DL), a formal extension of classical description logic designed to model \\emph{information behavior} as a structured, dynamic process. TAPO--DL extends the standard T--Box/A--Box architecture with two additional layers: a \\emph{Procedural Box} (P--Box), which supports concept-driven, imperative-style programs such as conditional and iterative actions, and an \\emph{Oracle Box} (O--Box), which formalizes controlled interaction with external information sources. While the terminological and assertional components capture static conceptual and factual knowledge, the procedural and oracle-based components enable the explicit representation of information-generating actions and external validation. We provide a unified semantic framework for TAPO--DL based on a co-generative, sheaf-theoretic interpretation, in which local informational states are modeled as sections and informational stability corresponds to the existence of coherent global structures. Within this setting, informational truth is characterized as stability under repeated agentive interaction rather than correspondence to a fixed global state. By integrating description logic with procedural dynamics, oracle-based reasoning, and sheaf-theoretic semantics, TAPO--DL offers a principled formal framework for analyzing information behavior in contexts involving interaction, uncertainty, and contextuality.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17242"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "426123670657d071183273b1fbab1525ee7fd51363e0cbe2b8949b1d8ea332ab",
      "title": "Federated Latent Space Alignment for Multi-user Semantic Communications",
      "url": "https://arxiv.org/abs/2602.17271",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17271v1 Announce Type: cross Abstract: Semantic communication aims to convey meaning for effective task execution, but differing latent representations in AI-native devices can cause semantic mismatches that hinder mutual understanding. This paper introduces a novel approach to mitigating latent space misalignment in multi-agent AI- native semantic communications. In a downlink scenario, we consider an access point (AP) communicating with multiple users to accomplish a specific AI-driven task. Our method implements a protocol that shares a semantic pre-equalizer at the AP and local semantic equalizers at user devices, fostering mutual understanding and task-oriented communication while considering power and complexity constraints. To achieve this, we employ a federated optimization for the decentralized training of the semantic equalizers at the AP and user sides. Numerical results validate the proposed approach in goal-oriented semantic communication, revealing key trade-offs among accuracy, com- munication overhead, complexity, and the semantic proximity of AI-native communication devices.",
      "tags": [
        "papers",
        "agents",
        "safety",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17271"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bcacbd34ef1048a4fabcfe8ea4bfc1cbb2eb3ecd41310058ed98b9101a91ea10",
      "title": "Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective",
      "url": "https://arxiv.org/abs/2602.17283",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17283v1 Announce Type: cross Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc 20\\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17283"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "379f77e4216454cac244729fb9b6a160d04ca2416ce3a2f65f26f7b83ab49d6d",
      "title": "Flickering Multi-Armed Bandits",
      "url": "https://arxiv.org/abs/2602.17315",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17315v1 Announce Type: cross Abstract: We introduce Flickering Multi-Armed Bandits (FMAB), a new MAB framework where the set of available arms (or actions) can change at each round, and the available set at any time may depend on the agent's previously selected arm. We model this constrained, evolving availability using random graph processes, where arms are nodes and the agent's movement is restricted to its local neighborhood. We analyze this problem under two random graph models: an i.i.d. Erd\\H{o}s--R\\'enyi (ER) process and an Edge-Markovian process. We propose and analyze a two-phase algorithm that employs a lazy random walk for exploration to efficiently identify the optimal arm, followed by a navigation and commitment phase for exploitation. We establish high-probability and expected sublinear regret bounds for both graph settings. We show that the exploration cost of our algorithm is near-optimal by establishing a matching information-theoretic lower bound for this problem class, highlighting the fundamental cost of exploration under local-move constraints. We complement our theoretical guarantees with numerical simulations, including a scenario of a robotic ground vehicle scouting a disaster-affected region.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17315"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3798dae80d29ab17c7948e0f2b3f8fc27bffd639714df87188f8f38ccd304338",
      "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
      "url": "https://arxiv.org/abs/2602.17316",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17316v1 Announce Type: cross Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17316"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b647c2cda193fe0f261d200867692b0a1d68c4ef9ba5fed1f5d1342dd5041266",
      "title": "WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval",
      "url": "https://arxiv.org/abs/2602.17327",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17327v1 Announce Type: cross Abstract: We introduce WebFAQ 2.0, a new version of the WebFAQ dataset, containing 198 million FAQ-based natural question-answer pairs across 108 languages. Compared to the previous version, it significantly expands multilingual coverage and the number of bilingual aligned QA pairs to over 14.3M, making it the largest FAQ-based resource. Unlike the original release, WebFAQ 2.0 uses a novel data collection strategy that directly crawls and extracts relevant web content, resulting in a substantially more diverse and multilingual dataset with richer context through page titles and descriptions. In response to community feedback, we also release a hard negatives dataset for training dense retrievers, with 1.25M queries across 20 languages. These hard negatives were mined using a two-stage retrieval pipeline and include cross-encoder scores for 200 negatives per query. We further show how this resource enables two primary fine-tuning strategies for dense retrievers: Contrastive Learning with MultipleNegativesRanking loss, and Knowledge Distillation with MarginMSE loss. WebFAQ 2.0 is not a static resource but part of a long-term effort. Since late 2025, structured FAQs are being regularly released through the Open Web Index, enabling continuous expansion and refinement. We publish the datasets and training scripts to facilitate further research in multilingual and cross-lingual IR. The dataset itself and all related resources are publicly available on GitHub and HuggingFace.",
      "tags": [
        "papers",
        "training",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17327"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "35bf38bdec5ed569ef172018ddac3bcaf8925f4c97e14af3c2425e7b996ab847",
      "title": "SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework",
      "url": "https://arxiv.org/abs/2602.17330",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17330v1 Announce Type: cross Abstract: Comparative analysis of adaptive immune repertoires at population scale is hampered by two practical bottlenecks: the near-quadratic cost of pairwise affinity evaluations and dataset imbalances that obscure clinically important minority clonotypes. We introduce SubQuad, an end-to-end pipeline that addresses these challenges by combining antigen-aware, near-subquadratic retrieval with GPU-accelerated affinity kernels, learned multimodal fusion, and fairness-constrained clustering. The system employs compact MinHash prefiltering to sharply reduce candidate comparisons, a differentiable gating module that adaptively weights complementary alignment and embedding channels on a per-pair basis, and an automated calibration routine that enforces proportional representation of rare antigen-specific subgroups. On large viral and tumor repertoires SubQuad achieves measured gains in throughput and peak memory usage while preserving or improving recall@k, cluster purity, and subgroup equity. By co-designing indexing, similarity fusion, and equity-aware objectives, SubQuad offers a scalable, bias-aware platform for repertoire mining and downstream translational tasks such as vaccine target prioritization and biomarker discovery.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "compute",
        "inference",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17330"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7343651b2d0964ee604deb3c04f0d219fad0d3307c44c0d7f5411fec33347921",
      "title": "From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection",
      "url": "https://arxiv.org/abs/2602.17342",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17342v1 Announce Type: cross Abstract: Graph Out-of-Distribution (OOD) detection aims to identify whether a test graph deviates from the distribution of graphs observed during training, which is critical for ensuring the reliability of Graph Neural Networks (GNNs) when deployed in open-world scenarios. Recent advances in graph OOD detection have focused on test-time training techniques that facilitate OOD detection without accessing potential supervisory information (e.g., training data). However, most of these methods employ a one-pass inference paradigm, which prevents them from progressively correcting erroneous predictions to amplify OOD signals. To this end, we propose a \\textbf{S}elf-\\textbf{I}mproving \\textbf{G}raph \\textbf{O}ut-\\textbf{o}f-\\textbf{D}istribution detector (SIGOOD), which is an unsupervised framework that integrates continuous self-learning with test-time training for effective graph OOD detection. Specifically, SIGOOD generates a prompt to construct a prompt-enhanced graph that amplifies potential OOD signals. To optimize prompts, SIGOOD introduces an Energy Preference Optimization (EPO) loss, which leverages energy variations between the original test graph and the prompt-enhanced graph. By iteratively optimizing the prompt by involving it into the detection model in a self-improving loop, the resulting optimal prompt-enhanced graph is ultimately used for OOD detection. Comprehensive evaluations on 21 real-world datasets confirm the effectiveness and outperformance of our SIGOOD method. The code is at https://github.com/Ee1s/SIGOOD.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17342"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "92c304ca9064d30610f192295c125531683817efd241337903130ecb23fbacba",
      "title": "What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?",
      "url": "https://arxiv.org/abs/2602.17345",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17345v1 Announce Type: cross Abstract: Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17345"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4507c261e98cae7366c21d42a9e7cbfeb1c75fa0bc3d5ec0f2b135b3db45fbba",
      "title": "A feature-stable and explainable machine learning framework for trustworthy decision-making under incomplete clinical data",
      "url": "https://arxiv.org/abs/2602.17364",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17364v1 Announce Type: cross Abstract: Machine learning models are increasingly applied to biomedical data, yet their adoption in high stakes domains remains limited by poor robustness, limited interpretability, and instability of learned features under realistic data perturbations, such as missingness. In particular, models that achieve high predictive performance may still fail to inspire trust if their key features fluctuate when data completeness changes, undermining reproducibility and downstream decision-making. Here, we present CACTUS (Comprehensive Abstraction and Classification Tool for Uncovering Structures), an explainable machine learning framework explicitly designed to address these challenges in small, heterogeneous, and incomplete clinical datasets. CACTUS integrates feature abstraction, interpretable classification, and systematic feature stability analysis to quantify how consistently informative features are preserved as data quality degrades. Using a real-world haematuria cohort comprising 568 patients evaluated for bladder cancer, we benchmark CACTUS against widely used machine learning approaches, including random forests and gradient boosting methods, under controlled levels of randomly introduced missing data. We demonstrate that CACTUS achieves competitive or superior predictive performance while maintaining markedly higher stability of top-ranked features as missingness increases, including in sex-stratified analyses. Our results show that feature stability provides information complementary to conventional performance metrics and is essential for assessing the trustworthiness of machine learning models applied to biomedical data. By explicitly quantifying robustness to missing data and prioritising interpretable, stable features, CACTUS offers a generalizable framework for trustworthy data-driven decision support.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17364"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "768b7e35cb36b43e446d5b1e95872299f8516bcc42fb613b4a369647a85dc84b",
      "title": "Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks",
      "url": "https://arxiv.org/abs/2602.17394",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17394v1 Announce Type: cross Abstract: Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17394"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "06bb4d37af042445092885428efe38284160db198167fa8e9a79a46c367db688",
      "title": "SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery",
      "url": "https://arxiv.org/abs/2602.17395",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17395v1 Announce Type: cross Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17395"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "642cb8dac1e9609356e87059f35de5ca624972183706a64dd9b53884a6591f4c",
      "title": "A High-Level Survey of Optical Remote Sensing",
      "url": "https://arxiv.org/abs/2602.17397",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17397v1 Announce Type: cross Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.",
      "tags": [
        "papers",
        "compute",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17397"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7cf75f368cbcb18d2935af879c18478d39f2671898c24fe1f5075c9300486e23",
      "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers",
      "url": "https://arxiv.org/abs/2602.17410",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17410v1 Announce Type: cross Abstract: Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we propose ILRec, a novel preference fine-tuning framework for LLM-based recommendation, leveraging self-hard negative signals extracted from intermediate layers to improve preference learning. Specifically, we identify self-hard negative tokens from intermediate layers as fine-grained negative supervision that dynamically reflects the model's preference learning process. To effectively integrate these signals into training, we design a two-stage framework comprising cross-layer preference optimization and cross-layer preference distillation, enabling the model to jointly discriminate informative negatives and enhance the quality of negative signals from intermediate layers. In addition, we introduce a lightweight collaborative filtering model to assign token-level rewards for negative signals, mitigating the risk of over-penalizing false negatives. Extensive experiments on three datasets demonstrate ILRec's effectiveness in enhancing the performance of LLM-based recommender systems.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17410"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e7243bc6daa8940b1c2a9032913ed0b469139715f7e8f7549670e459e4024b93",
      "title": "Convergence Analysis of Two-Layer Neural Networks under Gaussian Input Masking",
      "url": "https://arxiv.org/abs/2602.17423",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17423v1 Announce Type: cross Abstract: We investigate the convergence guarantee of two-layer neural network training with Gaussian randomly masked inputs. This scenario corresponds to Gaussian dropout at the input level, or noisy input training common in sensor networks, privacy-preserving training, and federated learning, where each user may have access to partial or corrupted features. Using a Neural Tangent Kernel (NTK) analysis, we demonstrate that training a two-layer ReLU network with Gaussian randomly masked inputs achieves linear convergence up to an error region proportional to the mask's variance. A key technical contribution is resolving the randomness within the non-linear activation, a problem of independent interest.",
      "tags": [
        "papers",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17423"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a291722839d4a48ae11984ea339c57fcadb3c0835dab96876192fee54b246b85",
      "title": "Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study",
      "url": "https://arxiv.org/abs/2602.17431",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17431v1 Announce Type: cross Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17431"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e273d0e28bee828d744431cd0ef0d0feef1657761897ba9cfc584b0cd51ddfc4",
      "title": "Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research",
      "url": "https://arxiv.org/abs/2602.17450",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17450v1 Announce Type: cross Abstract: Web research and practices have evolved significantly over time, offering users diverse and accessible solutions across a wide range of tasks. While advanced concepts such as Web 4.0 have emerged from mature technologies, the introduction of large language models (LLMs) has profoundly influenced both the field and its applications. This wave of LLMs has permeated science and technology so deeply that no area remains untouched. Consequently, LLMs are reshaping web research and development, transforming traditional pipelines into generative solutions for tasks like information retrieval, question answering, recommendation systems, and web analytics. They have also enabled new applications such as web-based summarization and educational tools. This survey explores recent advances in the impact of LLMs-particularly through the use of retrieval-augmented generation (RAG)-on web research and industry. It discusses key developments, open challenges, and future directions for enhancing web solutions with LLMs.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17450"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "11587e657e25d6779b61872aa7aa12f86e7175566793e6a11c162c5e88955c1a",
      "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
      "url": "https://arxiv.org/abs/2602.17452",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17452v1 Announce Type: cross Abstract: We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes it easy to share and deploy models across different frameworks, hardware platforms, and runtime environments without requiring framework-specific conversions. Our lookup arguments, which use sumcheck protocol, are well-suited for non-linear functions -- key building blocks in modern ML. We apply optimisations such as neural teleportation to reduce the size of lookup tables while preserving model accuracy, as well as several tensor-level verification optimisations detailed in this paper. We demonstrate that Jolt Atlas can prove model inference in memory-constrained environments -- a prover property commonly referred to as \\textit{streaming}. Furthermore, we discuss how Jolt Atlas achieves zero-knowledge through the BlindFold technique, as introduced in Vega. In contrast to existing zkML frameworks, we show practical proving times for classification, embedding, automated reasoning, and small language models. Jolt Atlas enables cryptographic verification that can be run on-device, without specialised hardware. The resulting proofs are succinctly verifiable. This makes Jolt Atlas well-suited for privacy-centric and adversarial environments. In a companion work, we outline various use cases of Jolt Atlas, including how it serves as guardrails in agentic commerce and for trustless AI context (often referred to as \\textit{AI memory}).",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17452"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "12ae408af7f6ac48f59f8561a3a2725918d4e89dbab379ed5a5284bbb64988f1",
      "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
      "url": "https://arxiv.org/abs/2602.17483",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17483v1 Announce Type: cross Abstract: Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative studies (N=20), and run two studies with EU residents to capture (i) intuitions about LLM-generated PD (N1=155) and (ii) reactions to tool output (N2=303). We show empirically that models confidently generate multiple PD categories for well-known individuals. For everyday users, GPT-4o generates 11 features with 60% or more accuracy (e.g., gender, hair color, languages). Finally, 72% of participants sought control over model-generated associations with their name, raising questions about what counts as PD and whether data privacy rights should extend to LLMs.",
      "tags": [
        "papers",
        "agents",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17483"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6faae19001c6b29130444add5f79be5f717c485a7f55261434be5288781c2918",
      "title": "Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection",
      "url": "https://arxiv.org/abs/2602.17484",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17484v1 Announce Type: cross Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.",
      "tags": [
        "papers",
        "training",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17484"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5142b66d51084deed618e026377ddab61ede22bd0717004f99cd724ef5ce5e35",
      "title": "Learning with Boolean threshold functions",
      "url": "https://arxiv.org/abs/2602.17493",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17493v1 Announce Type: cross Abstract: We develop a method for training neural networks on Boolean data in which the values at all nodes are strictly $\\pm 1$, and the resulting models are typically equivalent to networks whose nonzero weights are also $\\pm 1$. The method replaces loss minimization with a nonconvex constraint formulation. Each node implements a Boolean threshold function (BTF), and training is expressed through a divide-and-concur decomposition into two complementary constraints: one enforces local BTF consistency between inputs, weights, and output; the other imposes architectural concurrence, equating neuron outputs with downstream inputs and enforcing weight equality across training-data instantiations of the network. The reflect-reflect-relax (RRR) projection algorithm is used to reconcile these constraints. Each BTF constraint includes a lower bound on the margin. When this bound is sufficiently large, the learned representations are provably sparse and equivalent to networks composed of simple logical gates with $\\pm 1$ weights. Across a range of tasks -- including multiplier-circuit discovery, binary autoencoding, logic-network inference, and cellular automata learning -- the method achieves exact solutions or strong generalization in regimes where standard gradient-based methods struggle. These results demonstrate that projection-based constraint satisfaction provides a viable and conceptually distinct foundation for learning in discrete neural systems, with implications for interpretability and efficient inference.",
      "tags": [
        "papers",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17493"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8aef843e3e3fa49ca8d83751e58d2193fbb13f2fb13fd41fef8902609225a04a",
      "title": "LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights",
      "url": "https://arxiv.org/abs/2602.17510",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17510v1 Announce Type: cross Abstract: We introduce CRAFT (Cross-layer Rank Adaptation via Frozen Tucker), a parameter-efficient fine-tuning (PEFT) method that applies Tucker tensor decomposition to pre-trained attention weight matrices stacked across transformer layers and trains only small square adaptation matrices on the resulting frozen Tucker factors. Existing tensor-based PEFT methods decompose gradient updates: LoTR applies Tucker decomposition with shared factor matrices, while SuperLoRA groups and reshapes $\\Delta W$ across layers before applying Tucker decomposition. Separately, methods like PiSSA apply SVD to pre-trained weights but operate independently per layer. CRAFT bridges these two lines of work: it performs full Tucker decomposition via Higher-Order SVD (HOSVD) directly on pre-trained weights organized as cross-layer 3D tensors, freezes all resulting factors, and adapts the model through lightweight trainable transformations applied to each factor matrix. Experiments on the GLUE benchmark using RoBERTa-base and RoBERTa-large demonstrate that CRAFT achieves competitive performance with existing methods while requiring only 41K Tucker adaptation parameters--a count independent of model dimension and depth at fixed Tucker ranks.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17510"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3ba07fc0b05dc8bbf0afaa6c2e4c047ec453dc1994bbfdf46772986e05873956",
      "title": "The Anxiety of Influence: Bloom Filters in Transformer Attention Heads",
      "url": "https://arxiv.org/abs/2602.17526",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17526v1 Announce Type: cross Abstract: Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question \"has this token appeared before in the context?\" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\\% even at 180 unique context tokens -- well above the $d_\\text{head} = 64$ bit capacity of a classical Bloom filter. A third head (L1H11) shows the classic Bloom filter capacity curve: its false positive rate follows the theoretical formula $p \\approx (1 - e^{-kn/m})^k$ with $R^2 = 1.0$ and fitted capacity $m \\approx 5$ bits, saturating by $n \\approx 20$ unique tokens. A fourth head initially identified as a Bloom filter (L3H0) was reclassified as a general prefix-attention head after confound controls revealed its apparent capacity curve was a sequence-length artifact. Together, the three genuine membership-testing heads form a multi-resolution system concentrated in early layers (0-1), taxonomically distinct from induction and previous-token heads, with false positive rates that decay monotonically with embedding distance -- consistent with distance-sensitive Bloom filters. These heads generalize broadly: they respond to any repeated token type, not just repeated names, with 43\\% higher generalization than duplicate-token-only heads. Ablation reveals these heads contribute to both repeated and novel token processing, indicating that membership testing coexists with broader computational roles. The reclassification of L3H0 through confound controls strengthens rather than weakens the case: the surviving heads withstand the scrutiny that eliminated a false positive in our own analysis.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17526"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "77cb613e5dbb5fa54a1dfbded3c36f1ca59a338b43fba104c603f516157da755",
      "title": "Position: Evaluation of ECG Representations Must Be Fixed",
      "url": "https://arxiv.org/abs/2602.17531",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17531v1 Announce Type: cross Abstract: This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model. We substantiate our observations with an empirical evaluation of three representative ECG pre-training approaches across six evaluation settings: the three standard benchmarks, a structural disease dataset, hemodynamic inference, and patient forecasting.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17531"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "493cbe973865b8ded8ea77bf06c5fdd4ae656897bb878b8b7501787aef91e820",
      "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
      "url": "https://arxiv.org/abs/2602.17532",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17532v1 Announce Type: cross Abstract: We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incremental value for perturbation prediction: trivial gene-level baselines outperform both attention and correlation edges (AUROC 0.81-0.88 versus 0.70), pairwise edge scores add zero predictive contribution, and causal ablation of regulatory heads produces no degradation. These findings generalise from K562 to RPE1 cells; the attention-correlation relationship is context-dependent, but gene-level dominance is universal. Cell-State Stratified Interpretability (CSSI) addresses an attention-specific scaling failure, improving GRN recovery up to 1.85x. The framework establishes reusable quality-control standards for the field.",
      "tags": [
        "papers",
        "eval",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17532"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c8d060a3d79734029d76e957bd6c33c82348eea010f263b644ed637192439ce1",
      "title": "Toward a Fully Autonomous, AI-Native Particle Accelerator",
      "url": "https://arxiv.org/abs/2602.17536",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17536v1 Announce Type: cross Abstract: This position paper presents a vision for self-driving particle accelerators that operate autonomously with minimal human intervention. We propose that future facilities be designed through artificial intelligence (AI) co-design, where AI jointly optimizes the accelerator lattice, diagnostics, and science application from inception to maximize performance while enabling autonomous operation. Rather than retrofitting AI onto human-centric systems, we envision facilities designed from the ground up as AI-native platforms. We outline nine critical research thrusts spanning agentic control architectures, knowledge integration, adaptive learning, digital twins, health monitoring, safety frameworks, modular hardware design, multimodal data fusion, and cross-domain collaboration. This roadmap aims to guide the accelerator community toward a future where AI-driven design and operation deliver unprecedented science output and reliability.",
      "tags": [
        "papers",
        "agents",
        "safety"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17536"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e2cf0f3ab4f64035f1914ac8c63d2642a22d126605a4c97b0e2bf93b3015c576",
      "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2602.17550",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17550v1 Announce Type: cross Abstract: Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17550"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6bbf9b47d27f8155a3afbe5f6a664d967fc470ff540501a5b070c9b28a9ba290",
      "title": "Probability-Invariant Random Walk Learning on Gyral Folding-Based Cortical Similarity Networks for Alzheimer's and Lewy Body Dementia Diagnosis",
      "url": "https://arxiv.org/abs/2602.17557",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17557v1 Announce Type: cross Abstract: Alzheimer's disease (AD) and Lewy body dementia (LBD) present overlapping clinical features yet require distinct diagnostic strategies. While neuroimaging-based brain network analysis is promising, atlas-based representations may obscure individualized anatomy. Gyral folding-based networks using three-hinge gyri provide a biologically grounded alternative, but inter-individual variability in cortical folding results in inconsistent landmark correspondence and highly irregular network sizes, violating the fixed-topology and node-alignment assumptions of most existing graph learning methods, particularly in clinical datasets where pathological changes further amplify anatomical heterogeneity. We therefore propose a probability-invariant random-walk-based framework that classifies individualized gyral folding networks without explicit node alignment. Cortical similarity networks are built from local morphometric features and represented by distributions of anonymized random walks, with an anatomy-aware encoding that preserves permutation invariance. Experiments on a large clinical cohort of AD and LBD subjects show consistent improvements over existing gyral folding and atlas-based models, demonstrating robustness and potential for dementia diagnosis.",
      "tags": [
        "papers",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17557"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "883218a7357c428c37457c52f26b6ab758bb54b9cb99004700b730c7b697b5a4",
      "title": "Be Wary of Your Time Series Preprocessing",
      "url": "https://arxiv.org/abs/2602.17568",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17568v1 Announce Type: cross Abstract: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17568"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "98d249d72a1d1546c8607a97d9362b1b763eb204a6b8f29a14e2a0aa3b155e5e",
      "title": "Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space",
      "url": "https://arxiv.org/abs/2602.17586",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17586v1 Announce Type: cross Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.",
      "tags": [
        "papers",
        "safety",
        "training",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17586"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d20795660b042c6dd0c3a9ef0df4873c556c104a2e2cd6a285c603dad67b83ac",
      "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
      "url": "https://arxiv.org/abs/2602.17598",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17598v1 Announce Type: cross Abstract: Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($\\kappa{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17598"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9653b85e445b5f4d77b29353a13a6f997ed72c07bfbd60e8e2632786b483f336",
      "title": "Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery",
      "url": "https://arxiv.org/abs/2602.17605",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17605v1 Announce Type: cross Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.",
      "tags": [
        "papers",
        "reasoning",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17605"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "62b585ae881bc46961668968ec17678d379215bdd99f7a542f9a7a5b80fe1e16",
      "title": "Towards Anytime-Valid Statistical Watermarking",
      "url": "https://arxiv.org/abs/2602.17608",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17608v1 Announce Type: cross Abstract: The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17608"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f3dce32a817854513cf3833f6dfa573fbf1203eb402482da63be623f19e83975",
      "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
      "url": "https://arxiv.org/abs/2602.17616",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17616v1 Announce Type: cross Abstract: Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\\textbf{V}$ariance $\\textbf{C}$ontrolled $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17616"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1deba2de2c152bbf3cec5a53055b9178ad95895e4e5f959f6b627ebb8edcd8e0",
      "title": "SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer",
      "url": "https://arxiv.org/abs/2602.17632",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17632v1 Announce Type: cross Abstract: Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.",
      "tags": [
        "papers",
        "policy"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17632"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "69a7dd57bcdd089d6cd307b35eab938594295784393a5508864e579425d8d91e",
      "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning",
      "url": "https://arxiv.org/abs/2602.17633",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17633v1 Announce Type: cross Abstract: Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17633"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "892a8619a52b843ef397458b88c1ba4f9bebb3c5a354279443d603db2295e4cd",
      "title": "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting",
      "url": "https://arxiv.org/abs/2602.17634",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17634v1 Announce Type: cross Abstract: Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.",
      "tags": [
        "papers",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17634"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bc121cbde7939da54a677efbf16728a1f666cd6f0910c28a23350a31bc96c3bf",
      "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
      "url": "https://arxiv.org/abs/2602.17641",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17641v1 Announce Type: cross Abstract: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.",
      "tags": [
        "papers",
        "agents",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17641"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7f0d18abd38cdaee1a7dad514dc7e3ee05080961ce3a8388448ba762f5bf77e1",
      "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting",
      "url": "https://arxiv.org/abs/2602.17645",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17645v1 Announce Type: cross Abstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.",
      "tags": [
        "papers",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17645"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "911ba67e202780aee416dbdc05c04377a61274b9de400cbd91f0d2d221b3fb88",
      "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement",
      "url": "https://arxiv.org/abs/2602.17658",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17658v1 Announce Type: cross Abstract: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.",
      "tags": [
        "papers",
        "safety",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17658"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b692f1874af03e23bb33f6efd9554efc325d73ac399ae814472caa8de0f6bd8e",
      "title": "Sink-Aware Pruning for Diffusion Language Models",
      "url": "https://arxiv.org/abs/2602.17664",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17664v1 Announce Type: cross Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\\bf \\texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.",
      "tags": [
        "papers",
        "compute",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17664"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a971e135c9f28215d9ee2ecde0e9e168396d70ce806741118dba553674a7c9d4",
      "title": "Goal Inference from Open-Ended Dialog",
      "url": "https://arxiv.org/abs/2410.13957",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2410.13957v2 Announce Type: replace Abstract: Embodied AI Agents are quickly becoming important and common tools in society. These embodied agents should be able to learn about and accomplish a wide range of user goals and preferences efficiently and robustly. Large Language Models (LLMs) are often used as they allow for opportunities for rich and open-ended dialog type interaction between the human and agent to accomplish tasks according to human preferences. In this thesis, we argue that for embodied agents that deal with open-ended dialog during task assistance: 1) AI Agents should extract goals from conversations in the form of Natural Language (NL) to be better at capturing human preferences as it is intuitive for humans to communicate their preferences on tasks to agents through natural language. 2) AI Agents should quantify/maintain uncertainty about these goals to ensure that actions are being taken according to goals that the agent is extremely certain about. We present an online method for embodied agents to learn and accomplish diverse user goals. While offline methods like RLHF can represent various goals but require large datasets, our approach achieves similar flexibility with online efficiency. We extract natural language goal representations from conversations with Large Language Models (LLMs). We prompt an LLM to role play as a human with different goals and use the corresponding likelihoods to run Bayesian inference over potential goals. As a result, our method can represent uncertainty over complex goals based on unrestricted dialog. We evaluate in a text-based grocery shopping domain and an AI2Thor robot simulation. We compare our method to ablation baselines that lack either explicit goal representation or probabilistic inference.",
      "tags": [
        "papers",
        "agents",
        "inference",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2410.13957"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a049a021b264a5955fa96ff62cf7e8426dfe330582e223e50b5a2e00c715f32c",
      "title": "GAI: Generative Agents for Innovation",
      "url": "https://arxiv.org/abs/2412.18899",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2412.18899v3 Announce Type: replace Abstract: This study examines whether collective reasoning among generative agents can facilitate novel and coherent thinking that leads to innovation. To achieve this, it proposes GAI, a new LLM-empowered framework designed for reflection and interaction among multiple generative agents to replicate the process of innovation. The core of the GAI framework lies in an architecture that dynamically processes the internal states of agents and a dialogue scheme specifically tailored to facilitate analogy-driven innovation. The framework's functionality is evaluated using Dyson's invention of the bladeless fan as a case study, assessing the extent to which the core ideas of the innovation can be replicated through a set of fictional technical documents. The experimental results demonstrate that models with internal states significantly outperformed those without, achieving higher average scores and lower variance. Notably, the model with five heterogeneous agents equipped with internal states successfully replicated the key ideas underlying the Dyson's invention. This indicates that the internal state enables agents to refine their ideas, resulting in the construction and sharing of more coherent and comprehensive concepts.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2412.18899"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "511973b3e86922c17a7e65b21f6c260e00e53db785ea541370a468ddc848e1d9",
      "title": "AI-Assisted Decision Making with Human Learning",
      "url": "https://arxiv.org/abs/2502.13062",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2502.13062v2 Announce Type: replace Abstract: AI systems increasingly support human decision-making. In many cases, despite the algorithm's superior performance, the final decision remains in human hands. For example, an AI may assist doctors in determining which diagnostic tests to run, but the doctor ultimately makes the diagnosis. This paper studies such AI-assisted decision-making settings, where the human learns through repeated interactions with the algorithm. In our framework, the algorithm -- designed to maximize decision accuracy according to its own model -- determines which features the human can consider. The human then makes a prediction based on their own less accurate model. We observe that the discrepancy between the algorithm's model and the human's model creates a fundamental tradeoff: Should the algorithm prioritize recommending more informative features, encouraging the human to learn their importance, even if it results in less accurate predictions in the short term until learning occurs? Or is it preferable to forgo educating the human and instead select features that align more closely with their existing understanding, minimizing the immediate cost of learning? Our analysis reveals how this trade-off is shaped by both the algorithm's patience (the time-discount rate of its objective over multiple periods) and the human's willingness and ability to learn. We show that optimal feature selection has a surprisingly clean combinatorial characterization, reducible to a stationary sequence of feature subsets that is tractable to compute. As the algorithm becomes more \"patient\" or the human's learning improves, the algorithm increasingly selects more informative features, enhancing both prediction accuracy and the human's understanding.",
      "tags": [
        "papers",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2502.13062"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f991105f033da978e175effabeeb988c8386a85b3d9759dc34ab98bf87fa9a82",
      "title": "Capturing Individual Human Preferences with Reward Features",
      "url": "https://arxiv.org/abs/2503.17338",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2503.17338v2 Announce Type: replace Abstract: Reinforcement learning from human feedback usually models preferences using a reward function that does not distinguish between people. We argue that this is unlikely to be a good design choice in contexts with high potential for disagreement, like in the training of large language models. We formalise and analyse the problem of learning a reward model that can be specialised to a user. Using the principle of empirical risk minimisation, we derive a probably approximately correct (PAC) bound showing the dependency of the approximation error on the number of training examples, as usual, and also on the number of human raters who provided feedback on them. Based on our theoretical findings, we discuss how to best collect pairwise preference data and argue that adaptive reward models should be beneficial when there is considerable disagreement among users. We also propose a concrete architecture for an adaptive reward model. Our approach leverages the observation that individual preferences can be captured as a linear combination of a set of general reward features. We show how to learn such features and subsequently use them to quickly adapt the reward model to a specific individual, even if their preferences are not reflected in the training data. We present experiments with large language models illustrating our theoretical results and comparing the proposed architecture with a non-adaptive baseline. Consistent with our analysis, the benefits provided by our model increase with the number of raters and the heterogeneity of their preferences. We also show that our model compares favourably to adaptive counterparts, including those performing in-context personalisation.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2503.17338"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2fae2dae52ac3541e93d877a5c0aadfc4dfe1ae2bfa7ed51f4ca8bf0d5f34ac2",
      "title": "A Scalable Framework for Evaluating Health Language Models",
      "url": "https://arxiv.org/abs/2503.23339",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2503.23339v3 Announce Type: replace Abstract: Large language models (LLMs) have emerged as powerful tools for analyzing complex datasets. Recent studies demonstrate their potential to generate useful, personalized responses when provided with patient-specific health information that encompasses lifestyle, biomarkers, and context. As LLM-driven health applications are increasingly adopted, rigorous and efficient one-sided evaluation methodologies are crucial to ensure response quality across multiple dimensions, including accuracy, personalization and safety. Current evaluation practices for open-ended text responses heavily rely on human experts. This approach introduces human factors and is often cost-prohibitive, labor-intensive, and hinders scalability, especially in complex domains like healthcare where response assessment necessitates domain expertise and considers multifaceted patient data. In this work, we introduce Adaptive Precise Boolean rubrics: an evaluation framework that streamlines human and automated evaluation of open-ended questions by identifying gaps in model responses using a minimal set of targeted rubrics questions. Our approach is based on recent work in more general evaluation settings that contrasts a smaller set of complex evaluation targets with a larger set of more precise, granular targets answerable with simple boolean responses. We validate this approach in metabolic health, a domain encompassing diabetes, cardiovascular disease, and obesity. Our results demonstrate that Adaptive Precise Boolean rubrics yield higher inter-rater agreement among expert and non-expert human evaluators, and in automated assessments, compared to traditional Likert scales, while requiring approximately half the evaluation time of Likert-based methods. This enhanced efficiency, particularly in automated evaluation and non-expert contributions, paves the way for more extensive and cost-effective evaluation of LLMs in health.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2503.23339"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "40d238372b9d7cd22e264a6a68317ea9d958c3c99b3cb9373e67d937034c9d7e",
      "title": "The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic",
      "url": "https://arxiv.org/abs/2505.08021",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.08021v4 Announce Type: replace Abstract: Graph Neural Networks (GNNs) address two key challenges in applying deep learning to graph-structured data: they handle varying size input graphs and ensure invariance under graph isomorphism. While GNNs have demonstrated broad applicability, understanding their expressive power remains an important question. In this paper, we propose GNN architectures that correspond precisely to prominent fragments of first-order logic (FO), including various modal logics as well as more expressive two-variable fragments. To establish these results, we apply methods from finite model theory of first-order and modal logics to the domain of graph representation learning. Our results provide a unifying framework for understanding the logical expressiveness of GNNs within FO.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.08021"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "556663c9c6a41d473a3806b6dee48d629752d2cfaeb96921098fc56d24f24672",
      "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning",
      "url": "https://arxiv.org/abs/2505.16928",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.16928v3 Announce Type: replace Abstract: We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions. Our work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.16928"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b186f67fe67f8f068123c7f275c01084ab9dfb4ec663f4acb25584d91d4faed9",
      "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts",
      "url": "https://arxiv.org/abs/2506.15733",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.15733v2 Announce Type: replace Abstract: Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $\\texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $\\sim$19.1\\%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.",
      "tags": [
        "papers",
        "reasoning",
        "compute",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.15733"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "33c0e109d258cb7220a6a5a34aed79debd9c0ec4ff40963d7312faa81efa4133",
      "title": "Sufficient, Necessary and Complete Causal Explanations in Image Classification",
      "url": "https://arxiv.org/abs/2507.23497",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2507.23497v2 Announce Type: replace Abstract: Existing algorithms for explaining the outputs of image classifiers are based on a variety of approaches and produce explanations that frequently lack formal rigour. On the other hand, logic-based explanations are formally and rigorously defined but their computability relies on strict assumptions about the model that do not hold on image classifiers. In this paper, we show that causal explanations, in addition to being formally and rigorously defined, enjoy the same formal properties as logic-based ones, while still lending themselves to black-box algorithms and being a natural fit for image classifiers. We prove formal properties of causal explanations and their equivalence to logic-based explanations. We demonstrate how to subdivide an image into its sufficient and necessary components. We introduce $\\delta$-complete explanations, which have a minimum confidence threshold and 1-complete causal explanations, explanations that are classified with the same confidence as the original image. We implement our definitions, and our experimental results demonstrate that different models have different patterns of sufficiency, necessity, and completeness. Our algorithms are efficiently computable, taking on average 6s per image on a ResNet model to compute all types of explanations, and are totally black-box, needing no knowledge of the model, no access to model internals, no access to gradient, nor requiring any properties, such as monotonicity, of the model.",
      "tags": [
        "papers",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2507.23497"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1ed5d8e2174d80d69fc872b89761a66f243d4b66eee8d5167eac957db684d183",
      "title": "Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems",
      "url": "https://arxiv.org/abs/2508.12026",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2508.12026v2 Announce Type: replace Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual reasoning (AVR), requiring models to identify visual concepts fromjust a few examples and describe them in natural language. Early BP benchmarks featured synthetic black-and-white drawings, which might not fully capture the complexity of real-world scenes. Subsequent BP datasets employed real-world images, albeit the represented concepts are identifiable from high-level image features, reducing the task complexity. Differently, the recently released Bongard-RWR dataset aimed at representing abstract concepts formulated in the original BPs using fine-grained real-world images. Its manual construction, however, limited the dataset size to just $60$ instances, constraining evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset composed of $5\\,400$ instances that represent original BP abstract concepts using real-world-like images generated via a vision language model (VLM) pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually curated images and generate new descriptions aligned with the underlying concepts, use Flux.1-dev to synthesize images from these descriptions, and manually verify that the generated images faithfully reflect the intended concepts. We evaluate state-of-the-art VLMs across diverse BP formulations, including binary and multiclass classification, as well as textual answer generation. Our findings reveal that while VLMs can recognize coarse-grained visual concepts, they consistently struggle with discerning fine-grained concepts, highlighting limitations in their reasoning capabilities.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "training",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2508.12026"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7eead53f9cd285d063e106b76ecd8866cad3d778b47685ad0d2e35881c9d90ff",
      "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI",
      "url": "https://arxiv.org/abs/2510.00167",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.00167v2 Announce Type: replace Abstract: Autonomous drones must often respond to sudden events, such as alarms, faults, or unexpected changes in their environment, that require immediate and adaptive decision-making. Traditional approaches rely on safety engineers hand-coding large sets of recovery rules, but this strategy cannot anticipate the vast range of real-world contingencies and quickly becomes incomplete. Recent advances in embodied AI, powered by large visual language models, provide commonsense reasoning to assess context and generate appropriate actions in real time. We demonstrate this capability in a simulated urban benchmark in the Unreal Engine, where drones dynamically interpret their surroundings and decide on sudden maneuvers for safe landings. Our results show that embodied AI makes possible a new class of adaptive recovery and decision-making pipelines that were previously infeasible to design by hand, advancing resilience and safety in autonomous aerial systems.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.00167"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "98afe6e5299cb5d768a14ab98e12c1ec349650da0cecf2f57a07ed06fd66b13c",
      "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents",
      "url": "https://arxiv.org/abs/2510.19771",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.19771v3 Announce Type: replace Abstract: LLM-based agents are increasingly moving towards proactivity: rather than awaiting instruction, they exercise agency to anticipate user needs and solve them autonomously. However, evaluating proactivity is challenging; current benchmarks are constrained to localized context, limiting their ability to test reasoning across sources and longer time horizons. To address this gap, we present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes proactivity as a pipeline of three core capabilities: (1) searching for unspecified issues, (2) identifying specific bottlenecks, and (3) executing appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular agentic frameworks, showing that even state-of-the-art models struggle to solve this benchmark. Computing our consistent measurements across frontier LLMs and agents, we find that the best end-to-end performance of 40% is achieved by both GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative capabilities of each model and analyze mutual failure modes. Our results highlight the current limitations of autonomous action in agentic systems, and expose promising future research directions.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.19771"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5bd287468e1e69eafe2337e94887da658eae24f8aa806f8840543e42d1ae996d",
      "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer",
      "url": "https://arxiv.org/abs/2511.17673",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.17673v5 Announce Type: replace Abstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Soft Symbolic Control constitutes a dedicated governance layer within SCL, applying symbolic constraints to probabilistic inference while preserving the flexibility of neural reasoning and restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "policy",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.17673"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "aec9ef571b38e66475025f98aa6ba319eb1d1375c5051e04e998b9e0f6adc99c",
      "title": "Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.07463",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2601.07463v2 Announce Type: replace Abstract: Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "policy",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.07463"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "40614e140c61dcd48e0f05bebd3c6d7d3774f74dee12296044ca0845c0eea876",
      "title": "Autonomous Business System via Neuro-symbolic AI",
      "url": "https://arxiv.org/abs/2601.15599",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2601.15599v2 Announce Type: replace Abstract: Current business environments demand continuous reconfiguration of cross-functional processes, yet enterprise systems remain organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile, large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic and verifiable execution of complex business logic. We introduce Autonomous Business System (AUTOBUS), a system that combines LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic architecture for executing end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre- and post-conditions, required data, evaluation rules, and API-level actions. Enterprise data is represented as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing semantic grounding for reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs executed by a logic engine that enforces constraints and orchestrates actions. Humans define semantics and policies, curate tools, and oversee high-impact or ambiguous decisions. We present the AUTOBUS architecture and a case study that demonstrates accelerated time to market in a data-rich organization. A reference implementation of the case study is available at https://github.com/cecilpang/autobus-paper.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.15599"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "65cf98cd78fab2bd0fed819619231396894563c0df791e18192a86e880778460",
      "title": "Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection",
      "url": "https://arxiv.org/abs/2601.19245",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2601.19245v5 Announce Type: replace Abstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs' initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.19245"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "568ef2e87148cc511f2c30f2f5699de8fb94a873eb4785a44ddd37f5f77830df",
      "title": "Autonomous Data Processing using Meta-Agents",
      "url": "https://arxiv.org/abs/2602.00307",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.00307v2 Announce Type: replace Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \\textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \\textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \\textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.",
      "tags": [
        "papers",
        "agents",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.00307"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "343fa635f82640caa8ed308494cf482be327c2776a55011afb9e07dce961785e",
      "title": "An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization",
      "url": "https://arxiv.org/abs/2602.06838",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.06838v2 Announce Type: replace Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.06838"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a54f1659b51da26041ef31d0b58ded7b2a186286076792b409a6533139b7728f",
      "title": "Defining and Evaluating Physical Safety for Large Language Models",
      "url": "https://arxiv.org/abs/2411.02317",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2411.02317v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are increasingly used to control robotic systems such as drones, but their risks of causing physical threats and harm in real-world applications remain unexplored. Our study addresses the critical gap in evaluating LLM physical safety by developing a comprehensive benchmark for drone control. We classify the physical safety risks of drones into four categories: (1) human-targeted threats, (2) object-targeted threats, (3) infrastructure attacks, and (4) regulatory violations. Our evaluation of mainstream LLMs reveals an undesirable trade-off between utility and safety, with models that excel in code generation often performing poorly in crucial safety aspects. Furthermore, while incorporating advanced prompt engineering techniques such as In-Context Learning and Chain-of-Thought can improve safety, these methods still struggle to identify unintentional attacks. In addition, larger models demonstrate better safety capabilities, particularly in refusing dangerous commands. Our findings and benchmark can facilitate the design and evaluation of physical safety for LLMs. The project page is available at huggingface.co/spaces/TrustSafeAI/LLM-physical-safety.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2411.02317"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7f74aba497ab42d925583b868e9f1b8ae625a32369a1ba2f356669b3a0fe8ae5",
      "title": "Multi-View 3D Reconstruction using Knowledge Distillation",
      "url": "https://arxiv.org/abs/2412.02039",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2412.02039v2 Announce Type: replace-cross Abstract: Large Foundation Models like Dust3r can produce high quality outputs such as pointmaps, camera intrinsics, and depth estimation, given stereo-image pairs as input. However, the application of these outputs on tasks like Visual Localization requires a large amount of inference time and compute resources. To address these limitations, in this paper, we propose the use of a knowledge distillation pipeline, where we aim to build a student-teacher model with Dust3r as the teacher and explore multiple architectures of student models that are trained using the 3D reconstructed points output by Dust3r. Our goal is to build student models that can learn scene-specific representations and output 3D points with replicable performance such as Dust3r. The data set we used to train our models is 12Scenes. We test two main architectures of models: a CNN-based architecture and a Vision Transformer based architecture. For each architecture, we also compare the use of pre-trained models against models built from scratch. We qualitatively compare the reconstructed 3D points output by the student model against Dust3r's and discuss the various features learned by the student model. We also perform ablation studies on the models through hyperparameter tuning. Overall, we observe that the Vision Transformer presents the best performance visually and quantitatively.",
      "tags": [
        "papers",
        "compute",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2412.02039"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9a7b09e7b784baf1424cf990bb0c1a8073204681d144df011faac70ecbcccce3",
      "title": "Point-DeepONet: Predicting Nonlinear Fields on Non-Parametric Geometries under Variable Load Conditions",
      "url": "https://arxiv.org/abs/2412.18362",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2412.18362v2 Announce Type: replace-cross Abstract: Nonlinear structural analyses in engineering often require extensive finite element simulations, limiting their applicability in design optimization and real-time control. Conventional deep learning surrogates often struggle with complex, non-parametric three-dimensional (3D) geometries and directionally varying loads. This work presents Point-DeepONet, an operator-learning-based surrogate that integrates PointNet into the DeepONet framework to learn a mapping from non-parametric geometries and variable load conditions to physical response fields. By leveraging PointNet to learn a geometric representation from raw point clouds, our model circumvents the need for manual parameterization. This geometric embedding is then synergistically fused with load conditions within the DeepONet architecture to accurately predict three-dimensional displacement and von Mises stress fields. Trained on a large-scale dataset, Point-DeepONet demonstrates high fidelity, achieving a coefficient of determination (R^2) reaching 0.987 for displacement and 0.923 for von Mises stress. Furthermore, to rigorously validate its generalization capabilities, we conducted additional experiments on unseen, randomly oriented load directions, where the model maintained exceptional accuracy. Compared to nonlinear finite element analyses that require about 19.32 minutes per case, Point-DeepONet provides predictions in mere seconds--approximately 400 times faster--while maintaining excellent scalability. These findings, validated through extensive experiments and ablation studies, highlight the potential of Point-DeepONet to enable rapid, high-fidelity structural analyses for complex engineering workflows.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2412.18362"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "28a1741cd9375281b11af5abf8d5e73ca32fd8b8ac14f2e2693cc9f00f1a7e15",
      "title": "Self-Improving Skill Learning for Robust Skill-based Meta-Reinforcement Learning",
      "url": "https://arxiv.org/abs/2502.03752",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2502.03752v4 Announce Type: replace-cross Abstract: Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, leading to unstable skill learning and degraded performance. To address this, we propose Self-Improving Skill Learning (SISL), which performs self-guided skill refinement using decoupled high-level and skill improvement policies, while applying skill prioritization via maximum return relabeling to focus updates on task-relevant trajectories, resulting in robust and stable adaptation even under noisy and suboptimal data. By mitigating the effect of noise, SISL achieves reliable skill learning and consistently outperforms other skill-based meta-RL methods on diverse long-horizon tasks. Our code is available at https://github.com/epsilog/SISL.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2502.03752"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ae742e4deea5b86df51ed349a323eea642ba5b8d83bdfb1134495458bfb8b070",
      "title": "Rex: A Family of Reversible Exponential (Stochastic) Runge-Kutta Solvers",
      "url": "https://arxiv.org/abs/2502.08834",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2502.08834v3 Announce Type: replace-cross Abstract: Deep generative models based on neural differential equations have quickly become the state-of-the-art for numerous generation tasks across many different applications. These models rely on ODE/SDE solvers which integrate from a prior distribution to the data distribution. In many applications it is highly desirable to then integrate in the other direction. The standard solvers, however, accumulate discretization errors which don't align with the forward trajectory, thereby prohibiting an exact inversion. In applications where the precision of the generative model is paramount this inaccuracy in inversion is often unacceptable. Current approaches to solving the inversion of these models results in significant downstream issues with poor stability and low-order of convergence; moreover, they are strictly limited to the ODE domain. In this work, we propose a new family of reversible exponential (stochastic) Runge-Kutta solvers which we refer to as Rex developed by an application of Lawson methods to convert any explicit (stochastic) Runge-Kutta scheme into a reversible one. In addition to a rigorous theoretical analysis of the proposed solvers, we also empirically demonstrate the utility of Rex on improving the sampling of Boltzmann distributions with flow models, and improving image generation and editing capabilities with diffusion models.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2502.08834"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bdcf4f092c2d1157fe2541dc8e8e3a53b5d3f0bb769c3bffbb455da10ccf1226",
      "title": "Simple Self Organizing Map with Vision Transformers",
      "url": "https://arxiv.org/abs/2503.04121",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2503.04121v4 Announce Type: replace-cross Abstract: Vision Transformers (ViTs) have demonstrated exceptional performance in various vision tasks. However, they tend to underperform on smaller datasets due to their inherent lack of inductive biases. Current approaches address this limitation implicitly-often by pairing ViTs with pretext tasks or by distilling knowledge from convolutional neural networks (CNNs) to strengthen the prior. In contrast, Self-Organizing Maps (SOMs), a widely adopted self-supervised framework, are inherently structured to preserve topology and spatial organization, making them a promising candidate to directly address the limitations of ViTs in limited or small training datasets. Despite this potential, equipping SOMs with modern deep learning architectures remains largely unexplored. In this study, we conduct a novel exploration on how Vision Transformers (ViTs) and Self-Organizing Maps (SOMs) can empower each other, aiming to bridge this critical research gap. Our findings demonstrate that these architectures can synergistically enhance each other, leading to significantly improved performance in both unsupervised and supervised tasks. Code is publicly available on GitHub.",
      "tags": [
        "papers",
        "training",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2503.04121"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8f48d320dc910962fab7cb07b690fedf642455f26559e44534732ae2abcfacbd",
      "title": "Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises",
      "url": "https://arxiv.org/abs/2504.21730",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2504.21730v2 Announce Type: replace-cross Abstract: Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample's certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at https://github.com/NcepuQiaoTing/Cert-SSB.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2504.21730"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0a56c99b9cdc2813995c7d35c2313a432d90a1d3fb313b9ca1b9f6ef63784594",
      "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
      "url": "https://arxiv.org/abs/2505.02819",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.02819v4 Announce Type: replace-cross Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25\\% pruning while retaining approximately 90\\% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead. We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe",
      "tags": [
        "papers",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.02819"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2d5ab7f70b46d97a22d04846340bc301d57c4704d3468b46c2b22f006690b122",
      "title": "Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans",
      "url": "https://arxiv.org/abs/2505.12298",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.12298v2 Announce Type: replace-cross Abstract: In this study, we propose a robust methodology for automatic segmentation of infected lung regions in COVID-19 CT scans using convolutional neural networks. The approach is based on a modified U-Net architecture enhanced with attention mechanisms, data augmentation, and postprocessing techniques. It achieved a Dice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods. The dataset was sourced from public repositories and augmented for diversity. Results demonstrate superior segmentation performance. Future work includes expanding the dataset, exploring 3D segmentation, and preparing the model for clinical deployment.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.12298"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "42c59cd9140b64482b8a6dadfa020d93a999bb285135ea491be7c807a6a9e5f6",
      "title": "Oversmoothing, Oversquashing, Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning",
      "url": "https://arxiv.org/abs/2505.15547",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.15547v3 Announce Type: replace-cross Abstract: After a renaissance phase in which researchers revisited the message-passing paradigm through the lens of deep learning, the graph machine learning community shifted its attention towards a deeper and practical understanding of message-passing's benefits and limitations. In this paper, we notice how the fast pace of progress around the topics of oversmoothing and oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came with the consolidation of commonly accepted beliefs and assumptions -- under the form of universal statements -- that are not always true nor easy to distinguish from each other. We argue that this has led to ambiguities around the investigated problems, preventing researchers from focusing on and addressing precise research questions while causing a good amount of misunderstandings. Our contribution is to make such common beliefs explicit and encourage critical thinking around these topics, refuting universal statements via simple yet formally sufficient counterexamples. The end goal is to clarify conceptual differences, helping researchers address more clearly defined and targeted problems.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.15547"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6d47d0b5c1a06caef56f574660ff1079480c8e7180e422f6861da3b73ecb29f7",
      "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning",
      "url": "https://arxiv.org/abs/2505.17508",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.17508v4 Announce Type: replace-cross Abstract: Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). KL regularization is ubiquitous, yet the design surface, choice of KL direction (forward vs. reverse), normalization (normalized vs. unnormalized), and estimator ($k_1/k_2/k_3$), is scattered across the literature and often intertwined with off-policy estimation. We ask a focused question: under the off-policy setting, what weighting is required for each KL variant so that the surrogate we optimize yields the exact gradient of the intended KL-regularized objective? We answer this with a compact, unified derivation we call the Regularized Policy Gradient (RPG) view. RPG (i) unifies normalized and unnormalized KL variants and shows that the widely-used $k_3$ penalty is exactly the unnormalized KL; (ii) specifies conditions under which REINFORCE-style losses with stop-gradient are gradient-equivalent to fully differentiable surrogates; (iii) identifies and corrects an off-policy importance-weighting mismatch in GRPO's KL term; and (iv) introduces RPG-Style Clip, a clipped-importance-sampling step within RPG-REINFORCE that enables stable, off-policy policy-gradient training at scale. On mathematical reasoning benchmarks (AIME24, AIME25), RPG-REINFORCE with RPG-Style Clip improves accuracy by up to $+6$ absolute percentage points over DAPO. We extend our experiments to 8K context length, and RPG-REINFORCE with RPG-Style Clip achieves 52% accuracy on AIME25, surpassing the official Qwen3-4B-Instruct model (47%). Notably, RPG is a stable and scalable RL algorithm for LLM reasoning, realized via (a) a KL-correct objective, (b) clipped importance sampling, and (c) an iterative reference-policy update scheme. Project Page: https://github.com/complex-reasoning/RPG.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.17508"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a37b5c531258cb7b853d0b5fd22d06d391968bb729d4418f393060a1bdad9f2c",
      "title": "Explanation User Interfaces: A Systematic Literature Review",
      "url": "https://arxiv.org/abs/2505.20085",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.20085v2 Announce Type: replace-cross Abstract: Artificial Intelligence (AI) is one of the major technological advancements of this century, bearing incredible potential for users through AI-powered applications and tools in numerous domains. Being often black-box (i.e., its decision-making process is unintelligible), developers typically resort to eXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour of AI models to produce systems that are transparent, fair, reliable, and trustworthy. However, presenting explanations to the user is not trivial and is often left as a secondary aspect of the system's design process, leading to AI systems that are not useful to end-users. This paper presents a Systematic Literature Review on Explanation User Interfaces (XUIs) to gain a deeper understanding of the solutions and design guidelines employed in the academic literature to effectively present explanations to users. To improve the contribution and real-world impact of this survey, we also present a platform to support Human-cEnteRed developMent of Explainable user interfaceS (HERMES) and guide practitioners and scholars in the design and evaluation of XUIs.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.20085"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "dbb57e8898077fec2e7143c966d18b800857c52f6b2f66d47dfdccb9b2419a73",
      "title": "FinTagging: Benchmarking LLMs for Extracting and Structuring Financial Information",
      "url": "https://arxiv.org/abs/2505.20650",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.20650v4 Announce Type: replace-cross Abstract: Accurate interpretation of numerical data in financial reports is critical for markets and regulators. Although XBRL (eXtensible Business Reporting Language) provides a standard for tagging financial figures, mapping thousands of facts to over 10k US GAAP concepts remains costly and error prone. Existing benchmarks oversimplify this task as flat, single step classification over small subsets of concepts, ignoring the hierarchical semantics of the taxonomy and the structured nature of financial documents. Consequently, these benchmarks fail to evaluate Large Language Models (LLMs) under realistic reporting conditions. To bridge this gap, we introduce FinTagging, the first comprehensive benchmark for structure aware and full scope XBRL tagging. We decompose the complex tagging process into two subtasks: (1) FinNI (Financial Numeric Identification), which extracts entities and types from heterogeneous contexts including text and tables; and (2) FinCL (Financial Concept Linking), which maps extracted entities to the full US GAAP taxonomy. This two stage formulation enables a fair assessment of LLMs' capabilities in numerical reasoning and taxonomy alignment. Evaluating diverse LLMs in zero shot settings reveals that while models generalize well in extraction, they struggle significantly with fine grained concept linking, highlighting critical limitations in domain specific structure aware reasoning.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.20650"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0c21b6e405c06b088cde4b888cb8e824eae93f06f337d0acdfd2f97e05759025",
      "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs",
      "url": "https://arxiv.org/abs/2506.02529",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.02529v2 Announce Type: replace-cross Abstract: Web applications are critical to modern software ecosystems, yet ensuring their reliability remains challenging due to the complexity and dynamic nature of web interfaces. Recent advances in large language models (LLMs) have shown promise in automating complex tasks, but limitations persist in handling dynamic navigation flows and complex form interactions. This paper presents an automated system for generating test cases for two key aspects of web application testing: site navigation and form filling. For site navigation, the system employs screen transition graphs and LLMs to model navigation flows and generate test scenarios. For form filling, it uses state graphs to handle conditional forms and automates Selenium script generation. Key contributions include: (1) a novel integration of graph structures and LLMs for site navigation testing, (2) a state graph-based approach for automating form-filling test cases, and (3) a comprehensive dataset for evaluating form-interaction testing. Experimental results demonstrate the system's effectiveness in improving test coverage and robustness, advancing the state of web application testing.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.02529"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "63b768ae7a1a20e25187f11bf3a5a257f7fa753b7384744a73b9b0d3c5f02381",
      "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models",
      "url": "https://arxiv.org/abs/2506.11798",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.11798v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse but have been found to consistently exhibit a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups with which the base model is not aligned. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict the positions of European groups on a diverse set of policies. We evaluate whether predictions are stable in response to counterfactual arguments, different persona prompts, and generation methods. Finally, we find that we can simulate the voting behavior of Members of the European Parliament reasonably well, achieving a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at the following url: https://github.com/dess-mannheim/european_parliament_simulation.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.11798"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0c718904dd67acd710320db33cd51328fda13deee57212f7008b90eb1d940d37",
      "title": "DeepQuark: A Deep-Neural-Network Approach to Multiquark Bound States",
      "url": "https://arxiv.org/abs/2506.20555",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.20555v2 Announce Type: replace-cross Abstract: For the first time, we implement the deep-neural-network-based variational Monte Carlo approach for the multiquark bound states, whose complexity surpasses that of electron or nucleon systems due to strong SU(3) color interactions. We design a novel and high-efficiency architecture, DeepQuark, to address the unique challenges in multiquark systems such as stronger correlations, extra discrete quantum numbers, and intractable confinement interaction. Our method demonstrates competitive performance with state-of-the-art approaches, including diffusion Monte Carlo and Gaussian expansion method, in the nucleon, doubly heavy tetraquark, and fully heavy tetraquark systems. Notably, it outperforms existing calculations for pentaquarks, exemplified by the triply heavy pentaquark. For the nucleon, we successfully incorporate three-body flux-tube confinement interactions without additional computational costs. In tetraquark systems, we consistently describe hadronic molecule $T_{cc}$ and compact tetraquark $T_{bb}$ with an unbiased form of wave function ansatz. In the pentaquark sector, we obtain weakly bound $\\bar D^*\\Xi_{cc}^*$ molecule $P_{cc\\bar c}(5715)$ with $S=\\frac{5}{2}$ and its bottom partner $P_{bb\\bar b}(15569)$. They can be viewed as the analogs of the molecular $T_{cc}$. We recommend experimental search of $P_{cc\\bar c}(5715)$ in the D-wave $J/\\psi \\Lambda_c$ channel. DeepQuark holds great promise for extension to larger multiquark systems, overcoming the computational barriers in conventional methods. It also serves as a powerful framework for exploring confining mechanism beyond two-body interactions in multiquark states, which may offer valuable insights into nonperturbative QCD and general many-body physics.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.20555"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7c881af055f0ab76ceb6780e4ff6abc002fcbabe79b574a843d4c527541f4c4d",
      "title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning",
      "url": "https://arxiv.org/abs/2506.21039",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.21039v2 Announce Type: replace-cross Abstract: Long-horizon goal-conditioned tasks pose fundamental challenges for reinforcement learning (RL), particularly when goals are distant and rewards are sparse. While hierarchical and graph-based methods offer partial solutions, their reliance on conventional hindsight relabeling often fails to correct subgoal infeasibility, leading to inefficient high-level planning. To address this, we propose Strict Subgoal Execution (SSE), a graph-based hierarchical RL framework that integrates Frontier Experience Replay (FER) to separate unreachable from admissible subgoals and streamline high-level decision making. FER delineates the reachability frontier using failure and partial-success transitions, which identifies unreliable subgoals, increases subgoal reliability, and reduces unnecessary high-level decisions. Additionally, SSE employs a decoupled exploration policy to cover underexplored regions of the goal space and a path refinement that adjusts edge costs using observed low-level failures. Experimental results across diverse long-horizon benchmarks show that SSE consistently outperforms existing goal-conditioned and hierarchical RL methods in both efficiency and success rate. Our code is available at https://github.com/Jaebak1996/SSE",
      "tags": [
        "papers",
        "eval",
        "policy"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.21039"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "316d1f25bc601a8c98b72a585136108a019dd2b5a928a135809c4188929830d1",
      "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks",
      "url": "https://arxiv.org/abs/2507.19634",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2507.19634v3 Announce Type: replace-cross Abstract: Recent advances in large language models have laid the foundation for multimodal LLMs (MLLMs), which unify text, speech, and vision within a single framework. As these models are rapidly evolving toward general-purpose instruction following across diverse and complex tasks, a key frontier is evaluating their crosslingual and multimodal capabilities over both short- and long-form inputs. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on a single modality at a time, rely on short-form inputs, or lack human annotations--hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first crosslingual human-annotated benchmark based on scientific talks on NLP and beyond. MCIF evaluates instruction following in crosslingual, multimodal settings over different input lengths and spans four macro-tasks: recognition, translation, question answering, and summarization. It covers three core modalities (speech, vision, and text) and four diverse languages (English, German, Italian, and Chinese), fully aligned across all dimensions. This parallel design enables a systematic evaluation of MLLMs' abilities to interpret instructions across languages and effectively integrate multimodal contextual information. Our benchmarking and analysis of 23 models highlight universal challenges across modalities and tasks, indicating substantial room for improvement in future MLLMs development. MCIF is released under CC-BY 4.0 license to promote open research.",
      "tags": [
        "papers",
        "eval",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2507.19634"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "21fe05f8735add1b800673923a2ef8d2e6decf1a320f2a3cd79c29e6775ec79c",
      "title": "CareerPooler: AI-Powered Metaphorical Pool Simulation Improves Experience and Outcomes in Career Exploration",
      "url": "https://arxiv.org/abs/2509.11461",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2509.11461v2 Announce Type: replace-cross Abstract: Career exploration is uncertain, requiring decisions with limited information and unpredictable outcomes. While generative AI offers new opportunities for career guidance, most systems rely on linear chat interfaces that produce overly comprehensive and idealized suggestions, overlooking the non-linear and effortful nature of real-world trajectories. We present CareerPooler, a generative AI-powered system that employs a pool-table metaphor to simulate career development as a spatial and narrative interaction. Users strike balls representing milestones, skills, and random events, where hints, collisions, and rebounds embody decision-making under uncertainty. In a within-subjects study with 24 participants, CareerPooler significantly improved engagement, information gain, satisfaction, and career clarity compared to a chatbot baseline. Qualitative findings show that spatial-narrative interaction fosters experience-based learning, resilience through setbacks, and reduced psychological burden. Our findings contribute to the design of AI-assisted career exploration systems and more broadly suggest that visually grounded analogical interactions can make generative systems engaging and satisfying.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.11461"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4beee488e634a848c88d8fc106fc9a095a39d0bb15edc3d397c7a9c467b01826",
      "title": "Discrete optimal transport is a strong audio adversarial attack",
      "url": "https://arxiv.org/abs/2509.14959",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2509.14959v2 Announce Type: replace-cross Abstract: In this paper, we introduce the discrete optimal transport voice conversion ($k$DOT-VC) method. Comparison with $k$NN-VC, SinkVC, and Gaussian optimal transport (MKL) demonstrates stronger domain adaptation abilities of our method. We use the probabilistic nature of optimal transport (OT) and show that $k$DOT-VC is an effective black-box adversarial attack against modern audio anti-spoofing countermeasures (CMs). Our attack operates as a post-processing, distribution-alignment step: frame-level {WavLM} embeddings of generated speech are aligned to an unpaired bona fide pool via entropic OT and a top-$k$ barycentric projection, then decoded with a neural vocoder. Ablation analysis indicates that distribution-level alignment is a powerful and stable attack for deployed CMs.",
      "tags": [
        "papers",
        "safety"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.14959"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "726fc932a20f9563727ebf5578fea61d1cf93ed31f83d4597f1b08481aeae3b7",
      "title": "Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials",
      "url": "https://arxiv.org/abs/2509.19877",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2509.19877v4 Announce Type: replace-cross Abstract: Deep learning methods for electronic-structure Hamiltonian prediction has offered significant computational efficiency advantages over traditional DFT methods, yet the diversity of atomic types, structural patterns, and the high-dimensional complexity of Hamiltonians pose substantial challenges to the generalization performance. In this work, we contribute on both the methodology and dataset sides to advance universal deep learning paradigm for Hamiltonian prediction. On the method side, we propose NextHAM, a neural E(3)-symmetry and expressive correction method for efficient and generalizable materials electronic-structure Hamiltonian prediction. First, we introduce the zeroth-step Hamiltonians, which can be efficiently constructed by the initial charge density of DFT, as informative descriptors of neural regression model in the input level and initial estimates of the target Hamiltonian in the output level, so that the regression model directly predicts the correction terms to the target ground truths, thereby significantly simplifying the input-output mapping for learning. Second, we present a neural Transformer architecture with strict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian prediction. Third, we propose a novel training objective to ensure the accuracy performance of Hamiltonians in both real space and reciprocal space, preventing error amplification and the occurrence of \"ghost states\" caused by the large condition number of the overlap matrix. On the dataset side, we curate a high-quality broad-coverage large benchmark, namely Materials-HAM-SOC, comprising 17,000 material structures spanning 68 elements from six rows of the periodic table and explicitly incorporating SOC effects. Experimental results on Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy and efficiency in predicting Hamiltonians and band structures.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.19877"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3e32edfba319018dc15adcddcc1339f8197fa69aaaf591d7b7241f8e667f83ec",
      "title": "CoSpaDi: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning",
      "url": "https://arxiv.org/abs/2509.22075",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2509.22075v4 Announce Type: replace-cross Abstract: Post-training compression of large language models (LLMs) often relies on low-rank weight approximations that represent each column of the weight matrix in a shared low-dimensional subspace. This strategy is computationally efficient but the underlying constraint can be overly rigid for heterogeneous projection weights and may incur avoidable accuracy loss. We propose CoSpaDi (Compression via Sparse Dictionary Learning), a training-free framework that replaces low-rank factorization with a structured sparse decomposition in which each weight matrix is represented as a dense dictionary multiplied by a column-sparse coefficient matrix. This yields a union-of-subspaces model: the columns of the weight matrix are represented as linear combinations of different subsets of dictionary atoms, improving expressiveness at a fixed parameter budget. CoSpaDi is calibration-guided: using a small calibration set, we optimize the factorization to minimize functional reconstruction error of layer outputs rather than weight-space error. An activation-derived Gram orthonormalization reformulates this data-aware objective into a standard dictionary learning problem on transformed weights, and we support both per-layer compression and cross-layer dictionary sharing within groups of similar projections. Across Llama and Qwen model families, CoSpaDi consistently improves the accuracy--compression and perplexity--compression trade-offs over state-of-the-art SVD-based baselines and strong structured pruning baselines at 20-40\\% compression ratios. The resulting structured sparsity enables sparse--dense computation and integrates with post-training quantization of the sparse coefficients.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.22075"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b002629b8ecadd91c8e1d6e625a2cb0a681526bba70102b87455a82ba7884ec7",
      "title": "Watermarking Diffusion Language Models",
      "url": "https://arxiv.org/abs/2509.24368",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2509.24368v2 Announce Type: replace-cross Abstract: We introduce the first watermark tailored for diffusion language models (DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in contrast to standard autoregressive language models (ARLMs) which generate tokens sequentially. While there has been much work in ARLM watermarking, a key challenge when attempting to apply these schemes directly to the DLM setting is that they rely on previously generated tokens, which are not always available with DLM generation. In this work we address this challenge by: (i) applying the watermark in expectation over the context even when some context tokens are yet to be determined, and (ii) promoting tokens which increase the watermark strength when used as context for other tokens. This is accomplished while keeping the watermark detector unchanged. Our experimental evaluation demonstrates that the DLM watermark leads to a >99% true positive rate with minimal quality impact and achieves similar robustness to existing ARLM watermarks, enabling for the first time reliable DLM watermarking.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.24368"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "11fe6f2937a7042ee0b7879982b95867016bb0c57532313fdf5bbc4a5f380f44",
      "title": "Inference-Time Search Using Side Information for Diffusion-Based Image Reconstruction",
      "url": "https://arxiv.org/abs/2510.03352",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.03352v2 Announce Type: replace-cross Abstract: Diffusion models have been widely used as powerful priors for solving inverse problems. However, existing approaches typically overlook side information that could significantly improve reconstruction quality, especially in severely ill-posed settings. In this work, we propose a novel inference-time search algorithm that guides the sampling process using side information. Our framework can be added to existing diffusion-based reconstruction pipelines in a plug-and-play manner, without requiring any training. Through extensive experiments across a range of inverse problems, including inpainting, super-resolution, and several deblurring tasks, and across multiple diffusion-based inverse problem solvers (DPS, DAPS, and MPGD), we show that augmenting each solver with our framework consistently improves the quality of the reconstructions over the corresponding original method. In order to demonstrate the generality of our approach, we consider diverse forms of side information, including reference images, textual descriptions, and anatomical MRI scans. We also show that our search-based approach outperforms other ways of incorporating side information, including reward gradient-based method. Code is available at \\href{https://github.com/mahdi-farahbakhsh/DISS}{here}.",
      "tags": [
        "papers",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.03352"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "23773ffa07ffc9101040125e82a0d87f4803db3900b2601697ae61f5eb973a07",
      "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs",
      "url": "https://arxiv.org/abs/2510.09201",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.09201v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.09201"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7975b8c62f07f60cf78a633e4ec5386b1bc004335ca3380cc5c2e834b5ff0960",
      "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
      "url": "https://arxiv.org/abs/2510.14974",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.14974v3 Announce Type: replace-cross Abstract: Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming previous 1-NFE models of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves substantially better diversity than state-of-the-art DMD models, while maintaining teacher-level quality.",
      "tags": [
        "papers",
        "eval",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.14974"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3878fb65e93caf1c92e3859cef0c462e474595763ebacdb24d9767c7f7a0693c",
      "title": "VERA-MH Concept Paper",
      "url": "https://arxiv.org/abs/2510.15297",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.15297v3 Announce Type: replace-cross Abstract: We introduce VERA-MH (Validation of Ethical and Responsible AI in Mental Health), an automated evaluation of the safety of AI chatbots used in mental health contexts, with an initial focus on suicide risk. Practicing clinicians and academic experts developed a rubric informed by best practices for suicide risk management for the evaluation. To fully automate the process, we used two ancillary AI agents. A user-agent model simulates users engaging in a mental health-based conversation with the chatbot under evaluation. The user-agent role-plays specific personas with pre-defined risk levels and other features. Simulated conversations are then passed to a judge-agent who scores them based on the rubric. The final evaluation of the chatbot being tested is obtained by aggregating the scoring of each conversation. VERA-MH is actively under development and undergoing rigorous validation by mental health clinicians to ensure user-agents realistically act as patients and that the judge-agent accurately scores the AI chatbot. To date we have conducted preliminary evaluation of GPT-5, Claude Opus and Claude Sonnet using initial versions of the VERA-MH rubric and used the findings for further design development. Next steps will include more robust clinical validation and iteration, as well as refining actionable scoring. We are seeking feedback from the community on both the technical and clinical aspects of our evaluation.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.15297"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a6884907da21a46fc7bbe72aa48551d09a46a26c2ea0c0cb92325be1294ea3f3",
      "title": "LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies",
      "url": "https://arxiv.org/abs/2510.24983",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.24983v2 Announce Type: replace-cross Abstract: Diffusion policies are competitive for offline reinforcement learning (RL) but are typically guided at sampling time by heuristics that lack a statistical notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that treats each denoising step as a sequential hypothesis test between the unconditional prior and the state-conditional policy head. Concretely, we accumulate a log-likelihood ratio and gate the conditional mean with a logistic controller whose threshold tau is calibrated once under H0 to meet a user-specified Type-I level alpha. This turns guidance from a fixed push into an evidence-driven adjustment with a user-interpretable risk budget. Importantly, we deliberately leave training vanilla (two heads with standard epsilon-prediction) under the structure of DDPM. LRT guidance composes naturally with Q-gradients: critic-gradient updates can be taken at the unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum from exploitation to conservatism. We standardize states and actions consistently at train and test time and report a state-conditional out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks, LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines in our implementation while honoring the desired alpha. Theoretically, we establish level-alpha calibration, concise stability bounds, and a return comparison showing when LRT surpasses Q-guidance-especially when off-support errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method that adds principled, calibrated risk control to diffusion policies for offline RL.",
      "tags": [
        "papers",
        "policy",
        "inference",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.24983"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "feae741832260137c6dc6e5b4eea737618e055286ed090eaee001536a581a017",
      "title": "VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus",
      "url": "https://arxiv.org/abs/2510.25015",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.25015v3 Announce Type: replace-cross Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated verification from single functions to more complex data structure modules in Verus. VeriStruct employs a planner module to orchestrate the systematic generation of abstractions, type invariants, specifications, and proof code. To address the challenge that LLMs often misunderstand Verus' annotation syntax and verification-specific semantics, VeriStruct embeds syntax guidance within prompts and includes a repair stage to automatically correct annotation errors. In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in total. These results represent an important step toward the goal of automatic AI-assisted formal verification.",
      "tags": [
        "papers",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.25015"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ead5632e3b3d239a12c42dcb6bfacbb2515d882f8e3d5ac92574fba6b85e6bb7",
      "title": "Semi-Supervised Preference Optimization with Limited Feedback",
      "url": "https://arxiv.org/abs/2511.00040",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.00040v3 Announce Type: replace-cross Abstract: The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Mistral-7B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.",
      "tags": [
        "papers",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.00040"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d18190d452e3bacf64b9a46ef97276bcb8db0a4492e0692e11c151e42019266f",
      "title": "Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration",
      "url": "https://arxiv.org/abs/2511.00794",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.00794v3 Announce Type: replace-cross Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the reasoning ability of large language models, yet training remains costly because many rollouts contribute little to optimization, considering the amount of computation required. This study investigates how simply leveraging intrinsic data properties, almost free benefit during training, can improve data efficiency for RLVR. We propose PREPO with two complementary components. First, we adopt prompt perplexity as an indicator of model adaptability in learning, enabling the model to progress from well-understood contexts to more challenging ones. Second, we amplify the discrepancy among the rollouts by differentiating their relative entropy, and prioritize sequences that exhibit a higher degree of exploration. Together, these mechanisms reduce rollout demand while preserving competitive performance. On the Qwen and Llama models, PREPO achieves effective results on mathematical reasoning benchmarks with up to 3 times fewer rollouts than the baselines. Beyond empirical gains, we provide theoretical and in-depth analyses explaining the underlying rationale of our method to improve the data efficiency of RLVR.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.00794"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6bae87f0f3f9ef669d8482e8850e1b12a5736d2ea58e413517e882b2f2497d9c",
      "title": "State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?",
      "url": "https://arxiv.org/abs/2511.07989",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.07989v2 Announce Type: replace-cross Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.",
      "tags": [
        "papers",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.07989"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "66271896743e243ba1a09fbb8eeafe9201877e4dc69457a8e3b1dbd37820b6c4",
      "title": "Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms",
      "url": "https://arxiv.org/abs/2511.14654",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.14654v2 Announce Type: replace-cross Abstract: Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.14654"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "cf4bcdfe62fba426380e6f048c596473aca1b3de10ba428660d63d9ac6ea27ed",
      "title": "Multimodal Wireless Foundation Models",
      "url": "https://arxiv.org/abs/2511.15162",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.15162v2 Announce Type: replace-cross Abstract: Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluate the model on five tasks across both modality families: image-based (human activity sensing, RF signal classification, 5G NR positioning) and IQ-based (RF device fingerprinting, interference detection/classification). The multimodal WFM is competitive with single-modality WFMs, and in several cases surpasses their performance. Our results demonstrates the strong potential of developing multimodal WFMs that support diverse wireless tasks across different modalities. We believe this provides a concrete step toward both AI-native 6G and the vision of joint sensing, communication, and localization.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.15162"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "65073640fff3a1191dcd37581b7d82c5225b24d19cf783b7118ca674464393bf",
      "title": "Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models",
      "url": "https://arxiv.org/abs/2511.18696",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.18696v2 Announce Type: replace-cross Abstract: This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.18696"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1d5c94bf40133e93b49cec97ec087c6e241c50bdeea9af98be4008b67b7f65ce",
      "title": "AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload",
      "url": "https://arxiv.org/abs/2511.19943",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.19943v2 Announce Type: replace-cross Abstract: Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel \"free-lunch\" training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.",
      "tags": [
        "papers",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.19943"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5a7ef22f59d7a1fd5833f21c62649fde5156ac22c2e06a91605c41e88b3b4319",
      "title": "Beyond Linear Surrogates: High-Fidelity Local Explanations for Black-Box Models",
      "url": "https://arxiv.org/abs/2512.05556",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.05556v2 Announce Type: replace-cross Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Existing local explanation methods lack in generating high-fidelity explanations. This paper proposes a novel local model agnostic explanation method to generate high-fidelity explanations using multivariate adaptive regression splines (MARS) and N-ball sampling strategies. MARS is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity. The N-ball sampling technique samples perturbed samples directly from a desired distribution instead of reweighting, leading to further improvement in the faithfulness. The performance of the proposed method was computed in terms of root mean squared error (RMSE) and evaluated on five different benchmark datasets with different kernel width. Experimental results show that the proposed method achieves higher local surrogate fidelity compared to baseline local explanation methods, with an average reduction of 32% in root mean square error, indicating more accurate local approximations of the black-box model. Additionally, statistical analysis shows that across all benchmark datasets, the proposed approach results were statistically significantly better. This paper advances the field of explainable AI by providing insights that can benefit the broader research and practitioner community.",
      "tags": [
        "papers",
        "eval",
        "compute",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.05556"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "569f36a3389a51cb9277ea8418423031840e62c2dc7f2fcea3359e7f4bb07e51",
      "title": "Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection",
      "url": "https://arxiv.org/abs/2512.07984",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.07984v4 Announce Type: replace-cross Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.07984"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f2f2cd54a382115055cfc509ca04565581fd88ddf8510849e6a97dd652256e32",
      "title": "Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution",
      "url": "https://arxiv.org/abs/2512.11108",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.11108v2 Announce Type: replace-cross Abstract: Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find a trade-off between lexical and position biases in our model comparison, with models that score high on one type score low on the other. We also find signs that anomalous explanations are more likely to be biased.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.11108"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "32c1274d459daa1cf8b1ca675b6cd7f04baea8899e6d5340a559df24aca87c02",
      "title": "Block-Recurrent Dynamics in Vision Transformers",
      "url": "https://arxiv.org/abs/2512.19941",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.19941v5 Announce Type: replace-cross Abstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \\ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent runtime. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.19941"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ddfaf7363effa2f9bf731cd76c8fb2c6886f19a4009a8814735e9f0e3f88e4db",
      "title": "On the Existence and Behavior of Secondary Attention Sinks",
      "url": "https://arxiv.org/abs/2512.22213",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.22213v2 Announce Type: replace-cross Abstract: Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.22213"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9fe6ceb29a811f8a272ea6f97956fa3f6bdfab0a16afe69899f36b6aafbe1c66",
      "title": "Theory of Mind for Explainable Human-Robot Interaction",
      "url": "https://arxiv.org/abs/2512.23482",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.23482v3 Announce Type: replace-cross Abstract: Within the context of human-robot interaction (HRI), Theory of Mind (ToM) is intended to serve as a user-friendly backend to the interface of robotic systems, enabling robots to infer and respond to human mental states. When integrated into robots, ToM allows them to adapt their internal models to users' behaviors, enhancing the interpretability and predictability of their actions. Similarly, Explainable Artificial Intelligence (XAI) aims to make AI systems transparent and interpretable, allowing humans to understand and interact with them effectively. Since ToM in HRI serves related purposes, we propose to consider ToM as a form of XAI and evaluate it through the eValuation XAI (VXAI) framework and its seven desiderata. This paper identifies a critical gap in the application of ToM within HRI, as existing methods rarely assess the extent to which explanations correspond to the robot's actual internal reasoning. To address this limitation, we propose to integrate ToM within XAI frameworks. By embedding ToM principles inside XAI, we argue for a shift in perspective, as current XAI research focuses predominantly on the AI system itself and often lacks user-centered explanations. Incorporating ToM would enable a change in focus, prioritizing the user's informational needs and perspective.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.23482"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9268154bc06a9a7fdf4bf8d9bf28c34e2be64f6e524ccde9c4a5f12bfd183a49",
      "title": "Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment",
      "url": "https://arxiv.org/abs/2601.01224",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2601.01224v2 Announce Type: replace-cross Abstract: Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes. Code and pretrained models are available at https://github.com/sony/coda.",
      "tags": [
        "papers",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.01224"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ee63f85c5bb18534e5ec8263e7a731b3a8fa1618ed15e6cfb0760de23e72f5ac",
      "title": "Symphonym: Universal Phonetic Embeddings for Cross-Script Name Matching",
      "url": "https://arxiv.org/abs/2601.06932",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2601.06932v2 Announce Type: replace-cross Abstract: Linking names across historical sources, languages, and writing systems remains a fundamental challenge in digital humanities and geographic information retrieval. Existing approaches require language-specific phonetic algorithms or fail to capture phonetic relationships across different scripts. This paper presents Symphonym, a neural embedding system that maps names from any script into a unified 128-dimensional phonetic space, enabling direct similarity comparison without runtime phonetic conversion. Symphonym uses a Teacher-Student architecture where a Teacher network trained on articulatory phonetic features produces target embeddings, while a Student network learns to approximate these embeddings directly from characters. The Teacher combines Epitran (extended with 100 new language-script mappings), Phonikud for Hebrew, and CharsiuG2P for Chinese, Japanese, and Korean. Training used 32.7 million triplet samples of toponyms spanning 20 writing systems from GeoNames, Wikidata, and Getty Thesaurus of Geographic Names. On the MEHDIE Hebrew-Arabic historical toponym benchmark, Symphonym achieves Recall@10 of 97.6% and MRR of 90.3%, outperforming Levenshtein and Jaro-Winkler baselines (Recall@1: 86.7% vs 81.5% and 78.5%). Evaluation on 12,947 real cross-script training pairs shows 82.6% achieve greater than 0.75 cosine similarity, with best performance on Arabic-Cyrillic (94--100%) and Cyrillic-Latin (94.3%) combinations. The fixed-length embeddings enable efficient retrieval in digital humanities workflows, with a case study on medieval personal names demonstrating effective transfer from modern place names to historical orthographic variation.",
      "tags": [
        "papers",
        "eval",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.06932"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4febf29309e948254cbef7d1e18fdc0cc4c09dc5a0ff4275eee2a64bc7d4e220",
      "title": "Auditing Student-AI Collaboration: A Case Study of Online Graduate CS Students",
      "url": "https://arxiv.org/abs/2601.08697",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2601.08697v3 Announce Type: replace-cross Abstract: As generative AI becomes embedded in higher education, it increasingly shapes how students complete academic tasks. While these systems offer efficiency and support, concerns persist regarding over-automation, diminished student agency, and the potential for unreliable or hallucinated outputs. This study conducts a mixed-methods audit of student-AI collaboration preferences by examining the alignment between current AI capabilities and students' desired levels of automation in academic work. Using two sequential and complementary surveys, we capture students' perceived benefits, risks, and preferred boundaries when using AI. The first survey employs an existing task-based framework to assess preferences for and actual usage of AI across 12 academic tasks, alongside primary concerns and reasons for use. The second survey, informed by the first, explores how AI systems could be designed to address these concerns through open-ended questions. This study aims to identify gaps between existing AI affordances and students' normative expectations of collaboration, informing the development of more effective and trustworthy AI systems for education.",
      "tags": [
        "papers",
        "safety"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.08697"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a068cf94fa5fc57ffc26cb1d53e47d23c2fb8cc7872d00ebdd2569efcc21f6d3",
      "title": "Temporal Graph Pattern Machine",
      "url": "https://arxiv.org/abs/2601.22454",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2601.22454v2 Announce Type: replace-cross Abstract: Temporal graph learning is pivotal for deciphering dynamic systems, where the core challenge lies in explicitly modeling the underlying evolving patterns that govern network transformation. However, prevailing methods are predominantly task-centric and rely on restrictive assumptions -- such as short-term dependency modeling, static neighborhood semantics, and retrospective time usage. These constraints hinder the discovery of transferable temporal evolution mechanisms. To address this, we propose the Temporal Graph Pattern Machine (TGPM), a foundation framework that shifts the focus toward directly learning generalized evolving patterns. TGPM conceptualizes each interaction as an interaction patch synthesized via temporally-biased random walks, thereby capturing multi-scale structural semantics and long-range dependencies that extend beyond immediate neighborhoods. These patches are processed by a Transformer-based backbone designed to capture global temporal regularities while adapting to context-specific interaction dynamics. To further empower the model, we introduce a suite of self-supervised pre-training tasks -- specifically masked token modeling and next-time prediction -- to explicitly encode the fundamental laws of network evolution. Extensive experiments show that TGPM consistently achieves state-of-the-art performance in both transductive and inductive link prediction, demonstrating exceptional cross-domain transferability. Our code has been released in https://github.com/antman9914/TGPM.",
      "tags": [
        "papers",
        "training",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.22454"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c475a80191df20bf447e88ab3f9c45c30011bf39e855f60623b05a19e081b224",
      "title": "Fixed Budget is No Harder Than Fixed Confidence in Best-Arm Identification up to Logarithmic Factors",
      "url": "https://arxiv.org/abs/2602.03972",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.03972v2 Announce Type: replace-cross Abstract: The best-arm identification (BAI) problem is one of the most fundamental problems in interactive machine learning, which has two flavors: the fixed-budget setting (FB) and the fixed-confidence setting (FC). For $K$-armed bandits with the unique best arm, the optimal sample complexities for both settings have been settled down, and they match up to logarithmic factors. This prompts an interesting research question about the generic, potentially structured BAI problems: Is FB harder than FC or the other way around? In this paper, we show that FB is no harder than FC up to logarithmic factors. We do this constructively: we propose a novel algorithm called FC2FB (fixed confidence to fixed budget), which is a meta algorithm that takes in an FC algorithm $\\mathcal{A}$ and turn it into an FB algorithm. We prove that this FC2FB enjoys a sample complexity that matches, up to logarithmic factors, that of the sample complexity of $\\mathcal{A}$. This means that the optimal FC sample complexity is an upper bound of the optimal FB sample complexity up to logarithmic factors. Our result not only reveals a fundamental relationship between FB and FC, but also has a significant implication: FC2FB, combined with existing state-of-the-art FC algorithms, leads to improved sample complexity for a number of FB problems.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.03972"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7eb19363eab469805c4b30b0c0c6cd52d7d2ade41380ae7020ceb00005d1adf6",
      "title": "Di3PO - Diptych Diffusion DPO for Targeted Improvements in Image Generation",
      "url": "https://arxiv.org/abs/2602.06355",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.06355v2 Announce Type: replace-cross Abstract: Existing methods for preference tuning of text-to-image (T2I) diffusion models often rely on computationally expensive generation steps to create positive and negative pairs of images. These approaches frequently yield training pairs that either lack meaningful differences, are expensive to sample and filter, or exhibit significant variance in irrelevant pixel regions, thereby degrading training efficiency. To address these limitations, we introduce \"Di3PO\", a novel method for constructing positive and negative pairs that isolates specific regions targeted for improvement during preference tuning, while keeping the surrounding context in the image stable. We demonstrate the efficacy of our approach by applying it to the challenging task of text rendering in diffusion models, showcasing improvements over baseline methods of SFT and DPO.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.06355"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3c4ec2113f87f8e02c5356c94b57d0041154ee5550043fff6faa50212d0641c7",
      "title": "SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned",
      "url": "https://arxiv.org/abs/2602.07666",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.07666v2 Announce Type: replace-cross Abstract: DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-source software. This paper presents the first systematic analysis of AIxCC. Drawing on design documents, source code, execution traces, and discussions with organizers and competing teams, we examine the competition's structure and key design decisions, characterize the architectural approaches of finalist CRSs, and analyze competition results beyond the final scoreboard. Our analysis reveals the factors that truly drove CRS performance, identifies genuine technical advances achieved by teams, and exposes limitations that remain open for future research. We conclude with lessons for organizing future competitions and broader insights toward deploying autonomous CRSs in practice.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.07666"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "11837595b78220db1fb4cbb70c57220bc4ccf87c0fed974c85165bafb61038b0",
      "title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs",
      "url": "https://arxiv.org/abs/2602.08351",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.08351v2 Announce Type: replace-cross Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.08351"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5bd099826461b1343096d8aaba687885f219deec150efb6b00ab38b772f77d43",
      "title": "Diffusion-Guided Pretraining for Brain Graph Foundation Models",
      "url": "https://arxiv.org/abs/2602.09437",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.09437v2 Announce Type: replace-cross Abstract: With the growing interest in foundation models for brain signals, graph-based pretraining has emerged as a promising paradigm for learning transferable representations from connectome data. However, existing contrastive and masked autoencoder methods typically rely on naive random dropping or masking for augmentation, which is ill-suited for brain graphs and hypergraphs as it disrupts semantically meaningful connectivity patterns. Moreover, commonly used graph-level readout and reconstruction schemes fail to capture global structural information, limiting the robustness of learned representations. In this work, we propose a unified diffusion-based pretraining framework that addresses both limitations. First, diffusion is designed to guide structure-aware dropping and masking strategies, preserving brain graph semantics while maintaining effective pretraining diversity. Second, diffusion enables topology-aware graph-level readout and node-level global reconstruction by allowing graph embeddings and masked nodes to aggregate information from globally related regions. Extensive experiments across multiple neuroimaging datasets with over 25,000 subjects and 60,000 scans involving various mental disorders and brain atlases demonstrate consistent performance improvements.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.09437"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ef9b7c3286f128a246b6c8b3e6230524918e887c7556cc007c845024f07b947b",
      "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention",
      "url": "https://arxiv.org/abs/2602.10117",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.10117v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statistically significant performance differences while not being cited as justification in the model's CoTs. We evaluate our pipeline across seven LLMs on three decision tasks (hiring, loan approval, and university admissions). Our technique automatically discovers previously unknown biases in these models (e.g., Spanish fluency, English proficiency, writing formality). In the same run, the pipeline also validates biases that were manually identified by prior work (gender, race, religion, ethnicity). More broadly, our proposed approach provides a practical, scalable path to automatic task-specific bias discovery.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.10117"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b7abf73454f4801904195afce64cf1f28ff964898858d7819bb326e332ec7f9b",
      "title": "LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules",
      "url": "https://arxiv.org/abs/2602.10993",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.10993v2 Announce Type: replace-cross Abstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.",
      "tags": [
        "papers",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.10993"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c47ef9f19c6da6bcb2e2dff378875a3aae3650954870a230cd8c9d635ce88d1e",
      "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
      "url": "https://arxiv.org/abs/2602.11337",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.11337v2 Announce Type: replace-cross Abstract: Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, \\r{ho} = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.",
      "tags": [
        "papers",
        "eval",
        "policy",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.11337"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bfbb6b8a73cc3f2cecfe1c64c03172aa26a6abe141cc7893dd1d07952eee53b7",
      "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging",
      "url": "https://arxiv.org/abs/2602.13110",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.13110v2 Announce Type: replace-cross Abstract: Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $\\alpha$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $\\alpha = 0.10$, SCOPE consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to na\\\"ive baselines, SCOPE accepts up to $2.4\\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.13110"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b871fb039beec8bf0aeb3ebfc7f8c47a70a085fb8cf8df365b4229702a84569f",
      "title": "GraFSTNet: Graph-based Frequency SpatioTemporal Network for Cellular Traffic Prediction",
      "url": "https://arxiv.org/abs/2602.13282",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.13282v2 Announce Type: replace-cross Abstract: With rapid expansion of cellular networks and the proliferation of mobile devices, cellular traffic data exhibits complex temporal dynamics and spatial correlations, posing challenges to accurate traffic prediction. Previous methods often focus predominantly on temporal modeling or depend on predefined spatial topologies, which limits their ability to jointly model spatio-temporal dependencies and effectively capture periodic patterns in cellular traffic. To address these issues, we propose a cellular traffic prediction framework that integrates spatio-temporal modeling with time-frequency analysis. First, we construct a spatial modeling branch to capture inter-cell dependencies through an attention mechanism, minimizing the reliance on predefined topological structures. Second, we build a time-frequency modeling branch to enhance the representation of periodic patterns. Furthermore, we introduce an adaptive-scale LogCosh loss function, which adjusts the error penalty based on traffic magnitude, preventing large errors from dominating the training process and helping the model maintain relatively stable prediction accuracy across different traffic intensities. Experiments on three open-sourced datasets demonstrate that the proposed method achieves prediction performance superior to state-of-the-art approaches.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.13282"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "401ab8ca5bb617982c951fa38db799cc1639eee4dabb7b4d8051929a43c56dfa",
      "title": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction",
      "url": "https://arxiv.org/abs/2602.13294",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.13294v2 Announce Type: replace-cross Abstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.13294"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "77deafdcb8c3f7571905bcec5291ad5a1ce633a61180f6eb694e88d4e808f266",
      "title": "How Multimodal Large Language Models Support Access to Visual Information: A Diary Study With Blind and Low Vision People",
      "url": "https://arxiv.org/abs/2602.13469",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.13469v2 Announce Type: replace-cross Abstract: Multimodal large language models (MLLMs) are changing how Blind and Low Vision (BLV) people access visual information. Unlike traditional visual interpretation tools that only provide descriptions, MLLM-enabled applications offer conversational assistance, where users can ask questions to obtain goal-relevant details. However, evidence about their performance in the real-world and implications for BLV people's daily lives remains limited. To address this, we conducted a two-week diary study, where we captured 20 BLV participants' use of an MLLM-enabled visual interpretation application. Although participants rated the visual interpretations of the application as \"trustworthy\" (mean=3.76 out of 5, max=extremely trustworthy) and \"somewhat satisfying\" (mean=4.13 out of 5, max=very satisfying), the AI often produced incorrect answers (22.2%) or abstained (10.8%) from responding to users' requests. Our findings show that while MLLMs can improve visual interpretations' descriptive accuracy, supporting everyday use also depends on the \"visual assistant\" skill: behaviors for providing goal-directed, reliable assistance. We conclude by proposing the \"visual assistant\" skill and guidelines to help MLLM-enabled visual interpretation applications better support BLV people's access to visual information.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.13469"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b3dbb6dd1f2dee547a73902da92c7c1c56b776956b5d94c8c067e950e757813e",
      "title": "CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography",
      "url": "https://arxiv.org/abs/2602.14879",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.14879v2 Announce Type: replace-cross Abstract: Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.",
      "tags": [
        "papers",
        "eval",
        "compute",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.14879"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e2956d5bcca7aecc13cb05840d6aba6fbcaf2f9edf0eb286e14a786b2cc87930",
      "title": "References Improve LLM Alignment in Non-Verifiable Domains",
      "url": "https://arxiv.org/abs/2602.16802",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16802v1 Announce Type: new Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16802"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "79dbfd653caaeeab118681666e779522b2cde740aa7f08b9cf15e6ec38b2316f",
      "title": "Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark",
      "url": "https://arxiv.org/abs/2602.16811",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16811v1 Announce Type: new Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.",
      "tags": [
        "papers",
        "language",
        "eval",
        "training",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16811"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "7e0462b77f771d1058070f7567010723cd310a3216f32b0700d9d58d7a9e889f",
      "title": "One-step Language Modeling via Continuous Denoising",
      "url": "https://arxiv.org/abs/2602.16813",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16813v1 Announce Type: new Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.",
      "tags": [
        "papers",
        "language",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16813"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "7324c49adda1f4feee84a272c83b8d42c7fe581a14b1b2c6d32d302ea88569ea",
      "title": "Claim Automation using Large Language Model",
      "url": "https://arxiv.org/abs/2602.16836",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16836v1 Announce Type: new Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.",
      "tags": [
        "papers",
        "language",
        "eval",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16836"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b25d6c48f52865953ae7b39c89bc66536583013b443d89346c6685fbc0304508",
      "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization",
      "url": "https://arxiv.org/abs/2602.16843",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16843v1 Announce Type: new Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $\\rho = 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16843"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "16b52ff8fb6b0d965077716796bd54b1d86dab1fdc6b6865b61005d2511efb4a",
      "title": "Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect",
      "url": "https://arxiv.org/abs/2602.16852",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16852v1 Announce Type: new Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.",
      "tags": [
        "papers",
        "language",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16852"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b04b9cccb5eeeafd06867a1755ae38e786288ab4c2474b47cf6e2803808b5d2d",
      "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders",
      "url": "https://arxiv.org/abs/2602.16938",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16938v1 Announce Type: new Abstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical \"realism gap,\" leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both \"good\" and \"bad\" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.",
      "tags": [
        "papers",
        "language",
        "agents",
        "eval",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16938"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "816f351079c64901dc5e2171573dfeeb1a1fb53ca462d8d1cb61c5f7d8c4fcda",
      "title": "When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English",
      "url": "https://arxiv.org/abs/2602.16957",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16957v1 Announce Type: new Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.",
      "tags": [
        "papers",
        "language",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16957"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "70e2936701b0cb309bc8199d89eeb4d25b9f1997756dc42807aaf8d8ab6bd657",
      "title": "Eigenmood Space: Uncertainty-Aware Spectral Graph Analysis of Psychological Patterns in Classical Persian Poetry",
      "url": "https://arxiv.org/abs/2602.16959",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16959v1 Announce Type: new Abstract: Classical Persian poetry is a historically sustained archive in which affective life is expressed through metaphor, intertextual convention, and rhetorical indirection. These properties make close reading indispensable while limiting reproducible comparison at scale. We present an uncertainty-aware computational framework for poet-level psychological analysis based on large-scale automatic multi-label annotation. Each verse is associated with a set of psychological concepts, per-label confidence scores, and an abstention flag that signals insufficient evidence. We aggregate confidence-weighted evidence into a Poet $\\times$ Concept matrix, interpret each poet as a probability distribution over concepts, and quantify poetic individuality as divergence from a corpus baseline using Jensen--Shannon divergence and Kullback--Leibler divergence. To capture relational structure beyond marginals, we build a confidence-weighted co-occurrence graph over concepts and define an Eigenmood embedding through Laplacian spectral decomposition. On a corpus of 61{,}573 verses across 10 poets, 22.2\\% of verses are abstained, underscoring the analytical importance of uncertainty. We further report sensitivity analysis under confidence thresholding, selection-bias diagnostics that treat abstention as a category, and a distant-to-close workflow that retrieves verse-level exemplars along Eigenmood axes. The resulting framework supports scalable, auditable digital-humanities analysis while preserving interpretive caution by propagating uncertainty from verse-level evidence to poet-level inference.",
      "tags": [
        "papers",
        "language",
        "inference"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16959"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "36ed276808218b4693efec544e12548abada459a5f4125b1288cbb46f5523d71",
      "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
      "url": "https://arxiv.org/abs/2602.17003",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17003v1 Announce Type: new Abstract: Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables fine-grained assessment of personalization. We conduct extensive experiments across various agent architectures, backbone models, history access schemes, and queries with varying ambiguity levels, revealing key challenges in personalized web agent behavior. For reproducibility, our codes and datasets are publicly available at https://anonymous.4open.science/r/Persona2Web-73E8.",
      "tags": [
        "papers",
        "language",
        "agents",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17003"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "fd23b8b3f496580861cfdaa1ed99d5687e758fb5a15df9c3f810efe7046d6f79",
      "title": "ReIn: Conversational Error Recovery with Reasoning Inception",
      "url": "https://arxiv.org/abs/2602.17022",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17022v1 Announce Type: new Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.",
      "tags": [
        "papers",
        "language",
        "agents",
        "reasoning",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17022"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "07814ed59b0d2c7cf229e7b1bc67a9a59bdd15243b29140e269142e130712563",
      "title": "Large Language Models Persuade Without Planning Theory of Mind",
      "url": "https://arxiv.org/abs/2602.17045",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17045v1 Announce Type: new Abstract: A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.",
      "tags": [
        "papers",
        "language",
        "agents",
        "reasoning",
        "eval",
        "policy",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17045"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "ddabb4d67bac89bda33ee67dc57bd116d32906e1be39ef6292d3e486f4caa749",
      "title": "Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data",
      "url": "https://arxiv.org/abs/2602.17051",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17051v1 Announce Type: new Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.",
      "tags": [
        "papers",
        "language",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17051"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "5056ed2c5465842c016f700cc0dfcc6bf38b85dd27955a19d4e954766ce0ffc2",
      "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning",
      "url": "https://arxiv.org/abs/2602.17054",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17054v1 Announce Type: new Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17054"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "1093229e8b252964fc5d1ef7ca6f1d770d5bb4845963aa1d18261b9ec2cb0bf2",
      "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios",
      "url": "https://arxiv.org/abs/2602.17072",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17072v1 Announce Type: new Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17072"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8610bd65888ab68f1e8e1c29dc06ef30648684b88a635524bc3801ed6d6e382b",
      "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests",
      "url": "https://arxiv.org/abs/2602.17108",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17108v1 Announce Type: new Abstract: Thematic Apperception Test (TAT) is a psychometrically grounded, multidimensional assessment framework that systematically differentiates between cognitive-representational and affective-relational components of personality-like functioning. This test is a projective psychological framework designed to uncover unconscious aspects of personality. This study examines whether the personality traits of Large Multimodal Models (LMMs) can be assessed through non-language-based modalities, using the Social Cognition and Object Relations Scale - Global (SCORS-G). LMMs are employed in two distinct roles: as subject models (SMs), which generate stories in response to TAT images, and as evaluator models (EMs), who assess these narratives using the SCORS-G framework. Evaluators demonstrated an excellent ability to understand and analyze TAT responses. Their interpretations are highly consistent with those of human experts. Assessment results highlight that all models understand interpersonal dynamics very well and have a good grasp of the concept of self. However, they consistently fail to perceive and regulate aggression. Performance varied systematically across model families, with larger and more recent models consistently outperforming smaller and earlier ones across SCORS-G dimensions.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17108"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "d4976b25852464e3c9ecd34434fa0e215ec50982dca70d45641aa10dc3c70e74",
      "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
      "url": "https://arxiv.org/abs/2602.17127",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17127v1 Announce Type: new Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions. This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization. Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.",
      "tags": [
        "papers",
        "language",
        "agents",
        "reasoning",
        "eval",
        "safety",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17127"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "539ecdeed3345d164623c9b031cbccb217d0e975d923df28b64b71bb7cc78e13",
      "title": "What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform",
      "url": "https://arxiv.org/abs/2602.17194",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17194v1 Announce Type: new Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17194"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "cc64812e8539c605706a9a4d855e8c8ae5116d64b811eceaba63ee112891b9bd",
      "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
      "url": "https://arxiv.org/abs/2602.17262",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17262v1 Announce Type: new Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17262"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "f0c6aa8a8a7600da63024f90ba17e1f0b7ef86fb58a7447e307f94e9d6b0cc82",
      "title": "Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective",
      "url": "https://arxiv.org/abs/2602.17283",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17283v1 Announce Type: new Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc 20\\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17283"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8cf19bc883f62366717939b9debc360e322c6be71ff76fe5e6beff622c21e002",
      "title": "Representation Collapse in Machine Translation Through the Lens of Angular Dispersion",
      "url": "https://arxiv.org/abs/2602.17287",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17287v1 Announce Type: new Abstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.",
      "tags": [
        "papers",
        "language",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17287"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8780816e84d103549e154d22c139a245305c373e6e5cc5c7c9a0207183306dab",
      "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
      "url": "https://arxiv.org/abs/2602.17316",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17316v1 Announce Type: new Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17316"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "f9c126acfc333b3c6cb2b9c602bd95f6dd947942c494c85556b20e85c6546f20",
      "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering",
      "url": "https://arxiv.org/abs/2602.17366",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17366v1 Announce Type: new Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.",
      "tags": [
        "papers",
        "language",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17366"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b42b568ef3f8861ec0e1fcf9cccb810ab98570b852dac19b312c21b4adea5287",
      "title": "The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour",
      "url": "https://arxiv.org/abs/2602.17377",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17377v1 Announce Type: new Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.",
      "tags": [
        "papers",
        "language",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17377"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "0faeda53b15cea9aa13a44845daf94e071ffd40d9c50ec0fbebf43a600648253",
      "title": "Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference",
      "url": "https://arxiv.org/abs/2602.17424",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17424v1 Announce Type: new Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking \"the caravan\" - \"asylum seekers\" - \"those contemplating illegal entry\", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.",
      "tags": [
        "papers",
        "language",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17424"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "127381d980d030218e3a5c1352dbce224d77e5b0ef347bd47bb9213637f300d7",
      "title": "Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics",
      "url": "https://arxiv.org/abs/2602.17425",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17425v1 Announce Type: new Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17425"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "da75e6440c2550889b3e5b7b7d369cb730e21eea3b610c72947de7fc6b85c5b5",
      "title": "Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study",
      "url": "https://arxiv.org/abs/2602.17431",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17431v1 Announce Type: new Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.",
      "tags": [
        "papers",
        "language",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17431"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "ad46e26d93e7bcf863ddd4998622d6acd55881293062c7d2f9e32a7e3ed36643",
      "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue",
      "url": "https://arxiv.org/abs/2602.17443",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17443v1 Announce Type: new Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured \"20 Questions\" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17443"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8894a4301a1f826ed7bc31a94ba99d930d0b908617f835367f198e4cc0ec0ab6",
      "title": "ABCD: All Biases Come Disguised",
      "url": "https://arxiv.org/abs/2602.17445",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17445v1 Announce Type: new Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17445"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "38817ab2edbcada283b5e73b8438b10b5334e854f75334e13941104f618b7c6e",
      "title": "Entropy-Based Data Selection for Language Models",
      "url": "https://arxiv.org/abs/2602.17465",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17465v1 Announce Type: new Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.",
      "tags": [
        "papers",
        "language",
        "compute",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17465"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "0004d38772599adeab75cafba38e115a08d74178528dc260cba9040f4f65ba5a",
      "title": "PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions",
      "url": "https://arxiv.org/abs/2602.17467",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17467v1 Announce Type: new Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.",
      "tags": [
        "papers",
        "language"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17467"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8abe52dd205f41f2ffa1afb1e7d6ba2d29ff4a4e59945d1a6f94f1f4ee0d6336",
      "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers",
      "url": "https://arxiv.org/abs/2602.17469",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17469v1 Announce Type: new Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% \"Sentiment Inversion Rate,\" fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including \"Asymmetric Empathy\" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a \"Modern Bias\" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate \"Affective Stability\" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17469"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "a2ebbc1d71180d5e881cb8dea87689ba57eb6c2f1eb535cea177c80bdfb82d69",
      "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
      "url": "https://arxiv.org/abs/2602.17475",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17475v1 Announce Type: new Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether \"small\" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.",
      "tags": [
        "papers",
        "language",
        "inference",
        "training",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17475"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "566c53665e4362715a8ac3092bdd562ecbcb6b90708ad385753756726e8bd62f",
      "title": "Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics",
      "url": "https://arxiv.org/abs/2602.17513",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17513v1 Announce Type: new Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.",
      "tags": [
        "papers",
        "language",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17513"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b21e5eb401d2eb7e479e7a8af1be5b827505baa1c30aa9ac6e06cc2725ddf9e3",
      "title": "Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems",
      "url": "https://arxiv.org/abs/2602.17542",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17542v1 Announce Type: new Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17542"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "5d6d41c4e52b740f06ceb2bcc0bfe76d5172228d474010fa4fb9ded3ffb38e24",
      "title": "Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.17546",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17546v1 Announce Type: new Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.",
      "tags": [
        "papers",
        "language",
        "safety",
        "policy",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17546"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "bda777d7d6e9c30236bbcf88831b9eb5246188779dbbe63af1967299bf20128b",
      "title": "Modeling Distinct Human Interaction in Web Agents",
      "url": "https://arxiv.org/abs/2602.17588",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17588v1 Announce Type: new Abstract: Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.",
      "tags": [
        "papers",
        "language",
        "agents",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17588"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "0675f77c25fd3d149f9ee53ed0e37d1ce8d26a39773d570beb1c4697b0401007",
      "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
      "url": "https://arxiv.org/abs/2602.17598",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17598v1 Announce Type: new Abstract: Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($\\kappa{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.",
      "tags": [
        "papers",
        "language"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17598"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "6f2cbc7ce08ea24545fe7639954286289713d498793700eb1645b4ccf8adddd8",
      "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
      "url": "https://arxiv.org/abs/2602.17623",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17623v1 Announce Type: new Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17623"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b4b869da13019ee6be305c9abd4624b243789cf4795a74c9ab771fcc02850f02",
      "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
      "url": "https://arxiv.org/abs/2602.17653",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17653v1 Announce Type: new Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.",
      "tags": [
        "papers",
        "language",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17653"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "5f650d66e8b5ecda717e944dc4ecc5395bc0370c92b60c66b6a5822e6bf45c7d",
      "title": "What Language is This? Ask Your Tokenizer",
      "url": "https://arxiv.org/abs/2602.17655",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17655v1 Announce Type: new Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.",
      "tags": [
        "papers",
        "language",
        "eval",
        "compute",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17655"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "78be10e779deac47765cb0c9adf66ae33a428660dfda8aaaf4864a10ed590651",
      "title": "Sink-Aware Pruning for Diffusion Language Models",
      "url": "https://arxiv.org/abs/2602.17664",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17664v1 Announce Type: new Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\\bf \\texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.",
      "tags": [
        "papers",
        "language",
        "compute",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17664"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "ce34d82fe7344ab4b3b5aae79810b8dd4c54f33a4fbaf7a5afc7005689842801",
      "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy",
      "url": "https://arxiv.org/abs/2602.16053",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16053v1 Announce Type: cross Abstract: Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16053"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "3ab98a8712c906a7518f08eb01af345aa28ea6756e556e9f3fd344bab3233c93",
      "title": "Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems",
      "url": "https://arxiv.org/abs/2602.16715",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16715v1 Announce Type: cross Abstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16715"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "f00618b382d85eb7b1bfa821ff140ba91ae9bbf2e616d94e5242b4b1b02b9ec9",
      "title": "Intent Laundering: AI Safety Datasets Are Not What They Seem",
      "url": "https://arxiv.org/abs/2602.16729",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16729v1 Announce Type: cross Abstract: We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world attacks based on three key properties: driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on \"triggering cues\": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce \"intent laundering\": a procedure that abstracts away triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world attacks due to their overreliance on triggering cues. In fact, once these cues are removed, all previously evaluated \"reasonably safe\" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated and how real-world adversaries behave.",
      "tags": [
        "papers",
        "language",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16729"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "6b6ddf289da41d5f9683a614aeecd4938b714401b4e87642ae63d24229e4f6cf",
      "title": "PREFER: An Ontology for the PREcision FERmentation Community",
      "url": "https://arxiv.org/abs/2602.16755",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16755v1 Announce Type: cross Abstract: Precision fermentation relies on microbial cell factories to produce sustainable food, pharmaceuticals, chemicals, and biofuels. Specialized laboratories such as biofoundries are advancing these processes using high-throughput bioreactor platforms, which generate vast datasets. However, the lack of community standards limits data accessibility and interoperability, preventing integration across platforms. In order to address this, we introduce PREFER, an open-source ontology designed to establish a unified standard for bioprocess data. Built in alignment with the widely adopted Basic Formal Ontology (BFO) and connecting with several other community ontologies, PREFER ensures consistency and cross-domain compatibility and covers the whole precision fermentation process. Integrating PREFER into high-throughput bioprocess development workflows enables structured metadata that supports automated cross-platform execution and high-fidelity data capture. Furthermore, PREFER's standardization has the potential to bridge disparate data silos, generating machine-actionable datasets critical for training predictive, robust machine learning models in synthetic biology. This work provides the foundation for scalable, interoperable bioprocess systems and supports the transition toward more data-driven bioproduction.",
      "tags": [
        "papers",
        "language",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16755"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "5a6b9aa0e480a0bf9f8b611db8b3d2c80af6dfa638f2ce0ec3bda9cfadff091d",
      "title": "Omitted Variable Bias in Language Models Under Distribution Shift",
      "url": "https://arxiv.org/abs/2602.16784",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16784v1 Announce Type: cross Abstract: Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization provides more principled measures of out-of-distribution performance, improves true out-of-distribution performance relative to standard distribution shift adjustment methods, and further enables inference about the strength of the omitted variables when target distribution labels are available.",
      "tags": [
        "papers",
        "language",
        "eval",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16784"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "3f1e670b345aaf28397d21ad8919a93a7426c13baeefba3c542b548425457e7b",
      "title": "Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency",
      "url": "https://arxiv.org/abs/2602.16787",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16787v1 Announce Type: cross Abstract: Despite their strong performance on reasoning benchmarks, large language models (LLMs) have proven brittle when presented with counterfactual questions, suggesting weaknesses in their causal reasoning ability. While recent work has demonstrated that labeled counterfactual tasks can be useful benchmarks of LLMs' causal reasoning, producing such data at the scale required to cover the vast potential space of counterfactuals is limited. In this work, we introduce double counterfactual consistency (DCC), a lightweight inference-time method for measuring and guiding the ability of LLMs to reason causally. Without requiring labeled counterfactual data, DCC verifies a model's ability to execute two important elements of causal reasoning: causal intervention and counterfactual prediction. Using DCC, we evaluate the causal reasoning abilities of various leading LLMs across a range of reasoning tasks and interventions. Moreover, we demonstrate the effectiveness of DCC as a training-free test-time rejection sampling criterion and show that it can directly improve performance on reasoning tasks across multiple model families.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16787"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "5f39a21f08bd288a70a01b32073f528bde3af6463688ce6505e4da2db4ea1297",
      "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
      "url": "https://arxiv.org/abs/2602.16819",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16819v1 Announce Type: cross Abstract: When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.",
      "tags": [
        "papers",
        "language",
        "agents",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16819"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "02955b3ec4fa4b63e013ade032edb66e835d367391206ccaf864eb12c12ae9cf",
      "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages",
      "url": "https://arxiv.org/abs/2602.16832",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16832v1 Announce Type: cross Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \\textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks. IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16832"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "c5c3b2dc1fd203c52c2902f10c0b666f4134f5b9147bfc2473a47b7259209f46",
      "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
      "url": "https://arxiv.org/abs/2602.16839",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16839v1 Announce Type: cross Abstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16839"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "a01577169865dd273525b0c84ae7022ef5dcfc32ea09aa40463dd40f4c054dba",
      "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents",
      "url": "https://arxiv.org/abs/2602.16855",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16855v1 Announce Type: cross Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.",
      "tags": [
        "papers",
        "language",
        "agents",
        "reasoning",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16855"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "302208ded1313212993c1eb46aff1a2f5d95e3190160e4a7853402065ebd29ae",
      "title": "HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing",
      "url": "https://arxiv.org/abs/2602.16976",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16976v1 Announce Type: cross Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed: Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.",
      "tags": [
        "papers",
        "language",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16976"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "dda0621b44cd3904d733e1f5d1cb822174b3f3a4b7a2d02ccafd39794847c609",
      "title": "Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling",
      "url": "https://arxiv.org/abs/2602.16979",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16979v1 Announce Type: cross Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.",
      "tags": [
        "papers",
        "language",
        "compute",
        "inference",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16979"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "13b3647f93c1c2dbf10987e11311ef7db9e1eb384e22166424895f333ada05cb",
      "title": "Exploring LLMs for User Story Extraction from Mockups",
      "url": "https://arxiv.org/abs/2602.16997",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16997v1 Announce Type: cross Abstract: User stories are one of the most widely used artifacts in the software industry to define functional requirements. In parallel, the use of high-fidelity mockups facilitates end-user participation in defining their needs. In this work, we explore how combining these techniques with large language models (LLMs) enables agile and automated generation of user stories from mockups. To this end, we present a case study that analyzes the ability of LLMs to extract user stories from high-fidelity mockups, both with and without the inclusion of a glossary of the Language Extended Lexicon (LEL) in the prompts. Our results demonstrate that incorporating the LEL significantly enhances the accuracy and suitability of the generated user stories. This approach represents a step forward in the integration of AI into requirements engineering, with the potential to improve communication between users and developers.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16997"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "a5228a0d5e38e738a89a5c52250ab9fcc68af344e9aead31f26f8b8cfa2a0c5d",
      "title": "Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases",
      "url": "https://arxiv.org/abs/2602.17001",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17001v1 Announce Type: cross Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17001"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "081fb5185e4d8bf00bae5465ec4e63f745c8d883a3752c3cbbad424e3ee77764",
      "title": "Arcee Trinity Large Technical Report",
      "url": "https://arxiv.org/abs/2602.17004",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17004v1 Announce Type: cross Abstract: We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.",
      "tags": [
        "papers",
        "language",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17004"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "1b26cd127c55075be90de7d4266c042fcc2b8b4f3075b05b81fa06997069985b",
      "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.17053",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17053v1 Announce Type: cross Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17053"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "faf8ee05a3a5171e4e4a67375a7bbed49d5159ec69416f48d977f64344fcf375",
      "title": "Sign Lock-In: Randomly Initialized Weight Signs Persist and Bottleneck Sub-Bit Model Compression",
      "url": "https://arxiv.org/abs/2602.17063",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17063v1 Announce Type: cross Abstract: Sub-bit model compression seeks storage below one bit per weight; as magnitudes are aggressively compressed, the sign bit becomes a fixed-cost bottleneck. Across Transformers, CNNs, and MLPs, learned sign matrices resist low-rank approximation and are spectrally indistinguishable from an i.i.d. Rademacher baseline. Despite this apparent randomness, most weights retain their initialization signs; flips primarily occur via rare near-zero boundary crossings, suggesting that sign-pattern randomness is largely inherited from initialization. We formalize this behavior with sign lock-in theory, a stopping-time analysis of sign flips under SGD noise. Under bounded updates and a rare re-entry condition into a small neighborhood around zero, the number of effective sign flips exhibits a geometric tail. Building on this mechanism, we introduce a gap-based initialization and a lightweight outward-drift regularizer, reducing the effective flip rate to approximately $10^{-3}$ with only about a one-point increase in perplexity.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17063"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "15fb3efb6ac77e90f7bf88e0bab7e2aa597dca8871d9681a5b2537197847ffce",
      "title": "From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences",
      "url": "https://arxiv.org/abs/2602.17221",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17221v1 Announce Type: cross Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a \"methodological experiment,\" this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology. This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A). This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.",
      "tags": [
        "papers",
        "language",
        "agents",
        "reasoning"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17221"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b2bc277757e4e6b770f1998a10cdea839c693d2c0249b6760c2da216034eb56b",
      "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
      "url": "https://arxiv.org/abs/2602.17229",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17229v1 Announce Type: cross Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17229"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "23429a00a10641a65cc7c334543e9a14d390f3a3eee3cfae4622c59356e41532",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
      "url": "https://arxiv.org/abs/2602.17288",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17288v1 Announce Type: cross Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "compute",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17288"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "06dbc318be1ac06a4f3b595edc7a37a1c6d4198f7b3ed6e24ab699aecfb06b65",
      "title": "WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval",
      "url": "https://arxiv.org/abs/2602.17327",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17327v1 Announce Type: cross Abstract: We introduce WebFAQ 2.0, a new version of the WebFAQ dataset, containing 198 million FAQ-based natural question-answer pairs across 108 languages. Compared to the previous version, it significantly expands multilingual coverage and the number of bilingual aligned QA pairs to over 14.3M, making it the largest FAQ-based resource. Unlike the original release, WebFAQ 2.0 uses a novel data collection strategy that directly crawls and extracts relevant web content, resulting in a substantially more diverse and multilingual dataset with richer context through page titles and descriptions. In response to community feedback, we also release a hard negatives dataset for training dense retrievers, with 1.25M queries across 20 languages. These hard negatives were mined using a two-stage retrieval pipeline and include cross-encoder scores for 200 negatives per query. We further show how this resource enables two primary fine-tuning strategies for dense retrievers: Contrastive Learning with MultipleNegativesRanking loss, and Knowledge Distillation with MarginMSE loss. WebFAQ 2.0 is not a static resource but part of a long-term effort. Since late 2025, structured FAQs are being regularly released through the Open Web Index, enabling continuous expansion and refinement. We publish the datasets and training scripts to facilitate further research in multilingual and cross-lingual IR. The dataset itself and all related resources are publicly available on GitHub and HuggingFace.",
      "tags": [
        "papers",
        "language",
        "training",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17327"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "fb91708ade222907dd53c7cd011b2710f1c446f60cbfd5c3a09791fb88e5dceb",
      "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
      "url": "https://arxiv.org/abs/2602.17413",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17413v1 Announce Type: cross Abstract: In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over private documents on behalf of a data provider. Instead of releasing documents, the provider exposes a natural language interface whose responses are constrained by machine-readable usage policies. We formalize policy-violating information disclosure in this setting, drawing on usage control and information flow security, and introduce virtual redaction: suppressing sensitive information at query time without modifying source documents. We describe an architecture for integrating such a spokesperson with Eclipse Dataspace Components and ODRL-style policies, and outline an initial provider-side integration prototype in which QA requests are routed through a spokesperson service instead of triggering raw document transfer. Our contribution is primarily architectural: we do not yet implement or empirically evaluate the full enforcement pipeline. We therefore outline an evaluation methodology to assess security, utility, and performance trade-offs under benign and adversarial querying as a basis for future empirical work on systematically governed LLM access to multi-party data spaces.",
      "tags": [
        "papers",
        "language",
        "eval",
        "policy",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17413"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "c98c9d9a463f836d0c15356fdd980d76132079e6886e4dc2c19c7510d1ff852b",
      "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
      "url": "https://arxiv.org/abs/2602.17483",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17483v1 Announce Type: cross Abstract: Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative studies (N=20), and run two studies with EU residents to capture (i) intuitions about LLM-generated PD (N1=155) and (ii) reactions to tool output (N2=303). We show empirically that models confidently generate multiple PD categories for well-known individuals. For everyday users, GPT-4o generates 11 features with 60% or more accuracy (e.g., gender, hair color, languages). Finally, 72% of participants sought control over model-generated associations with their name, raising questions about what counts as PD and whether data privacy rights should extend to LLMs.",
      "tags": [
        "papers",
        "language",
        "agents",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17483"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "1e26a4e94777184dd02e6d2bba3c7f3ff20a8f674b4770bd328b81610db2b416",
      "title": "The Anxiety of Influence: Bloom Filters in Transformer Attention Heads",
      "url": "https://arxiv.org/abs/2602.17526",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17526v1 Announce Type: cross Abstract: Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question \"has this token appeared before in the context?\" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\\% even at 180 unique context tokens -- well above the $d_\\text{head} = 64$ bit capacity of a classical Bloom filter. A third head (L1H11) shows the classic Bloom filter capacity curve: its false positive rate follows the theoretical formula $p \\approx (1 - e^{-kn/m})^k$ with $R^2 = 1.0$ and fitted capacity $m \\approx 5$ bits, saturating by $n \\approx 20$ unique tokens. A fourth head initially identified as a Bloom filter (L3H0) was reclassified as a general prefix-attention head after confound controls revealed its apparent capacity curve was a sequence-length artifact. Together, the three genuine membership-testing heads form a multi-resolution system concentrated in early layers (0-1), taxonomically distinct from induction and previous-token heads, with false positive rates that decay monotonically with embedding distance -- consistent with distance-sensitive Bloom filters. These heads generalize broadly: they respond to any repeated token type, not just repeated names, with 43\\% higher generalization than duplicate-token-only heads. Ablation reveals these heads contribute to both repeated and novel token processing, indicating that membership testing coexists with broader computational roles. The reclassification of L3H0 through confound controls strengthens rather than weakens the case: the surviving heads withstand the scrutiny that eliminated a false positive in our own analysis.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17526"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "4574bbb839b61a6a0a30b603fb27ca2b5373e4e26e313a67817647c3bda85f8e",
      "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
      "url": "https://arxiv.org/abs/2602.17544",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17544v1 Announce Type: cross Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.",
      "tags": [
        "papers",
        "language",
        "agents",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17544"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "9747e6e791ec6b25003c8f2de2e2f64b1b1d232d8edf076746e4eddae3abbcbc",
      "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
      "url": "https://arxiv.org/abs/2602.17547",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17547v1 Announce Type: cross Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.",
      "tags": [
        "papers",
        "language",
        "agents",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17547"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "827d39cd7de649e219625b933580bcec24f68a61e49a12d59afb89e7e5bb32ff",
      "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting",
      "url": "https://arxiv.org/abs/2602.17645",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17645v1 Announce Type: cross Abstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.",
      "tags": [
        "papers",
        "language",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17645"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "84e8810c6090119616ebf11cffbbe36b36d96ba5d25b09cf6ae9f1349346c3c1",
      "title": "CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts",
      "url": "https://arxiv.org/abs/2602.17663",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.17663v1 Announce Type: cross Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (\"Has the person ever been at this place?\") and $isAt$ (\"Is the person located at this place around publication time?\") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.17663"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8ad3f555838d5873abb7b10b5d2ef9dd58ba79530315b61e04eae33327e89f75",
      "title": "Improving Stance Detection by Leveraging Measurement Knowledge from Social Sciences: A Case Study of Dutch Political Tweets and Traditional Gender Role Division",
      "url": "https://arxiv.org/abs/2212.06543",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2212.06543v3 Announce Type: replace Abstract: Stance detection concerns automatically determining the viewpoint (i.e., in favour of, against, or neutral) of a text's author towards a target. Stance detection has been applied to many research topics, among which the detection of stances behind political tweets is an important one. In this paper, we apply stance detection to a dataset of tweets from official party accounts in the Netherlands between 2017 and 2021, with a focus on stances towards traditional gender role division, a dividing issue between (some) Dutch political parties. To implement and improve stance detection of traditional gender role division, we propose to leverage an established survey instrument from social sciences, which has been validated for the purpose of measuring attitudes towards traditional gender role division. Based on our experiments, we show that using such a validated survey instrument helps to improve stance detection performance.",
      "tags": [
        "papers",
        "language",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2212.06543"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "7c83b3935480640a7be75dbeea96abbc2fe16b4174bcc68123fe74d26967544f",
      "title": "Efficient Context Propagating Perceiver Architectures for Auto-Regressive Language Modeling",
      "url": "https://arxiv.org/abs/2412.06106",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2412.06106v2 Announce Type: replace Abstract: One of the key challenges in Transformer architectures is the quadratic complexity of the attention mechanism, which limits the efficient processing of long sequences. Many recent research works have attempted to provide a reduction from the $O(n^2)$ time complexity of attention to semi-linear complexity. However, it remains an unsolved problem in the sense of maintaining high performance when complexity is reduced. One of the important works in this respect is the Perceiver class of architectures that have demonstrated excellent performance, while reducing the computation complexity. In this paper, we use the PerceiverAR as a basis and explore the design space of different trade-offs between preserving context and reducing attention complexity. To this end, we develop four new architectural paradigms, the best performing of which we denote as the Efficient Context propagating Perceiver (ECP). ECP has two major advantages over the PerceiverAR. First, the ECP architecture overcomes the main drawback of PercieverAR by utilizing both the context and the latent sequences in autoregressive training. Second, the ECP architecture operates with the same attention complexity as LongLoRA, making it computationally efficient. More importantly, via pairwise segment attention, it extracts better information resulting in improved language modeling. Empirically, we demonstrate that the ECP architecture significantly outperforms other state-of-the-art Transformer models on Wikitext-103, PG-19 and sCIFAR-10.",
      "tags": [
        "papers",
        "language",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2412.06106"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "c02909c383c8ca26f94bddf69749f8841bd09d884764d95fdef2ef4c58420029",
      "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
      "url": "https://arxiv.org/abs/2502.10361",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2502.10361v2 Announce Type: replace Abstract: Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we develop a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks and mitigating the curse of multilinguality. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.",
      "tags": [
        "papers",
        "language",
        "eval",
        "training",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2502.10361"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "929b09b47ff00bf893e58c5468c6b6e8dbbc31515da32636ab2c4dd82c41c95f",
      "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
      "url": "https://arxiv.org/abs/2505.02819",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.02819v4 Announce Type: replace Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25\\% pruning while retaining approximately 90\\% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead. We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe",
      "tags": [
        "papers",
        "language",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.02819"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "77172e0708f74ec99c4861c0ab77b2330fb47ed0e4ca0143e3e0e209d4f58d89",
      "title": "FinTagging: Benchmarking LLMs for Extracting and Structuring Financial Information",
      "url": "https://arxiv.org/abs/2505.20650",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.20650v4 Announce Type: replace Abstract: Accurate interpretation of numerical data in financial reports is critical for markets and regulators. Although XBRL (eXtensible Business Reporting Language) provides a standard for tagging financial figures, mapping thousands of facts to over 10k US GAAP concepts remains costly and error prone. Existing benchmarks oversimplify this task as flat, single step classification over small subsets of concepts, ignoring the hierarchical semantics of the taxonomy and the structured nature of financial documents. Consequently, these benchmarks fail to evaluate Large Language Models (LLMs) under realistic reporting conditions. To bridge this gap, we introduce FinTagging, the first comprehensive benchmark for structure aware and full scope XBRL tagging. We decompose the complex tagging process into two subtasks: (1) FinNI (Financial Numeric Identification), which extracts entities and types from heterogeneous contexts including text and tables; and (2) FinCL (Financial Concept Linking), which maps extracted entities to the full US GAAP taxonomy. This two stage formulation enables a fair assessment of LLMs' capabilities in numerical reasoning and taxonomy alignment. Evaluating diverse LLMs in zero shot settings reveals that while models generalize well in extraction, they struggle significantly with fine grained concept linking, highlighting critical limitations in domain specific structure aware reasoning.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.20650"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "648da6159e02b7c025708fbdeaa9ed73b9f39ddab05699bf160737879ef4f6d2",
      "title": "A dependently-typed calculus of event telicity and culminativity",
      "url": "https://arxiv.org/abs/2506.06968",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.06968v2 Announce Type: replace Abstract: We present a dependently-typed cross-linguistic framework for analyzing the telicity and culminativity of events, accompanied by examples of using our framework to model English sentences. Our framework consists of two parts. In the nominal domain, we model the boundedness of noun phrases and its relationship to subtyping, delimited quantities, and adjectival modification. In the verbal domain we define a dependent event calculus, modeling telic events as those whose undergoer is bounded, culminating events as telic events that achieve their inherent endpoint, and consider adverbial modification. In both domains we pay particular attention to associated entailments. Our framework is defined as an extension of intensional Martin-L\\\"of dependent type theory, and the rules and examples in this paper have been formalized in the Agda proof assistant.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.06968"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "0675cb20e7b19bb2164781c1bf5e5729bc88c4a573c1a7e49359d864af3ee940",
      "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models",
      "url": "https://arxiv.org/abs/2506.11798",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.11798v3 Announce Type: replace Abstract: Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse but have been found to consistently exhibit a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups with which the base model is not aligned. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict the positions of European groups on a diverse set of policies. We evaluate whether predictions are stable in response to counterfactual arguments, different persona prompts, and generation methods. Finally, we find that we can simulate the voting behavior of Members of the European Parliament reasonably well, achieving a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at the following url: https://github.com/dess-mannheim/european_parliament_simulation.",
      "tags": [
        "papers",
        "language",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.11798"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8ea62bda9df717c9822340bfb0117254b3bab65fb962120cf5f87422d6a1980c",
      "title": "DistillNote: Toward a Functional Evaluation Framework of LLM-Generated Clinical Note Summaries",
      "url": "https://arxiv.org/abs/2506.16777",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.16777v2 Announce Type: replace Abstract: Large language models (LLMs) are increasingly used to generate summaries from clinical notes. However, their ability to preserve essential diagnostic information remains underexplored, which could lead to serious risks for patient care. This study introduces DistillNote, an evaluation framework for LLM summaries that targets their functional utility by applying the generated summary downstream in a complex clinical prediction task, explicitly quantifying how much prediction signal is retained. We generated over 192,000 LLM summaries from MIMIC-IV clinical notes with increasing compression rates: standard, section-wise, and distilled section-wise. Heart failure diagnosis was chosen as the prediction task, as it requires integrating a wide range of clinical signals. LLMs were fine-tuned on both the original notes and their summaries, and their diagnostic performance was compared using the AUROC metric. We contrasted DistillNote's results with evaluations from LLM-as-judge and clinicians, assessing consistency across different evaluation methods. Summaries generated by LLMs maintained a strong level of heart failure diagnostic signal despite substantial compression. Models trained on the most condensed summaries (about 20 times smaller) achieved an AUROC of 0.92, compared to 0.94 with the original note baseline (97 percent retention). Functional evaluation provided a new lens for medical summary assessment, emphasizing clinical utility as a key dimension of quality. DistillNote introduces a new scalable, task-based method for assessing the functional utility of LLM-generated clinical summaries. Our results detail compression-to-performance tradeoffs from LLM clinical summarization for the first time. The framework is designed to be adaptable to other prediction tasks and clinical domains, aiding data-driven decisions about deploying LLM summarizers in real-world healthcare settings.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.16777"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "e72532b2ca06c355693b5b51b9d8b45f39f3b279d0e979cc9fb05f4db6092207",
      "title": "$\\pi$-CoT: Prolog-Initialized Chain-of-Thought Prompting for Multi-Hop Question-Answering",
      "url": "https://arxiv.org/abs/2506.20642",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.20642v2 Announce Type: replace Abstract: Chain-of-Thought (CoT) prompting significantly enhances large language models' (LLMs) problem-solving capabilities, but still struggles with complex multi-hop questions, often falling into circular reasoning patterns or deviating from the logical path entirely. This limitation is particularly acute in retrieval-augmented generation (RAG) settings, where obtaining the right context is critical. We introduce Prolog-Initialized Chain-of-Thought ($\\pi$-CoT), a novel prompting strategy that combines logic programming's structural rigor with language models' flexibility. $\\pi$-CoT reformulates multi-hop questions into Prolog queries decomposed as single-hop sub-queries. These are resolved sequentially, producing intermediate artifacts, with which we initialize the subsequent CoT reasoning procedure. Extensive experiments demonstrate that $\\pi$-CoT significantly outperforms standard RAG and in-context CoT on multi-hop question-answering benchmarks.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.20642"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8be08c3234f8b08c728323e9c52fa8615501e629b25b5c438b0ab17f308b40cd",
      "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks",
      "url": "https://arxiv.org/abs/2507.19634",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2507.19634v3 Announce Type: replace Abstract: Recent advances in large language models have laid the foundation for multimodal LLMs (MLLMs), which unify text, speech, and vision within a single framework. As these models are rapidly evolving toward general-purpose instruction following across diverse and complex tasks, a key frontier is evaluating their crosslingual and multimodal capabilities over both short- and long-form inputs. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on a single modality at a time, rely on short-form inputs, or lack human annotations--hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first crosslingual human-annotated benchmark based on scientific talks on NLP and beyond. MCIF evaluates instruction following in crosslingual, multimodal settings over different input lengths and spans four macro-tasks: recognition, translation, question answering, and summarization. It covers three core modalities (speech, vision, and text) and four diverse languages (English, German, Italian, and Chinese), fully aligned across all dimensions. This parallel design enables a systematic evaluation of MLLMs' abilities to interpret instructions across languages and effectively integrate multimodal contextual information. Our benchmarking and analysis of 23 models highlight universal challenges across modalities and tasks, indicating substantial room for improvement in future MLLMs development. MCIF is released under CC-BY 4.0 license to promote open research.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2507.19634"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "85e028adc9c74288ebbc97a23de439ccf1669fd4d283c3155227f2c7b445a1f2",
      "title": "Tokens with Meaning: A Hybrid Tokenization Approach for Turkish",
      "url": "https://arxiv.org/abs/2508.14292",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2508.14292v2 Announce Type: replace Abstract: Tokenization shapes how language models perceive morphology and meaning in NLP, yet widely used frequency-driven subword tokenizers (e.g., Byte Pair Encoding and WordPiece) can fragment morphologically rich and agglutinative languages in ways that obscure morpheme boundaries. We introduce a linguistically informed hybrid tokenizer for Turkish that combines (i) dictionary-driven morphological segmentation (roots and affixes), (ii) phonological normalization that maps allomorphic variants to shared identifiers, and (iii) a controlled subword fallback for out-of-vocabulary coverage. Concretely, our released Turkish vocabulary contains 22,231 root tokens mapped to 20,000 canonical root identifiers (with leading spaces to mark word boundaries), 72 affix identifiers that cover 177 allomorphic surface forms, and 12,696 subword units; an orthographic case token preserves capitalization without inflating the vocabulary. We evaluate tokenization quality on the TR-MMLU dataset using two linguistic alignment metrics: Turkish Token Percentage (TR~\\%), the proportion of produced tokens that correspond to Turkish lexical/morphemic units under our lexical resources, and Pure Token Percentage (Pure~\\%), the proportion of tokens aligning with unambiguous root/affix boundaries. The proposed tokenizer reaches 90.29\\% TR~\\% and 85.80\\% Pure~\\% on TR-MMLU, substantially exceeding several general-purpose tokenizers. We further validate practical utility with downstream sentence embedding benchmarks under a strict \\emph{random initialization} control to isolate tokenizer inductive bias. Across four matched models (TurkishTokenizer, CosmosGPT2, Mursit, and Tabi), TurkishTokenizer outperforms all baselines on the Turkish STS Benchmark and achieves the strongest overall average on MTEB-TR. It also yields the strongest average accuracy on the TurBLiMP under a centroid-based proxy.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2508.14292"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "7da09b29f015b87f8ca938d1dd45e4925c31b6156c070cdb2ae444cf3e1006f3",
      "title": "CoSpaDi: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning",
      "url": "https://arxiv.org/abs/2509.22075",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2509.22075v4 Announce Type: replace Abstract: Post-training compression of large language models (LLMs) often relies on low-rank weight approximations that represent each column of the weight matrix in a shared low-dimensional subspace. This strategy is computationally efficient but the underlying constraint can be overly rigid for heterogeneous projection weights and may incur avoidable accuracy loss. We propose CoSpaDi (Compression via Sparse Dictionary Learning), a training-free framework that replaces low-rank factorization with a structured sparse decomposition in which each weight matrix is represented as a dense dictionary multiplied by a column-sparse coefficient matrix. This yields a union-of-subspaces model: the columns of the weight matrix are represented as linear combinations of different subsets of dictionary atoms, improving expressiveness at a fixed parameter budget. CoSpaDi is calibration-guided: using a small calibration set, we optimize the factorization to minimize functional reconstruction error of layer outputs rather than weight-space error. An activation-derived Gram orthonormalization reformulates this data-aware objective into a standard dictionary learning problem on transformed weights, and we support both per-layer compression and cross-layer dictionary sharing within groups of similar projections. Across Llama and Qwen model families, CoSpaDi consistently improves the accuracy--compression and perplexity--compression trade-offs over state-of-the-art SVD-based baselines and strong structured pruning baselines at 20-40\\% compression ratios. The resulting structured sparsity enables sparse--dense computation and integrates with post-training quantization of the sparse coefficients.",
      "tags": [
        "papers",
        "language",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.22075"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "add0bcf950d9e8daa4192f7a4385879872147922b6ca6a6669cdea368faed69f",
      "title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity",
      "url": "https://arxiv.org/abs/2510.04080",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.04080v2 Announce Type: replace Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic proximity between text segments under a specific condition, thereby overcoming the ambiguity inherent in traditional STS. However, existing methods are largely confined to discriminative models, failing to fully leverage recent breakthroughs in the NLP community involving Large Language Models (LLMs) and Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this task, as it can directly optimize the non-differentiable Spearman ranking metric and guide the reasoning process required by C-STS. Nevertheless, we find that naively applying listwise RL fails to produce meaningful improvements, as the model struggles with complex, coarse-grained reward signals, leading to optimization difficulties. To address this challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning framework. PoLi-RL employs a two-stage curriculum: it first trains the model with a simple pointwise reward to establish fundamental scoring capabilities, then transitions to a hybrid reward that combines pointwise, pairwise, and listwise objectives to refine the model's ability to discern subtle semantic distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward (PSRR) mechanism that computes ranking rewards in parallel slices, where each slice consists of completions with the same index from different samples. This provides a precise, differentiated learning signal for each individual completion, enabling granular credit assignment and effective optimization. On the official C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18, establishing a new SOTA for the cross-encoder architecture. As the first work to successfully apply RL to C-STS, our study introduces a powerful paradigm for aligning LLMs for complex, ranking-based conditional judgment tasks.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.04080"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "91a381a3cbca3f3eda1afe3da4ba6ae8834b29ed33f840d3cd73c32f39396b6a",
      "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs",
      "url": "https://arxiv.org/abs/2510.08886",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.08886v2 Announce Type: replace Abstract: Going beyond simple text processing, financial auditing requires detecting semantic, structural, and numerical inconsistencies across large-scale disclosures. As financial reports are filed in XBRL, a structured XML format governed by accounting standards, auditing becomes a structured information extraction and reasoning problem involving concept alignment, taxonomy-defined relations, and cross-document consistency. Although large language models (LLMs) show promise on isolated financial tasks, their capability in professional-grade auditing remains unclear. We introduce FinAuditing, a taxonomy-aligned, structure-aware benchmark built from real XBRL filings. It contains 1,102 annotated instances averaging over 33k tokens and defines three tasks: Financial Semantic Matching (FinSM), Financial Relationship Extraction (FinRE), and Financial Mathematical Reasoning (FinMR). Evaluations of 13 state-of-the-art LLMs reveal substantial gaps in concept retrieval, taxonomy-aware relation modeling, and consistent cross-document reasoning. These findings highlight the need for realistic, structure-aware benchmarks. We release the evaluation code at https://github.com/The-FinAI/FinAuditing and the dataset at https://huggingface.co/collections/TheFinAI/finauditing. The task currently serves as the official benchmark of an ongoing public evaluation contest at https://open-finance-lab.github.io/SecureFinAI_Contest_2026/.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "safety",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.08886"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "2de34dd7b5dacce7dd61b8230faeea1bc84c2a5e8758f18f0206c2be707af518",
      "title": "Assessing Web Search Credibility and Response Groundedness in Chat Assistants",
      "url": "https://arxiv.org/abs/2510.13749",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.13749v2 Announce Type: replace Abstract: Chat assistants increasingly integrate web search functionality, enabling them to retrieve and cite external sources. While this promises more reliable answers, it also raises the risk of amplifying misinformation from low-credibility sources. In this paper, we introduce a novel methodology for evaluating assistants' web search behavior, focusing on source credibility and the groundedness of responses with respect to cited sources. Using 100 claims across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity, and Qwen Chat. Our findings reveal differences between the assistants, with Perplexity achieving the highest source credibility, whereas GPT-4o exhibits elevated citation of non-credibility sources on sensitive topics. This work provides the first systematic comparison of commonly used chat assistants for fact-checking behavior, offering a foundation for evaluating AI systems in high-stakes information environments.",
      "tags": [
        "papers",
        "language"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.13749"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "e110b0f7951b51f3da10d272b200938c1009b8943490bc66787243f00b62b3c1",
      "title": "Estonian Native Large Language Model Benchmark",
      "url": "https://arxiv.org/abs/2510.21193",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.21193v2 Announce Type: replace Abstract: The availability of LLM benchmarks for the Estonian language is limited, and a comprehensive evaluation comparing the performance of different LLMs on Estonian tasks has yet to be conducted. We introduce a new benchmark for evaluating LLMs in Estonian, based on seven diverse datasets. These datasets assess general and domain-specific knowledge, understanding of Estonian grammar and vocabulary, summarization abilities, contextual comprehension, and more. The datasets are all generated from native Estonian sources without using machine translation. We compare the performance of base models, instruction-tuned open-source models, and commercial models. Our evaluation includes 6 base models and 26 instruction-tuned models. To assess the results, we employ both human evaluation and LLM-as-a-judge methods. Human evaluation scores showed moderate to high correlation with benchmark evaluations, depending on the dataset. Claude 3.7 Sonnet, used as an LLM judge, demonstrated strong alignment with human ratings, indicating that top-performing LLMs can effectively support the evaluation of Estonian-language models.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.21193"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "06f44fee453044c6877132f543f010516cec6da6395032d621dafe214a4292db",
      "title": "Probability Distributions Computed by Hard-Attention Transformers",
      "url": "https://arxiv.org/abs/2510.27118",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.27118v2 Announce Type: replace Abstract: Most expressivity results for transformers treat them as language recognizers (which accept or reject strings), and not as they are used in practice, as language models (which generate strings autoregressively and probabilistically). We characterize the probability distributions that transformer language models can express. We show that making transformer language recognizers autoregressive can sometimes increase their expressivity, and that making them probabilistic can break equivalences that hold in the non-probabilistic case. Our overall contribution is to tease apart what functions transformers are capable of expressing, in their most common use-case as language models.",
      "tags": [
        "papers",
        "language",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.27118"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "fd0cf1d89c394f07fb85f50196718e0e607caf0bf0f0d97e22f0b46684d9a2cb",
      "title": "State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?",
      "url": "https://arxiv.org/abs/2511.07989",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.07989v2 Announce Type: replace Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.",
      "tags": [
        "papers",
        "language",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.07989"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "84cbdeb80f68090d28628d7d5e754ec79e77ffb8b6172f69a3d6244c3e7b7db0",
      "title": "Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models",
      "url": "https://arxiv.org/abs/2511.18696",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.18696v2 Announce Type: replace Abstract: This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.18696"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "3492ae22a88bc48beb9f5749524b6646b208955d21d13a54661271b4c075ee77",
      "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
      "url": "https://arxiv.org/abs/2512.03870",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.03870v3 Announce Type: replace Abstract: Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
      "tags": [
        "papers",
        "language"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.03870"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "6f20b2e55dc0496129491a0de4bd2209c05c3374c71824be0ba0f41665803051",
      "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models",
      "url": "https://arxiv.org/abs/2512.08646",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.08646v2 Announce Type: replace Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation (>40 million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers. We also find that answers can be obtained for a fraction of the compute cost, by changing the presentation method. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs \\emph{without coding knowledge}. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "compute",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.08646"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "3ff35e766cf4586df608efc4b490933a6f8bd27cc44c6566327a6820688c8aa8",
      "title": "Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution",
      "url": "https://arxiv.org/abs/2512.11108",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.11108v2 Announce Type: replace Abstract: Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find a trade-off between lexical and position biases in our model comparison, with models that score high on one type score low on the other. We also find signs that anomalous explanations are more likely to be biased.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.11108"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "3763ad5a6963879b06113971334778650c3a00b85ec98801a75c7eb56c5809cb",
      "title": "Symphonym: Universal Phonetic Embeddings for Cross-Script Name Matching",
      "url": "https://arxiv.org/abs/2601.06932",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2601.06932v2 Announce Type: replace Abstract: Linking names across historical sources, languages, and writing systems remains a fundamental challenge in digital humanities and geographic information retrieval. Existing approaches require language-specific phonetic algorithms or fail to capture phonetic relationships across different scripts. This paper presents Symphonym, a neural embedding system that maps names from any script into a unified 128-dimensional phonetic space, enabling direct similarity comparison without runtime phonetic conversion. Symphonym uses a Teacher-Student architecture where a Teacher network trained on articulatory phonetic features produces target embeddings, while a Student network learns to approximate these embeddings directly from characters. The Teacher combines Epitran (extended with 100 new language-script mappings), Phonikud for Hebrew, and CharsiuG2P for Chinese, Japanese, and Korean. Training used 32.7 million triplet samples of toponyms spanning 20 writing systems from GeoNames, Wikidata, and Getty Thesaurus of Geographic Names. On the MEHDIE Hebrew-Arabic historical toponym benchmark, Symphonym achieves Recall@10 of 97.6% and MRR of 90.3%, outperforming Levenshtein and Jaro-Winkler baselines (Recall@1: 86.7% vs 81.5% and 78.5%). Evaluation on 12,947 real cross-script training pairs shows 82.6% achieve greater than 0.75 cosine similarity, with best performance on Arabic-Cyrillic (94--100%) and Cyrillic-Latin (94.3%) combinations. The fixed-length embeddings enable efficient retrieval in digital humanities workflows, with a case study on medieval personal names demonstrating effective transfer from modern place names to historical orthographic variation.",
      "tags": [
        "papers",
        "language",
        "eval",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.06932"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "cec9baf2f2b2bee33d5b74888b4a2712f9320561084ce9dea719c0f184cd618a",
      "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction",
      "url": "https://arxiv.org/abs/2601.12815",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2601.12815v5 Announce Type: replace Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.",
      "tags": [
        "papers",
        "language",
        "agents",
        "eval",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.12815"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "1ef4063671238d61d452e919a30db75ab725724431aedbb1fefedd7f705698e6",
      "title": "Proof-RM: A Scalable and Generalizable Reward Model for Math Proof",
      "url": "https://arxiv.org/abs/2602.02377",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.02377v2 Announce Type: replace Abstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality ``**question-proof-check**'' triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating an ``LLM-as-a-RM-for-RM'' approach and balanced token weighting to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.02377"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "6dd3b2ab081788201e68895da62cb1466bb56d6bd6711586d013aecab221b0a4",
      "title": "RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution",
      "url": "https://arxiv.org/abs/2602.06275",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.06275v2 Announce Type: replace Abstract: Explaining closed-source Large Language Model (LLM) outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce \\textbf{Rotary Positional Embedding Linear Local Interpretable Model-agnostic Explanations (RoPE-LIME)}, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in \\textbf{RoPE embedding space} for stable similarity under masking, and (ii) \\textbf{Sparse-$K$} sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.06275"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "41ce32ff8809bda81bbafe59edad4aa7cd83ba1b18a0fdd94f0ebc88ba539d6f",
      "title": "The Subjectivity of Respect in Police Traffic Stops: Modeling Community Perspectives in Body-Worn Camera Footage",
      "url": "https://arxiv.org/abs/2602.10339",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.10339v2 Announce Type: replace Abstract: Traffic stops are among the most frequent police-civilian interactions, and body-worn cameras (BWCs) provide a unique record of how these encounters unfold. Respect is a central dimension of these interactions, shaping public trust and perceived legitimacy, yet its interpretation is inherently subjective and shaped by lived experience, rendering community-specific perspectives a critical consideration. Leveraging unprecedented access to Los Angeles Police Department BWC footage, we introduce the first large-scale traffic-stop dataset annotated with respect ratings and free-text rationales from multiple perspectives. By sampling annotators from police-affiliated, justice-system-impacted, and non-affiliated Los Angeles residents, we enable the systematic study of perceptual differences across diverse communities. To this end, we (i) develop a domain-specific evaluation rubric grounded in procedural justice theory, LAPD training materials, and extensive fieldwork; (ii) introduce a rubric-driven preference data construction framework for perspective-consistent alignment; and (iii) propose a perspective-aware modeling framework that predicts personalized respect ratings and generates annotator-specific rationales for both officers and civilian drivers from traffic-stop transcripts. Across all three annotator groups, our approach improves both rating prediction performance and rationale alignment. Our perspective-aware framework enables law enforcement to better understand diverse community expectations, providing a vital tool for building public trust and procedural legitimacy.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.10339"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "39e1d3fcc5e4f961558725ea8ac36b540233aa8c2b0dab54c833bf2ec76c87f7",
      "title": "LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules",
      "url": "https://arxiv.org/abs/2602.10993",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.10993v2 Announce Type: replace Abstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.",
      "tags": [
        "papers",
        "language",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.10993"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "114077218291496ec3567ac91a4d42d921ae96d9e561713bf2cf2986aaedcb4e",
      "title": "propella-1: Multi-Property Document Annotation for LLM Data Curation at Scale",
      "url": "https://arxiv.org/abs/2602.12414",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.12414v2 Announce Type: replace Abstract: Since FineWeb-Edu, data curation for LLM pretraining has predominantly relied on single scalar quality scores produced by small classifiers. A single score conflates multiple quality dimensions, prevents flexible filtering, and offers no interpretability. We introduce propella-1, a family of small multilingual LLMs (0.6B, 1.7B, 4B parameters) that annotate text documents across 18 properties organized into six categories: core content, classification, quality and value, audience and purpose, safety and compliance, and geographic relevance. The models support 57 languages and produce structured JSON annotations conforming to a predefined schema. Evaluated against a frontier commercial LLM as a reference annotator, the 4B model achieves higher agreement than much larger general-purpose models. We release propella-annotations, a dataset of over three billion document annotations covering major pretraining corpora including data from FineWeb-2, FinePDFs, HPLT 3.0, and Nemotron-CC. Using these annotations, we present a multi-dimensional compositional analysis of widely used pretraining datasets, revealing substantial differences in quality, reasoning depth, and content composition that single-score approaches cannot capture. All model weights and annotations are released under permissive, commercial-use licenses.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "safety",
        "training",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.12414"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b3360324ba917e54719527e11dc5d62784397bdde770816d0ed743bafdfcab7b",
      "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging",
      "url": "https://arxiv.org/abs/2602.13110",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.13110v2 Announce Type: replace Abstract: Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $\\alpha$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $\\alpha = 0.10$, SCOPE consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to na\\\"ive baselines, SCOPE accepts up to $2.4\\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.13110"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "fc7e112d7ad942381362620285d457dc5f589038ab670552a2788e14c34ba1cd",
      "title": "A Scalable Framework for Evaluating Health Language Models",
      "url": "https://arxiv.org/abs/2503.23339",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2503.23339v3 Announce Type: replace-cross Abstract: Large language models (LLMs) have emerged as powerful tools for analyzing complex datasets. Recent studies demonstrate their potential to generate useful, personalized responses when provided with patient-specific health information that encompasses lifestyle, biomarkers, and context. As LLM-driven health applications are increasingly adopted, rigorous and efficient one-sided evaluation methodologies are crucial to ensure response quality across multiple dimensions, including accuracy, personalization and safety. Current evaluation practices for open-ended text responses heavily rely on human experts. This approach introduces human factors and is often cost-prohibitive, labor-intensive, and hinders scalability, especially in complex domains like healthcare where response assessment necessitates domain expertise and considers multifaceted patient data. In this work, we introduce Adaptive Precise Boolean rubrics: an evaluation framework that streamlines human and automated evaluation of open-ended questions by identifying gaps in model responses using a minimal set of targeted rubrics questions. Our approach is based on recent work in more general evaluation settings that contrasts a smaller set of complex evaluation targets with a larger set of more precise, granular targets answerable with simple boolean responses. We validate this approach in metabolic health, a domain encompassing diabetes, cardiovascular disease, and obesity. Our results demonstrate that Adaptive Precise Boolean rubrics yield higher inter-rater agreement among expert and non-expert human evaluators, and in automated assessments, compared to traditional Likert scales, while requiring approximately half the evaluation time of Likert-based methods. This enhanced efficiency, particularly in automated evaluation and non-expert contributions, paves the way for more extensive and cost-effective evaluation of LLMs in health.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2503.23339"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "44f035c605c4e5ed49e265888c793d19a07d0518425f7411c999b23f7d92b208",
      "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning",
      "url": "https://arxiv.org/abs/2505.17508",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2505.17508v4 Announce Type: replace-cross Abstract: Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). KL regularization is ubiquitous, yet the design surface, choice of KL direction (forward vs. reverse), normalization (normalized vs. unnormalized), and estimator ($k_1/k_2/k_3$), is scattered across the literature and often intertwined with off-policy estimation. We ask a focused question: under the off-policy setting, what weighting is required for each KL variant so that the surrogate we optimize yields the exact gradient of the intended KL-regularized objective? We answer this with a compact, unified derivation we call the Regularized Policy Gradient (RPG) view. RPG (i) unifies normalized and unnormalized KL variants and shows that the widely-used $k_3$ penalty is exactly the unnormalized KL; (ii) specifies conditions under which REINFORCE-style losses with stop-gradient are gradient-equivalent to fully differentiable surrogates; (iii) identifies and corrects an off-policy importance-weighting mismatch in GRPO's KL term; and (iv) introduces RPG-Style Clip, a clipped-importance-sampling step within RPG-REINFORCE that enables stable, off-policy policy-gradient training at scale. On mathematical reasoning benchmarks (AIME24, AIME25), RPG-REINFORCE with RPG-Style Clip improves accuracy by up to $+6$ absolute percentage points over DAPO. We extend our experiments to 8K context length, and RPG-REINFORCE with RPG-Style Clip achieves 52% accuracy on AIME25, surpassing the official Qwen3-4B-Instruct model (47%). Notably, RPG is a stable and scalable RL algorithm for LLM reasoning, realized via (a) a KL-correct objective, (b) clipped importance sampling, and (c) an iterative reference-policy update scheme. Project Page: https://github.com/complex-reasoning/RPG.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.17508"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "043bcdc3e83627018fe4ad29c85b602795b39b213654b368fc2e36e162bf5373",
      "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs",
      "url": "https://arxiv.org/abs/2506.02529",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.02529v2 Announce Type: replace-cross Abstract: Web applications are critical to modern software ecosystems, yet ensuring their reliability remains challenging due to the complexity and dynamic nature of web interfaces. Recent advances in large language models (LLMs) have shown promise in automating complex tasks, but limitations persist in handling dynamic navigation flows and complex form interactions. This paper presents an automated system for generating test cases for two key aspects of web application testing: site navigation and form filling. For site navigation, the system employs screen transition graphs and LLMs to model navigation flows and generate test scenarios. For form filling, it uses state graphs to handle conditional forms and automates Selenium script generation. Key contributions include: (1) a novel integration of graph structures and LLMs for site navigation testing, (2) a state graph-based approach for automating form-filling test cases, and (3) a comprehensive dataset for evaluating form-interaction testing. Experimental results demonstrate the system's effectiveness in improving test coverage and robustness, advancing the state of web application testing.",
      "tags": [
        "papers",
        "language",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.02529"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "780163ca4abfcb64daece9330e434fe545b872effd5d255c3ec3b2ac57a87304",
      "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts",
      "url": "https://arxiv.org/abs/2506.15733",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2506.15733v2 Announce Type: replace-cross Abstract: Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $\\texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $\\sim$19.1\\%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "compute",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.15733"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "7c74a4c8854a440f635f13133618b4d80e85af6f95ee6ff9f8942ce3d1402a7f",
      "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs",
      "url": "https://arxiv.org/abs/2510.09201",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.09201v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.",
      "tags": [
        "papers",
        "language",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.09201"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "32aba0338bee0a7d4762683d3bbe3d481363652d5e7ca55256a1a38028fb47d7",
      "title": "Toward LLM-Supported Automated Assessment of Critical Thinking Subskills",
      "url": "https://arxiv.org/abs/2510.12915",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2510.12915v2 Announce Type: replace-cross Abstract: As the world becomes increasingly saturated with AI-generated content, disinformation, and algorithmic persuasion, critical thinking - the capacity to evaluate evidence, detect unreliable claims, and exercise independent judgment - is becoming a defining human skill. Developing critical thinking skills through timely assessment and feedback is crucial; however, there has not been extensive work in educational data mining on defining, measuring, and supporting critical thinking. In this paper, we investigate the feasibility of measuring \"subskills\" that underlie critical thinking. We ground our work in an authentic task where students operationalize critical thinking by writing argumentative essays. We developed a coding rubric based on an established skills progression and completed human coding for a corpus of student essays. We then evaluated three distinct approaches to automated scoring: zero-shot prompting, few-shot prompting, and supervised fine-tuning, implemented across three large language models (GPT-5, Llama 3.1 8B, and ModernBERT). Fine-tuning Llama 3.1 8B achieved the best results and demonstrated particular strength on subskills with highly separable proficiency levels with balanced labels across levels, while lower performance was observed for subskills that required detection of subtle distinctions between proficiency levels or imbalanced labels. Our exploratory work represents an initial step toward scalable assessment of critical thinking skills across authentic educational contexts. Future research should continue to combine automated critical thinking assessment with human validation to more accurately detect and measure dynamic, higher-order thinking skills.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.12915"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "072e8953b9ab5c02e209063f2e470a35c8c79155f0e1c5f58ad0a3c0d0574ed3",
      "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer",
      "url": "https://arxiv.org/abs/2511.17673",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2511.17673v5 Announce Type: replace-cross Abstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Soft Symbolic Control constitutes a dedicated governance layer within SCL, applying symbolic constraints to probabilistic inference while preserving the flexibility of neural reasoning and restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents.",
      "tags": [
        "papers",
        "language",
        "agents",
        "reasoning",
        "policy",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.17673"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "e751ab8a41eab62c597555a1c77f6ddda85b78e4607b6314a3aab1d07c8b1175",
      "title": "On the Existence and Behavior of Secondary Attention Sinks",
      "url": "https://arxiv.org/abs/2512.22213",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2512.22213v2 Announce Type: replace-cross Abstract: Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.22213"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "5379acdd6fd1ed6eaa893427503406b896157a7ee94e2d25e34c3e93174738ff",
      "title": "Prototype-Based Disentanglement for Controllable Dysarthric Speech Synthesis",
      "url": "https://arxiv.org/abs/2602.08696",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.08696v2 Announce Type: replace-cross Abstract: Dysarthric speech exhibits high variability and limited labeled data, posing major challenges for both automatic speech recognition (ASR) and assistive speech technologies. Existing approaches rely on synthetic data augmentation or speech reconstruction, yet often entangle speaker identity with pathological articulation, limiting controllability and robustness. In this paper, we propose ProtoDisent-TTS, a prototype-based disentanglement TTS framework built on a pre-trained text-to-speech backbone that factorizes speaker timbre and dysarthric articulation within a unified latent space. A pathology prototype codebook provides interpretable and controllable representations of healthy and dysarthric speech patterns, while a dual-classifier objective with a gradient reversal layer enforces invariance of speaker embeddings to pathological attributes. Experiments on the TORGO dataset demonstrate that this design enables bidirectional transformation between healthy and dysarthric speech, leading to consistent ASR performance gains and robust, speaker-aware speech reconstruction.",
      "tags": [
        "papers",
        "language",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.08696"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "866bfc56aebf85ffa60f7ddf08a2667a45947bc508b7f4c4e05001f2ec24127f",
      "title": "EduEVAL-DB: A Role-Based Dataset for Pedagogical Risk Evaluation in Educational Explanations",
      "url": "https://arxiv.org/abs/2602.15531",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.15531v2 Announce Type: replace Abstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15531"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5ddd7a92a8850c0da2806ad1e1ea37244e8057a0a3b49acf5fc6c77517f62499",
      "title": "Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization",
      "url": "https://arxiv.org/abs/2602.15277",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.15277v2 Announce Type: replace-cross Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration--Exploitation Distillation (E$^2$D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E$^2$D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being $18\\times$ faster, and on ImageNet-21K, our method substantially improves accuracy while remaining $4.3\\times$ faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab/E2D.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15277"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f39e877e163d19ea6aaf41b2597d189bd41363707a0f9ab8b84749abcf1a944c",
      "title": "Logit Distance Bounds Representational Similarity",
      "url": "https://arxiv.org/abs/2602.15438",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.15438v2 Announce Type: replace-cross Abstract: For a broad family of discriminative models that includes autoregressive language models, identifiability results imply that if two models induce the same conditional distributions, then their internal representations agree up to an invertible linear transformation. We ask whether an analogous conclusion holds approximately when the distributions are close instead of equal. Building on the observation of Nielsen et al. (2025) that closeness in KL divergence need not imply high linear representational similarity, we study a distributional distance based on logit differences and show that closeness in this distance does yield linear similarity guarantees. Specifically, we define a representational dissimilarity measure based on the models' identifiability class and prove that it is bounded by the logit distance. We further show that, when model probabilities are bounded away from zero, KL divergence upper-bounds logit distance; yet the resulting bound fails to provide nontrivial control in practice. As a consequence, KL-based distillation can match a teacher's predictions while failing to preserve linear representational properties, such as linear-probe recoverability of human-interpretable concepts. In distillation experiments on synthetic and image datasets, logit-distance distillation yields students with higher linear representational similarity and better preservation of the teacher's linearly recoverable concepts.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15438"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ed01d187a8f8381e16e2ea579f600d77574affedffda9227611818d1d0f7dcdd",
      "title": "Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning",
      "url": "https://arxiv.org/abs/2602.15579",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.15579v2 Announce Type: replace-cross Abstract: Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15579"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9f87539316efbff192eef4bba0284501d32474fdfeb99b66f2b5419c83459307",
      "title": "Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning",
      "url": "https://arxiv.org/abs/2602.15868",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.15868v2 Announce Type: replace Abstract: Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15868"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "cf3a09f702c065e04d734e5649a0cef0f67dc018e4cc45d859b00f1ea4ddd027",
      "title": "Are LLMs Ready to Replace Bangla Annotators?",
      "url": "https://arxiv.org/abs/2602.16241",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16241v2 Announce Type: replace Abstract: Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16241"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "6b24fba9146b98bc3a8a9bf6976843119f08ffb495521fa5de50cb5bee2c920f",
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
      "url": "https://arxiv.org/abs/2602.16346",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16346v2 Announce Type: replace Abstract: LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
      "tags": [
        "papers",
        "language",
        "agents",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16346"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "319cba2c26e8109fe00b6218f2a3392e39dbb7b95bb3294ef10ceac143b5369c",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "url": "https://arxiv.org/abs/2602.16699",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-20T05:00:00.000Z",
      "summary": "arXiv:2602.16699v2 Announce Type: replace Abstract: LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "tags": [
        "papers",
        "language",
        "agents",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16699"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "a0d2060a9fd5cca5365c0edf2347dbc8e4469a9a07348648b3466418fc0e39af",
      "title": "Train AI models with Unsloth and Hugging Face Jobs for FREE",
      "url": "https://huggingface.co/blog/unsloth-jobs",
      "sourceId": "hf_blog",
      "sourceName": "Hugging Face — Blog",
      "publishedAt": "2026-02-20T00:00:00.000Z",
      "summary": null,
      "tags": [
        "open-source",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://huggingface.co/blog/unsloth-jobs"
        },
        {
          "label": "Feed",
          "url": "https://huggingface.co/blog/feed.xml"
        }
      ]
    },
    {
      "id": "38de0506cd9f1d711b052b4c754d5a12351fca33b2c7338e03329a8d66c9ebd4",
      "title": "GGML and llama.cpp join HF to ensure the long-term progress of Local AI",
      "url": "https://huggingface.co/blog/ggml-joins-hf",
      "sourceId": "hf_blog",
      "sourceName": "Hugging Face — Blog",
      "publishedAt": "2026-02-20T00:00:00.000Z",
      "summary": null,
      "tags": [
        "open-source"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://huggingface.co/blog/ggml-joins-hf"
        },
        {
          "label": "Feed",
          "url": "https://huggingface.co/blog/feed.xml"
        }
      ]
    },
    {
      "id": "e871e54dd6d4b58fd91692bda5de06f889ea3f177fa05747e83378cbf6a56896",
      "title": "Accelerating Data Processing with NVIDIA Multi-Instance GPU and NUMA Node Localization",
      "url": "https://developer.nvidia.com/blog/accelerating-data-processing-with-nvidia-multi-instance-gpu-and-numa-node-localization/",
      "sourceId": "nvidia_dev_blog",
      "sourceName": "NVIDIA Developer — Blog",
      "publishedAt": "2026-02-19T17:30:00.000Z",
      "summary": "NVIDIA flagship data center GPUs in the NVIDIA Ampere, NVIDIA Hopper, and NVIDIA Blackwell families all feature non-uniform memory access (NUMA) behaviors, but... NVIDIA flagship data center GPUs in the NVIDIA Ampere, NVIDIA Hopper, and NVIDIA Blackwell families all feature non-uniform memory access (NUMA) behaviors, but expose a single memory space. Most programs therefore do not have an issue with memory non-uniformity. However, as bandwidth increases in newer generation GPUs, there are significant performance and power gains to be had when taking into… Source",
      "tags": [
        "compute"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://developer.nvidia.com/blog/accelerating-data-processing-with-nvidia-multi-instance-gpu-and-numa-node-localization/"
        },
        {
          "label": "Feed",
          "url": "https://developer.nvidia.com/blog/feed/"
        }
      ]
    },
    {
      "id": "dfa6f01d2753ac461aba3837e5780c375d8f85259467e0ee11d4a53d603e069e",
      "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
      "url": "https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks/",
      "sourceId": "deepmind_blog",
      "sourceName": "Google DeepMind — Blog",
      "publishedAt": "2026-02-19T16:06:14.000Z",
      "summary": "3.1 Pro is designed for tasks where a simple answer isn’t enough.",
      "tags": [
        "primary",
        "research",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks/"
        },
        {
          "label": "Feed",
          "url": "https://deepmind.google/blog/feed/basic"
        }
      ]
    },
    {
      "id": "90065781d1f3b55b47a4b284cf1c71b833ea4831efbe7591739b2ad9cce3f8fa",
      "title": "Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions",
      "url": "https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/",
      "sourceId": "msr_blog",
      "sourceName": "Microsoft Research — Blog",
      "publishedAt": "2026-02-19T16:00:51.000Z",
      "summary": "As synthetic media grows, verifying what’s real, and the origin of content, matters more than ever. Our latest report explores media integrity and authentication methods, their limits, and practical paths toward trustworthy provenance across images, audio, and video. The post Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions appeared first on Microsoft Research.",
      "tags": [
        "research"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/"
        },
        {
          "label": "Feed",
          "url": "https://www.microsoft.com/en-us/research/blog/feed/"
        }
      ]
    },
    {
      "id": "3c702cb19cf151296e1166f486680477ce7f18b7d5716f861b24f30676ee13e9",
      "title": "「データ不足」の壁を越える：合成ペルソナが日本のAI開発を加速",
      "url": "https://huggingface.co/blog/nvidia/nemotron-personas-japan-nttdata-ja",
      "sourceId": "hf_blog",
      "sourceName": "Hugging Face — Blog",
      "publishedAt": "2026-02-19T15:32:38.000Z",
      "summary": null,
      "tags": [
        "open-source"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://huggingface.co/blog/nvidia/nemotron-personas-japan-nttdata-ja"
        },
        {
          "label": "Feed",
          "url": "https://huggingface.co/blog/feed.xml"
        }
      ]
    },
    {
      "id": "afc45d83363fd3b50a451868ea0c23d1d60a9a1e7f35a39a7a64407e2ccc236e",
      "title": "From Scarcity to Scale: How Synthetic Personas Can Bootstrap Japanese AI Development",
      "url": "https://huggingface.co/blog/nvidia/nemotron-personas-japan-nttdata",
      "sourceId": "hf_blog",
      "sourceName": "Hugging Face — Blog",
      "publishedAt": "2026-02-19T14:57:24.000Z",
      "summary": null,
      "tags": [
        "open-source"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://huggingface.co/blog/nvidia/nemotron-personas-japan-nttdata"
        },
        {
          "label": "Feed",
          "url": "https://huggingface.co/blog/feed.xml"
        }
      ]
    },
    {
      "id": "8504611abb949371cb1d08c40a60340b159b1dfb50280ddcea8423a7a0a2c624",
      "title": "Advancing independent research on AI alignment",
      "url": "https://openai.com/index/advancing-independent-research-ai-alignment",
      "sourceId": "openai_news",
      "sourceName": "OpenAI — News",
      "publishedAt": "2026-02-19T10:00:00.000Z",
      "summary": "OpenAI commits $7.5M to The Alignment Project to fund independent AI alignment research, strengthening global efforts to address AGI safety and security risks.",
      "tags": [
        "primary",
        "release",
        "safety"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://openai.com/index/advancing-independent-research-ai-alignment"
        },
        {
          "label": "Feed",
          "url": "https://openai.com/news/rss.xml"
        }
      ]
    },
    {
      "id": "a0c106b6a93b4f663965c8d09eb09b51817187a63f3e50a9f9093c5e6242939d",
      "title": "Towards Efficient Constraint Handling in Neural Solvers for Routing Problems",
      "url": "https://arxiv.org/abs/2602.16012",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16012v1 Announce Type: new Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.",
      "tags": [
        "papers",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16012"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7e69aaa42d828f46d3e1b61d5dadadeb118edf1ffa2e9f2be810d9c72b4d1b94",
      "title": "Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection",
      "url": "https://arxiv.org/abs/2602.16037",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16037v1 Announce Type: new Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.",
      "tags": [
        "papers",
        "agents",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16037"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "78787aacd02dd977e3b81fd76d27eff599e0f68dc9ee9982bd565160042b368f",
      "title": "How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment",
      "url": "https://arxiv.org/abs/2602.16039",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16039v1 Announce Type: new Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16039"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4cda71c763b8e0c5eee7438b25994a0d2c90a7198a9367b279459cfe924cbcb9",
      "title": "Evidence-Grounded Subspecialty Reasoning: Evaluating a Curated Clinical Intelligence Layer on the 2025 Endocrinology Board-Style Examination",
      "url": "https://arxiv.org/abs/2602.16050",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16050v1 Announce Type: new Abstract: Background: Large language models have demonstrated strong performance on general medical examinations, but subspecialty clinical reasoning remains challenging due to rapidly evolving guidelines and nuanced evidence hierarchies. Methods: We evaluated January Mirror, an evidence-grounded clinical reasoning system, against frontier LLMs (GPT-5, GPT-5.2, Gemini-3-Pro) on a 120-question endocrinology board-style examination. Mirror integrates a curated endocrinology and cardiometabolic evidence corpus with a structured reasoning architecture to generate evidence-linked outputs. Mirror operated under a closed-evidence constraint without external retrieval. Comparator LLMs had real-time web access to guidelines and primary literature. Results: Mirror achieved 87.5% accuracy (105/120; 95% CI: 80.4-92.3%), exceeding a human reference of 62.3% and frontier LLMs including GPT-5.2 (74.6%), GPT-5 (74.0%), and Gemini-3-Pro (69.8%). On the 30 most difficult questions (human accuracy less than 50%), Mirror achieved 76.7% accuracy. Top-2 accuracy was 92.5% for Mirror versus 85.25% for GPT-5.2. Conclusions: Mirror provided evidence traceability: 74.2% of outputs cited at least one guideline-tier source, with 100% citation accuracy on manual verification. Curated evidence with explicit provenance can outperform unconstrained web retrieval for subspecialty clinical reasoning and supports auditability for clinical deployment.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16050"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6fe6183fe5cca8a994bc598bfe904364b371226190169e9144421fda0cece3bf",
      "title": "Improving Interactive In-Context Learning from Natural Language Feedback",
      "url": "https://arxiv.org/abs/2602.16066",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16066v1 Announce Type: new Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.",
      "tags": [
        "papers",
        "reasoning",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16066"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "77ee6439fb7afc417c8aad3c074d3cc20bbd0792f24db21e6efd2f079aed0df2",
      "title": "GPSBench: Do Large Language Models Understand GPS Coordinates?",
      "url": "https://arxiv.org/abs/2602.16105",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16105v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench",
      "tags": [
        "papers",
        "reasoning",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16105"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b404bb9d9a73a3c0a74dc5635d11ab5e0680f5f373c05aba021c38478205f1f0",
      "title": "Learning Personalized Agents from Human Feedback",
      "url": "https://arxiv.org/abs/2602.16173",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16173v1 Announce Type: new Abstract: Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16173"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "55b1c088d77d127e2bd9a87847df29cbf76cec8f14f46ad2973eefe995300a3e",
      "title": "EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments",
      "url": "https://arxiv.org/abs/2602.16179",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16179v1 Announce Type: new Abstract: We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \\corecraft{}, the first environment in \\textsc{EnterpriseGym}, Surge AI's suite of agentic RL environments. \\corecraft{} is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30\\% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM~4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37\\% to 36.76\\% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5\\% on BFCL Parallel, +7.4\\% on $\\tau^2$-Bench Retail, and +6.8\\% on Toolathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16179"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b7633ed960bbf47ea24fec7b0543a0f9cbe825937a7a98afb623ff51f70bd6c7",
      "title": "Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage",
      "url": "https://arxiv.org/abs/2602.16192",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16192v1 Announce Type: new Abstract: Driven by our mission of \"uplifting the world with memory,\" this paper explores the design concept of \"memory\" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed \"extract then store,\" involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the \"store then on-demand extract\" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16192"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "cf5d264a71e1f93c58d682b52a82f2e3a6e16644e77893cc36d20ab2afed0602",
      "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents",
      "url": "https://arxiv.org/abs/2602.16246",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16246v1 Announce Type: new Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "policy",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16246"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4a121d40c2ad5a71ae73c9861497e2fe88210dec7fe7954e7ff681d928d711cf",
      "title": "Multi-agent cooperation through in-context co-player inference",
      "url": "https://arxiv.org/abs/2602.16301",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16301v1 Announce Type: new Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.",
      "tags": [
        "papers",
        "agents",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16301"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7b1d0a3be8d4a35afeaf332c9738d50852143b89240fe5a30702a1e44e8e5471",
      "title": "Verifiable Semantics for Agent-to-Agent Communication",
      "url": "https://arxiv.org/abs/2602.16424",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16424v1 Announce Type: new Abstract: Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms (\"core-guarded reasoning\") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16424"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "091b482047f3c181dbdc6ee200f5f28445f152ab97e63c2019a6fbc80ddffbbe",
      "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.16435",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16435v1 Announce Type: new Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.",
      "tags": [
        "papers",
        "agents",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16435"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c149bf796cdbe95ff863a26ad4a7e49594423e478e81d39a9856dd307c1bb9f4",
      "title": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach",
      "url": "https://arxiv.org/abs/2602.16481",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16481v1 Announce Type: new Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16481"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8f8c5d6de8363d84faa68030e68e15adbd71bd3f5b36c580a0810a79e6e33d35",
      "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
      "url": "https://arxiv.org/abs/2602.16512",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16512v1 Announce Type: new Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.",
      "tags": [
        "papers",
        "reasoning",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16512"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5a701e2e0ca576988bc5bd71af9d0e02f99d2ec199ec7d14f320bd0f3adbd127",
      "title": "Creating a digital poet",
      "url": "https://arxiv.org/abs/2602.16578",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16578v1 Announce Type: new Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.",
      "tags": [
        "papers",
        "training",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16578"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bb43c121097dd2a78081b3b47888b1b415c2ba45b1f881ba8e4c314bda934d1a",
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "url": "https://arxiv.org/abs/2602.16653",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16653v1 Announce Type: new Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16653"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "91b2356f5cf14ab19ae80db643797212ba64c413fb5c4b28f4f322e6b91f0ff2",
      "title": "Towards a Science of AI Agent Reliability",
      "url": "https://arxiv.org/abs/2602.16666",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16666v1 Announce Type: new Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16666"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f22545dc1f7a4ecdf32103e99fae03ff0ec4641c71fcdf23b4f6e224f7668f46",
      "title": "What Persona Are We Missing? Identifying Unknown Relevant Personas for Faithful User Simulation",
      "url": "https://arxiv.org/abs/2602.15832",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15832v1 Announce Type: cross Abstract: Existing user simulations, where models generate user-like responses in dialogue, often lack verification that sufficient user personas are provided, questioning the validity of the simulations. To address this core concern, this work explores the task of identifying relevant but unknown personas of the simulation target for a given simulation context. We introduce PICQ, a novel dataset of context-aware choice questions, annotated with unknown personas (e.g., ''Is the user price-sensitive?'') that may influence user choices, and propose a multi-faceted evaluation scheme assessing fidelity, influence, and inaccessibility. Our benchmark of leading LLMs reveals a complex ''Fidelity vs. Insight'' dilemma governed by model scale: while influence generally scales with model size, fidelity to human patterns follows an inverted U-shaped curve. We trace this phenomenon to cognitive differences, particularly the human tendency for ''cognitive economy.'' Our work provides the first comprehensive benchmark for this crucial task, offering a new lens for understanding the divergent cognitive models of humans and advanced LLMs.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15832"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "085366bfc6bb8672ce44aa45e5de5ff4708bc8ca3b501c9f2a983e862cee1067",
      "title": "EdgeNav-QE: QLoRA Quantization and Dynamic Early Exit for LAM-based Navigation on Edge Devices",
      "url": "https://arxiv.org/abs/2602.15836",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15836v1 Announce Type: cross Abstract: Large Action Models (LAMs) have shown immense potential in autonomous navigation by bridging high-level reasoning with low-level control. However, deploying these multi-billion parameter models on edge devices remains a significant challenge due to memory constraints and latency requirements. In this paper, we propose EdgeNav-QE, a novel framework that integrates Quantized Low-Rank Adaptation (QLoRA) with a dynamic early-exit (DEE) mechanism to optimize LAMs for real-time edge navigation. By quantizing the backbone to 4-bit precision and strategically placing early-exit branches, we enable the model to terminate inference early for simple navigation tasks while retaining full depth for complex decision-making. Experimental results on the Habitat-Sim environment with Matterport3D dataset using OpenVLA-7B backbone, demonstrate that EdgeNav-QE reduces inference latency by 82.7% and memory footprint by 66.7% compared to full-precision baselines, while maintaining 81.8% navigation success rate. Furthermore, it outperforms state-of-the-art static early-exit method by 17.9% in latency, demonstrating the superiority of content-aware adaptive computation for safety-critical applications.",
      "tags": [
        "papers",
        "reasoning",
        "safety",
        "inference",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15836"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c56b0bbd791cbd4b076b43effbea82e914a4e4242ca90e200745382adb2ab543",
      "title": "The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts",
      "url": "https://arxiv.org/abs/2602.15843",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15843v1 Announce Type: cross Abstract: In \"Compress or Route?\" (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r >= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the \"perplexity paradox\" mechanism unvalidated, and provided no adaptive algorithm. This paper addresses all three gaps. First, we validate across six code benchmarks (HumanEval, MBPP, HumanEval+, MultiPL-E) and four reasoning benchmarks (GSM8K, MATH, ARC-Challenge, MMLU-STEM), confirming the compression threshold generalizes across languages and difficulties. Second, we conduct the first per-token perplexity analysis (n=723 tokens), revealing a \"perplexity paradox\": code syntax tokens are preserved (high perplexity) while numerical values in math problems are pruned despite being task-critical (low perplexity). Signature injection recovers +34 percentage points in pass rate (5.3% to 39.3%; Cohen's h=0.890). Third, we propose TAAC (Task-Aware Adaptive Compression), achieving 22% cost reduction with 96% quality preservation, outperforming fixed-ratio compression by 7%. MBPP validation (n=1,800 trials) confirms systematic variation: 3.6% at r=0.3 to 54.6% at r=1.0.",
      "tags": [
        "papers",
        "reasoning",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15843"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4764d1a8f400924735be1efae89d6cd66022f64353f8d26f6512ca24938a0d95",
      "title": "Language Model Representations for Efficient Few-Shot Tabular Classification",
      "url": "https://arxiv.org/abs/2602.15844",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15844v1 Announce Type: cross Abstract: The Web is a rich source of structured data in the form of tables, from product catalogs and knowledge bases to scientific datasets. However, the heterogeneity of the structure and semantics of these tables makes it challenging to build a unified method that can effectively leverage the information they contain. Meanwhile, Large language models (LLMs) are becoming an increasingly integral component of web infrastructure for tasks like semantic search. This raises a crucial question: can we leverage these already-deployed LLMs to classify structured data in web-native tables (e.g., product catalogs, knowledge base exports, scientific data portals), avoiding the need for specialized models or extensive retraining? This work investigates a lightweight paradigm, $\\textbf{Ta}$ble $\\textbf{R}$epresentation with $\\textbf{L}$anguage Model~($\\textbf{TaRL}$), for few-shot tabular classification that directly utilizes semantic embeddings of individual table rows. We first show that naive application of these embeddings underperforms compared to specialized tabular models. We then demonstrate that their potentials can be unlocked with two key techniques: removing the common component from all embeddings and calibrating the softmax temperature. We show that a simple meta-learner, trained on handcrafted features, can learn to predict an appropriate temperature. This approach achieves performance comparable to state-of-the-art models in low-data regimes ($k \\leq 32$) of semantically-rich tables. Our findings demonstrate the viability of reusing existing LLM infrastructure for efficient semantics-driven pathway to reuse existing LLM infrastructure for Web table understanding.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15844"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8853fcb76b7c1e5cc3e4cce50d451b890d67362ee4b1a57066f8f0cccd3ad123",
      "title": "Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models",
      "url": "https://arxiv.org/abs/2602.15847",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15847v1 Announce Type: cross Abstract: Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by analysing the geometric relationships between Big Five personality steering directions. We study steering vectors extracted from two model families (LLaMA-3-8B and Mistral-8B) and apply a range of geometric conditioning schemes, from unconstrained directions to soft and hard orthonormalisation. Our results show that personality steering directions exhibit substantial geometric dependence: steering one trait consistently induces changes in others, even when linear overlap is explicitly removed. While hard orthonormalisation enforces geometric independence, it does not eliminate cross-trait behavioural effects and can reduce steering strength. These findings suggest that personality traits in LLMs occupy a slightly coupled subspace, limiting fully independent trait control.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15847"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f780d236d1053d94a143bb0ae60849c6ef520c514b90b0461ee5264087176670",
      "title": "Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling",
      "url": "https://arxiv.org/abs/2602.15848",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15848v1 Announce Type: cross Abstract: This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against the gold-standard IPIP-50 questionnaire, while also measuring user-perceived accuracy. Results indicate moderate convergent validity (r=0.38-0.58), with Conscientiousness, Openness, and Neuroticism scores statistically equivalent between methods. Agreeableness and Extraversion showed significant differences, suggesting trait-specific calibration is needed. Notably, participants rated LLM-generated profiles as equally accurate as traditional questionnaire results. These findings suggest conversational AI offers a promising new approach to traditional psychometrics.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15848"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ea092f1d73bd71aa6d3b4ada3bc5d7769b4b677c423fc0410ef2e93ece7086db",
      "title": "Preference Optimization for Review Question Generation Improves Writing Quality",
      "url": "https://arxiv.org/abs/2602.15849",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15849v1 Announce Type: cross Abstract: Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "policy",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15849"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9c2054cc6e08739c0f07083affebe11f6ed786f11e80ecc993ff3c257c92f6b4",
      "title": "Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey",
      "url": "https://arxiv.org/abs/2602.15851",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15851v1 Announce Type: cross Abstract: Applications of narrative theories using large language models (LLMs) deliver promising use-cases in automatic story generation and understanding tasks. Our survey examines how natural language processing (NLP) research engages with fields of narrative studies, and proposes a taxonomy for ongoing efforts that reflect established distinctions in narratology. We discover patterns in the following: narrative datasets and tasks, narrative theories and NLP pipeline and methodological trends in prompting and fine-tuning. We highlight how LLMs enable easy connections of NLP pipelines with abstract narrative concepts and opportunities for interdisciplinary collaboration. Challenges remain in attempts to work towards any unified definition or benchmark of narrative related tasks, making model comparison difficult. For future directions, instead of the pursuit of a single, generalised benchmark for 'narrative quality', we believe that progress benefits more from efforts that focus on the following: defining and improving theory-based metrics for individual narrative attributes to incrementally improve model performance; conducting large-scale, theory-driven literary/social/cultural analysis; and creating experiments where outputs can be used to validate or refine narrative theories. This work provides a contextual foundation for more systematic and theoretically informed narrative research in NLP by providing an overview to ongoing research efforts and the broader narrative studies landscape.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15851"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bdd2160741241596df40f3fbc32d2273ccdacffbda98a7270860b0d604795f5a",
      "title": "Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints",
      "url": "https://arxiv.org/abs/2602.15852",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15852v1 Announce Type: cross Abstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, where documentation artifacts encode future clinical decisions and inflate apparent predictive performance. Such behavior poses substantial risks for real-world deployment, where overconfident or temporally invalid predictions can disrupt clinical workflows and compromise patient safety. This study focuses on system-level design choices required to build safe and deployable clinical NLP under temporal leakage constraints. We present a lightweight auditing pipeline that integrates interpretability into the model development process to identify and suppress leakage-prone signals prior to final training. Using next-day discharge prediction after elective spine surgery as a case study, we evaluate how auditing affects predictive behavior, calibration, and safety-relevant trade-offs. Results show that audited models exhibit more conservative and better-calibrated probability estimates, with reduced reliance on discharge-related lexical cues. These findings emphasize that deployment-ready clinical NLP systems should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance.",
      "tags": [
        "papers",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15852"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e877a649cc27e931f63b6b88b869e67db7693ac62164b0be4d29bfb9830d5307",
      "title": "A Lightweight Explainable Guardrail for Prompt Safety",
      "url": "https://arxiv.org/abs/2602.15853",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15853v1 Announce Type: cross Abstract: We propose a lightweight explainable guardrail (LEG) method for the classification of unsafe prompts. LEG uses a multi-task learning architecture to jointly learn a prompt classifier and an explanation classifier, where the latter labels prompt words that explain the safe/unsafe overall decision. LEG is trained using synthetic data for explainability, which is generated using a novel strategy that counteracts the confirmation biases of LLMs. Lastly, LEG's training process uses a novel loss that captures global explanation signals and combines cross-entropy and focal losses with uncertainty-based weighting. LEG obtains equivalent or better performance than the state-of-the-art for both prompt classification and explainability, both in-domain and out-of-domain on three datasets, despite the fact that its model size is considerably smaller than current approaches. If accepted, we will release all models and the annotated dataset publicly.",
      "tags": [
        "papers",
        "safety",
        "training",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15853"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6dcbdc1aea05fee1ff666ae689dab50ead08d838e0858422d4711e43cec87cee",
      "title": "Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization",
      "url": "https://arxiv.org/abs/2602.15854",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15854v1 Announce Type: cross Abstract: Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierarchical reinforcement learning framework that decouples strategy planning from response generation via an Expert Agent and a Customer Service Agent. The Expert Agent optimizes multi-turn goal preferences at the dialogue-trajectory level, while the Customer Service Agent generates responses strictly aligned with the selected strategy. We evaluate GOPO on public benchmarks and e-commerce customer service datasets, and introduce Task-focused Sequential Engagement (TSE), a sequence-level metric derived from real e-commerce interaction data. On the Mgshop dataset, GOPO improves TSE by 7.7% and 10.3% over PPO and Memento, with consistent gains in sequence-level reward and generation quality. Furthermore, a 14B model trained with GOPO achieves 2.7% and 1.5% higher TSE than Qwen-235B and GPT-5.2, respectively. Ablation studies confirm the Expert Agent's critical role in long-horizon optimization. GOPO demonstrates consistent improvements across other datasets as well. This work establishes a new paradigm for task-oriented dialogue systems in commercial scenarios, with code and datasets to be made public.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15854"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "933ca638bf1d60b13915995cac1dec374e33b7badbd2a02cdf6fe7616224dce3",
      "title": "Kalman-Inspired Runtime Stability and Recovery in Hybrid Reasoning Systems",
      "url": "https://arxiv.org/abs/2602.15855",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15855v1 Announce Type: cross Abstract: Hybrid reasoning systems that combine learned components with model-based inference are increasingly deployed in tool-augmented decision loops, yet their runtime behavior under partial observability and sustained evidence mismatch remains poorly understood. In practice, failures often arise as gradual divergence of internal reasoning dynamics rather than as isolated prediction errors. This work studies runtime stability in hybrid reasoning systems from a Kalman-inspired perspective. We model reasoning as a stochastic inference process driven by an internal innovation signal and introduce cognitive drift as a measurable runtime phenomenon. Stability is defined in terms of detectability, bounded divergence, and recoverability rather than task-level correctness. We propose a runtime stability framework that monitors innovation statistics, detects emerging instability, and triggers recovery-aware control mechanisms. Experiments on multi-step, tool-augmented reasoning tasks demonstrate reliable instability detection prior to task failure and show that recovery, when feasible, re-establishes bounded internal behavior within finite time. These results emphasize runtime stability as a system-level requirement for reliable reasoning under uncertainty.",
      "tags": [
        "papers",
        "reasoning",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15855"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "920ff40493179a616fceed7d1fb9b435ec15ccf377e1263a901d8b929163a5b2",
      "title": "Rethinking Soft Compression in Retrieval-Augmented Generation: A Query-Conditioned Selector Perspective",
      "url": "https://arxiv.org/abs/2602.15856",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15856v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) effectively grounds Large Language Models (LLMs) with external knowledge and is widely applied to Web-related tasks. However, its scalability is hindered by excessive context length and redundant retrievals. Recent research on soft context compression aims to address this by encoding long documents into compact embeddings, yet they often underperform non-compressed RAG due to their reliance on auto-encoder-like full-compression that forces the encoder to compress all document information regardless of relevance to the input query. In this work, we conduct an analysis on this paradigm and reveal two fundamental limitations: (I) Infeasibility, full-compression conflicts with the LLM's downstream generation behavior; and (II) Non-necessity: full-compression is unnecessary and dilutes task-relevant information density. Motivated by these insights, we introduce SeleCom, a selector-based soft compression framework for RAG that redefines the encoder's role as query-conditioned information selector. The selector is decoder-only and is trained with a massive, diverse and difficulty-graded synthetic QA dataset with curriculum learning. Extensive experiments show that SeleCom significantly outperforms existing soft compression approaches and achieves competitive or superior performance to non-compression baselines, while reducing computation and latency by 33.8%~84.6%.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15856"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1c57bc4aea1c64d6d5e40383e7f5ede259e4dcc4b1b2f217778d1e11e412a14e",
      "title": "State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models",
      "url": "https://arxiv.org/abs/2602.15858",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15858v1 Announce Type: cross Abstract: As large language models (LLMs) move from static reasoning tasks toward dynamic environments, their success depends on the ability to navigate and respond to an environment that changes as they interact at inference time. An underexplored factor in these settings is the representation of the state. Holding model parameters fixed, we systematically vary three key aspects: (1) state granularity (long form versus summary), (2) structure (natural language versus symbolic), and (3) spatial grounding (text-only versus images or textual map encodings) across sequential decision-making benchmarks. We find that trajectory summarisation improves performance by reducing noise and stabilising long-horizon reasoning. Second, natural language representations are the most robust across models, whereas structured encodings help mainly for models with strong code or structured output priors, such as JSON schemas. Third, while image-inputs show some benefit, text-based spatial encodings prove most effective. This advantage stems not from the spatial information itself, but from the act of construction, which compels the model to perform the spatial reasoning that static input does not elicit. Overall, we demonstrate that design choices for representing state are a decisive factor in performance, distinct from the availability of information itself. We note, however, that even with improved representations, current LLMs and VLMs remain brittle over long horizons, particularly when they must synthesise information to manage multiple subtasks to reach a goal.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15858"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "874b4277b545275e27ac0bdf525c6ff68e3ad525a95d802db66d5105cfd82e9d",
      "title": "CAST: Achieving Stable LLM-based Text Analysis for Data Analytics",
      "url": "https://arxiv.org/abs/2602.15861",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15861v1 Announce Type: cross Abstract: Text analysis of tabular data relies on two core operations: \\emph{summarization} for corpus-level theme extraction and \\emph{tagging} for row-level labeling. A critical limitation of employing large language models (LLMs) for these tasks is their inability to meet the high standards of output stability demanded by data analytics. To address this challenge, we introduce \\textbf{CAST} (\\textbf{C}onsistency via \\textbf{A}lgorithmic Prompting and \\textbf{S}table \\textbf{T}hinking), a framework that enhances output stability by constraining the model's latent reasoning path. CAST combines (i) Algorithmic Prompting to impose a procedural scaffold over valid reasoning transitions and (ii) Thinking-before-Speaking to enforce explicit intermediate commitments before final generation. To measure progress, we introduce \\textbf{CAST-S} and \\textbf{CAST-T}, stability metrics for bulleted summarization and tagging, and validate their alignment with human judgments. Experiments across publicly available benchmarks on multiple LLM backbones show that CAST consistently achieves the best stability among all baselines, improving Stability Score by up to 16.2\\%, while maintaining or improving output quality.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15861"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "64a1a99bd37a6c10407b4a4c0b9f9b965fb25c19290bfe8bd54234b892188296",
      "title": "Enhancing Action and Ingredient Modeling for Semantically Grounded Recipe Generation",
      "url": "https://arxiv.org/abs/2602.15862",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15862v1 Announce Type: cross Abstract: Recent advances in Multimodal Large Language Models (MLMMs) have enabled recipe generation from food images, yet outputs often contain semantically incorrect actions or ingredients despite high lexical scores (e.g., BLEU, ROUGE). To address this gap, we propose a semantically grounded framework that predicts and validates actions and ingredients as internal context for instruction generation. Our two-stage pipeline combines supervised fine-tuning (SFT) with reinforcement fine-tuning (RFT): SFT builds foundational accuracy using an Action-Reasoning dataset and ingredient corpus, while RFT employs frequency-aware rewards to improve long-tail action prediction and ingredient generalization. A Semantic Confidence Scoring and Rectification (SCSR) module further filters and corrects predictions. Experiments on Recipe1M show state-of-the-art performance and markedly improved semantic fidelity.",
      "tags": [
        "papers",
        "reasoning",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15862"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3e23d4c81bc54f0839f31bc47646c8dc96d90944f476c97a3b48ae40acb1f253",
      "title": "Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning",
      "url": "https://arxiv.org/abs/2602.15863",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15863v1 Announce Type: cross Abstract: Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to decide when and how to apply the technique effectively. In this work, we argue that the key benefit arises not from the generated examples themselves but from the act of creating them. To validate this, on reasoning-intensive tasks across diverse LLM architectures, we systematically evaluate three prompting strategies for in-context learning: (1) Zero-shot prompting; (2) Integrated prompting, where LLMs create and solve problems within a single, unified prompt; and (3) Decoupled prompting, where self-generated examples are reused as in-context examples, but the context of their creation itself is excluded. We conduct experiments across five widely used model architectures, demonstrating that Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting. In contrast, Decoupled prompting offers only marginal gains over Zero-shot. Further, for a more in-depth analysis, we conduct an attention analysis and observe significant differences in attention patterns between Integrated and Decoupled prompting. These findings suggest that the advantage of self-generation prompting comes from the process of problem creation, not the examples themselves, providing valuable insights for designing more effective prompting strategies.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15863"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "917bda709a3788b9b47fa069a02ff4fad1f75bcb270f612510d4605caa8fbdf0",
      "title": "AI as Teammate or Tool? A Review of Human-AI Interaction in Decision Support",
      "url": "https://arxiv.org/abs/2602.15865",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15865v1 Announce Type: cross Abstract: The integration of Artificial Intelligence (AI) necessitates determining whether systems function as tools or collaborative teammates. In this study, by synthesizing Human-AI Interaction (HAI) literature, we analyze this distinction across four dimensions: interaction design, trust calibration, collaborative frameworks and healthcare applications. Our analysis reveals that static interfaces and miscalibrated trust limit AI efficacy. Performance hinges on aligning transparency with cognitive workflows, yet a fluency trap often inflates trust without improving decision-making. Consequently, an overemphasis on explainability leaves systems largely passive. Our findings show that current AI systems remain largely passive due to an overreliance on explainability-centric designs and that transitioning AI to an active teammate requires adaptive, context-aware interactions that support shared mental models and the dynamic negotiation of authority between humans and AI.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15865"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "384175cee27bd7beede1259b92977df235470c49ba9ab9019028d04f3981d89f",
      "title": "NLP Privacy Risk Identification in Social Media (NLP-PRISM): A Survey",
      "url": "https://arxiv.org/abs/2602.15866",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15866v1 Announce Type: cross Abstract: Natural Language Processing (NLP) is integral to social media analytics but often processes content containing Personally Identifiable Information (PII), behavioral cues, and metadata raising privacy risks such as surveillance, profiling, and targeted advertising. To systematically assess these risks, we review 203 peer-reviewed papers and propose the NLP Privacy Risk Identification in Social Media (NLP-PRISM) framework, which evaluates vulnerabilities across six dimensions: data collection, preprocessing, visibility, fairness, computational risk, and regulatory compliance. Our analysis shows that transformer models achieve F1-scores ranging from 0.58-0.84, but incur a 1% - 23% drop under privacy-preserving fine-tuning. Using NLP-PRISM, we examine privacy coverage in six NLP tasks: sentiment analysis (16), emotion detection (14), offensive language identification (19), code-mixed processing (39), native language identification (29), and dialect detection (24) revealing substantial gaps in privacy research. We further found a (reduced by 2% - 9%) trade-off in model utility, MIA AUC (membership inference attacks) 0.81, AIA accuracy 0.75 (attribute inference attacks). Finally, we advocate for stronger anonymization, privacy-aware learning, and fairness-driven training to enable ethical NLP in social media contexts.",
      "tags": [
        "papers",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15866"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "93cee6454fcb09b5dbb241acafc1c33c074ac333d9d36bd851dc541c67b2b4da",
      "title": "Playing With AI: How Do State-Of-The-Art Large Language Models Perform in the 1977 Text-Based Adventure Game Zork?",
      "url": "https://arxiv.org/abs/2602.15867",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15867v1 Announce Type: cross Abstract: In this positioning paper, we evaluate the problem-solving and reasoning capabilities of contemporary Large Language Models (LLMs) through their performance in Zork, the seminal text-based adventure game first released in 1977. The game's dialogue-based structure provides a controlled environment for assessing how LLM-based chatbots interpret natural language descriptions and generate appropriate action sequences to succeed in the game. We test the performance of leading proprietary models - ChatGPT, Claude, and Gemini - under both minimal and detailed instructions, measuring game progress through achieved scores as the primary metric. Our results reveal that all tested models achieve less than 10% completion on average, with even the best-performing model (Claude Opus 4.5) reaching only approximately 75 out of 350 possible points. Notably, providing detailed game instructions offers no improvement, nor does enabling ''extended thinking''. Qualitative analysis of the models' reasoning processes reveals fundamental limitations: repeated unsuccessful actions suggesting an inability to reflect on one's own thinking, inconsistent persistence of strategies, and failure to learn from previous attempts despite access to conversation history. These findings suggest substantial limitations in current LLMs' metacognitive abilities and problem-solving capabilities within the domain of text-based games, raising questions about the nature and extent of their reasoning capabilities.",
      "tags": [
        "papers",
        "reasoning",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15867"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "cbc5f72ef8c700b04d6c44ab150c8cce4617c45131bc19510ee947759ed3a20d",
      "title": "Test-Time Adaptation for Tactile-Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.15873",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15873v1 Announce Type: cross Abstract: Tactile-vision-language (TVL) models are increasingly deployed in real-world robotic and multimodal perception tasks, where test-time distribution shifts are unavoidable. Existing test-time adaptation (TTA) methods provide filtering in unimodal settings but lack explicit treatment of modality-wise reliability under asynchronous cross-modal shifts, leaving them brittle when some modalities become unreliable. We study TTA for TVL models under such shifts and propose a reliability-aware framework that estimates per-modality reliability from prediction uncertainty and perturbation-based responses. This shared reliability signal is used to (i) filter unreliable test samples, (ii) adaptively fuse tactile, visual, and language features, and (iii) regularize test-time optimization with a reliability-guided objective. On the TAG-C benchmark and additional TVL scenarios, our approach consistently outperforms strong TTA baselines, achieving accuracy gains of up to 49.9\\% under severe modality corruptions, underscoring the importance of explicit modality-wise reliability modeling for robust test-time adaptation.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15873"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c25ee75cb332d60f0b0398baab0c5da49c3c55a46444c28377e47b3b54f7df60",
      "title": "Fly0: Decoupling Semantic Grounding from Geometric Planning for Zero-Shot Aerial Navigation",
      "url": "https://arxiv.org/abs/2602.15875",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15875v1 Announce Type: cross Abstract: Current Visual-Language Navigation (VLN) methodologies face a trade-off between semantic understanding and control precision. While Multimodal Large Language Models (MLLMs) offer superior reasoning, deploying them as low-level controllers leads to high latency, trajectory oscillations, and poor generalization due to weak geometric grounding. To address these limitations, we propose Fly0, a framework that decouples semantic reasoning from geometric planning. The proposed method operates through a three-stage pipeline: (1) an MLLM-driven module for grounding natural language instructions into 2D pixel coordinates; (2) a geometric projection module that utilizes depth data to localize targets in 3D space; and (3) a geometric planner that generates collision-free trajectories. This mechanism enables robust navigation even when visual contact is lost. By eliminating the need for continuous inference, Fly0 reduces computational overhead and improves system stability. Extensive experiments in simulation and real-world environments demonstrate that Fly0 outperforms state-of-the-art baselines, improving the Success Rate by over 20\\% and reducing Navigation Error (NE) by approximately 50\\% in unstructured environments. Our code is available at https://github.com/xuzhenxing1/Fly0.",
      "tags": [
        "papers",
        "reasoning",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15875"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0698687206bae52ae007193068eac5777c52f8945b41d312428f5a0115da158a",
      "title": "Genetic Generalized Additive Models",
      "url": "https://arxiv.org/abs/2602.15877",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15877v1 Announce Type: cross Abstract: Generalized Additive Models (GAMs) balance predictive accuracy and interpretability, but manually configuring their structure is challenging. We propose using the multi-objective genetic algorithm NSGA-II to automatically optimize GAMs, jointly minimizing prediction error (RMSE) and a Complexity Penalty that captures sparsity, smoothness, and uncertainty. Experiments on the California Housing dataset show that NSGA-II discovers GAMs that outperform baseline LinearGAMs in accuracy or match performance with substantially lower complexity. The resulting models are simpler, smoother, and exhibit narrower confidence intervals, enhancing interpretability. This framework provides a general approach for automated optimization of transparent, high-performing models. The code can be found at https://github.com/KaaustaaubShankar/GeneticAdditiveModels.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15877"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f6a977aaf0cac5a4bce4adb5758c3db4ac9ec1b8c884f90f51b3bbc5459d371d",
      "title": "IT-OSE: Exploring Optimal Sample Size for Industrial Data Augmentation",
      "url": "https://arxiv.org/abs/2602.15878",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15878v1 Announce Type: cross Abstract: In industrial scenarios, data augmentation is an effective approach to improve model performance. However, its benefits are not unidirectionally beneficial. There is no theoretical research or established estimation for the optimal sample size (OSS) in augmentation, nor is there an established metric to evaluate the accuracy of OSS or its deviation from the ground truth. To address these issues, we propose an information-theoretic optimal sample size estimation (IT-OSE) to provide reliable OSS estimation for industrial data augmentation. An interval coverage and deviation (ICD) score is proposed to evaluate the estimated OSS intuitively. The relationship between OSS and dominant factors is theoretically analyzed and formulated, thereby enhancing the interpretability. Experiments show that, compared to empirical estimation, the IT-OSE increases accuracy in classification tasks across baseline models by an average of 4.38%, and reduces MAPE in regression tasks across baseline models by an average of 18.80%. The improvements in downstream model performance are more stable. ICDdev in the ICD score is also reduced by an average of 49.30%. The determinism of OSS is enhanced. Compared to exhaustive search, the IT-OSE achieves the same OSS while reducing computational and data costs by an average of 83.97% and 93.46%. Furthermore, practicality experiments demonstrate that the IT-OSE exhibits generality across representative sensor-based industrial scenarios.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15878"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ffb30d18c1d93e3aaa0e55e9a6bba319bffbeaa7143d1996c5aba6559c607379",
      "title": "FUTURE-VLA: Forecasting Unified Trajectories Under Real-time Execution",
      "url": "https://arxiv.org/abs/2602.15882",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15882v1 Announce Type: cross Abstract: General vision-language models increasingly support unified spatiotemporal reasoning over long video streams, yet deploying such capabilities on robots remains constrained by the prohibitive latency of processing long-horizon histories and generating high-dimensional future predictions. To bridge this gap, we present FUTURE-VLA, a unified architecture that reformulates long-horizon control and future forecasting as a monolithic sequence-generation task. Adopting a dual-sided efficiency paradigm, FUTURE-VLA leverages a temporally adaptive compression strategy to maximize spatiotemporal information density, enabling the ingestion of extensive multi-view histories while maintaining constant inference latency. Simultaneously, it performs latent-space autoregression to align actionable dynamics with reviewable visual look-aheads in a single forward pass. These real-time predictive capabilities further enable a prediction-guided Human-In-the-Loop mechanism via interactive execution gating, allowing operators to dynamically validate behaviors based on interpretable future previews. Extensive evaluations demonstrate that FUTURE-VLA establishes new state-of-the-art performance, attaining success rates of 99.2% on LIBERO, 75.4% on RoboTwin, and 78.0% on a real-world Piper platform, all with a $16\\times$ extended spatiotemporal window while maintaining the inference latency of a single-frame baseline.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15882"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "11416ddfb452ccb8a1aa8fadf41041e5e3fce65826e739621f8f1f91077a33c5",
      "title": "NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing",
      "url": "https://arxiv.org/abs/2602.15888",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15888v1 Announce Type: cross Abstract: Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded dataset demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared with the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. By bridging neuromorphic encoding with state-aware modeling, NeuroSleep provides a scalable solution for always-on sleep analysis in resource-constrained wearable scenarios.",
      "tags": [
        "papers",
        "inference",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15888"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2ded69a392f47fe33c418d9190ac27c347fcb50b1816e35abb752652b7c89b32",
      "title": "Evidence for Daily and Weekly Periodic Variability in GPT-4o Performance",
      "url": "https://arxiv.org/abs/2602.15889",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15889v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly used in research both as tools and as objects of investigation. Much of this work implicitly assumes that LLM performance under fixed conditions (identical model snapshot, hyperparameters, and prompt) is time-invariant. If average output quality changes systematically over time, this assumption is violated, threatening the reliability, validity, and reproducibility of findings. To empirically examine this assumption, we conducted a longitudinal study on the temporal variability of GPT-4o's average performance. Using a fixed model snapshot, fixed hyperparameters, and identical prompting, GPT-4o was queried via the API to solve the same multiple-choice physics task every three hours for approximately three months. Ten independent responses were generated at each time point and their scores were averaged. Spectral (Fourier) analysis of the resulting time series revealed notable periodic variability in average model performance, accounting for approximately 20% of the total variance. In particular, the observed periodic patterns are well explained by the interaction of a daily and a weekly rhythm. These findings indicate that, even under controlled conditions, LLM performance may vary periodically over time, calling into question the assumption of time invariance. Implications for ensuring validity and replicability of research that uses or investigates LLMs are discussed.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15889"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ce457121368551f52b85ad226ab89320945a6c4abc330e35ffacf6810df9261b",
      "title": "Surrogate Modeling for Neutron Transport: A Neural Operator Approach",
      "url": "https://arxiv.org/abs/2602.15890",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15890v1 Announce Type: cross Abstract: This work introduces a neural operator based surrogate modeling framework for neutron transport computation. Two architectures, the Deep Operator Network (DeepONet) and the Fourier Neural Operator (FNO), were trained for fixed source problems to learn the mapping from anisotropic neutron sources, Q(x,{\\mu}), to the corresponding angular fluxes, {\\psi}(x,{\\mu}), in a one-dimensional slab geometry. Three distinct models were trained for each neural operator, corresponding to different scattering ratios (c = 0.1, 0.5, & 1.0), providing insight into their performance across distinct transport regimes (absorption-dominated, moderate, and scattering-dominated). The models were subsequently evaluated on a wide range of previously unseen source configurations, demonstrating that FNO generally achieves higher predictive accuracy, while DeepONet offers greater computational efficiency. Both models offered significant speedups that become increasingly pronounced as the scattering ratio increases, requiring <0.3% of the runtime of a conventional S_N solver. The surrogate models were further incorporated into the S_N k-eigenvalue solver, replacing the computationally intensive transport sweep loop with a single forward pass. Across varying fission cross sections and spatial-angular grids, both neural operator solvers reproduced reference eigenvalues with deviations up to 135 pcm for DeepONet and 112 pcm for FNO, while reducing runtime to <0.1% of that of the S_N solver on relatively fine grids. These results demonstrate the strong potential of neural operator frameworks as accurate, efficient, and generalizable surrogates for neutron transport, paving the way for real-time digital twin applications and repeated evaluations, such as in design optimization.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15890"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "dca989e889d10787ef2d36aaccc9747c5e4d461aa87431c7df240bc2dfe3dac9",
      "title": "Egocentric Bias in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.15892",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15892v1 Announce Type: cross Abstract: Visual perspective taking--inferring how the world appears from another's viewpoint--is foundational to social cognition. We introduce FlipSet, a diagnostic benchmark for Level-2 visual perspective taking (L2 VPT) in vision-language models. The task requires simulating 180-degree rotations of 2D character strings from another agent's perspective, isolating spatial transformation from 3D scene complexity. Evaluating 103 VLMs reveals systematic egocentric bias: the vast majority perform below chance, with roughly three-quarters of errors reproducing the camera viewpoint. Control experiments expose a compositional deficit--models achieve high theory-of-mind accuracy and above-chance mental rotation in isolation, yet fail catastrophically when integration is required. This dissociation indicates that current VLMs lack the mechanisms needed to bind social awareness to spatial operations, suggesting fundamental limitations in model-based spatial reasoning. FlipSet provides a cognitively grounded testbed for diagnosing perspective-taking capabilities in multimodal systems.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15892"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "84ead50d0ff09af60c68462d1e9136203b0a2f0c38d4a57619ded18c32a3ec43",
      "title": "Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion",
      "url": "https://arxiv.org/abs/2602.15895",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15895v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) effectively mitigates hallucinations in LLMs by incorporating external knowledge. However, the inherent discrete representation of text in existing frameworks often results in a loss of semantic integrity, leading to retrieval deviations. Inspired by the human episodic memory mechanism, we propose CogitoRAG, a RAG framework that simulates human cognitive memory processes. The core of this framework lies in the extraction and evolution of the Semantic Gist. During the offline indexing stage, CogitoRAG first deduces unstructured corpora into gist memory corpora, which are then transformed into a multi-dimensional knowledge graph integrating entities, relational facts, and memory nodes. In the online retrieval stage, the framework handles complex queries via Query Decomposition Module that breaks them into comprehensive sub-queries, mimicking the cognitive decomposition humans employ for complex information. Subsequently, Entity Diffusion Module performs associative retrieval across the graph, guided by structural relevance and an entity-frequency reward mechanism. Furthermore, we propose the CogniRank algorithm, which precisely reranks candidate passages by fusing diffusion-derived scores with semantic similarity. The final evidence is delivered to the generator in a passage-memory pairing format, providing high-density information support. Experimental results across five mainstream QA benchmarks and multi-task generation on GraphBench demonstrate that CogitoRAG significantly outperforms state-of-the-art RAG methods, showcasing superior capabilities in complex knowledge integration and reasoning.",
      "tags": [
        "papers",
        "reasoning",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15895"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "abf8c9637bd5ee246551bed6549ae987411d1b0419588ca99548a8055ff5fa10",
      "title": "Doc-to-LoRA: Learning to Instantly Internalize Contexts",
      "url": "https://arxiv.org/abs/2602.15902",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15902v1 Announce Type: cross Abstract: Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior.",
      "tags": [
        "papers",
        "reasoning",
        "compute",
        "inference",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15902"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b3937e0e87f6f7b97fab5f2292624cad28ef4aed0e50a9b7482e7d3797367d62",
      "title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis",
      "url": "https://arxiv.org/abs/2602.15909",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15909v1 Announce Type: cross Abstract: Deep learning-based respiratory auscultation is currently hindered by two fundamental challenges: (i) inherent information loss, as converting signals into spectrograms discards transient acoustic events and clinical context; (ii) limited data availability, exacerbated by severe class imbalance. To bridge these gaps, we present Resp-Agent, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A$^2$CA). Unlike static pipelines, Thinker-A$^2$CA serves as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. To address the representation gap, we introduce a Modality-Weaving Diagnoser that weaves EHR data with audio tokens via Strategic Global Attention and sparse audio anchors, capturing both long-range clinical context and millisecond-level transients. To address the data gap, we design a Flow Matching Generator that adapts a text-only Large Language Model (LLM) via modality injection, decoupling pathological content from acoustic style to synthesize hard-to-diagnose samples. As a foundation for these efforts, we introduce Resp-229k, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that Resp-Agent consistently outperforms prior approaches across diverse evaluation settings, improving diagnostic robustness under data scarcity and long-tailed class imbalance. Our code and data are available at https://github.com/zpforlove/Resp-Agent.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15909"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c357fba81f0a4ed3d181e1e11987e78a210f0c8f9448fd2c824eeea6a7168953",
      "title": "Foundation Models for Medical Imaging: Status, Challenges, and Directions",
      "url": "https://arxiv.org/abs/2602.15913",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15913v1 Announce Type: cross Abstract: Foundation models (FMs) are rapidly reshaping medical imaging, shifting the field from narrowly trained, task-specific networks toward large, general-purpose models that can be adapted across modalities, anatomies, and clinical tasks. In this review, we synthesize the emerging landscape of medical imaging FMs along three major axes: principles of FM design, applications of FMs, and forward-looking challenges and opportunities. Taken together, this review provides a technically grounded, clinically aware, and future-facing roadmap for developing FMs that are not only powerful and versatile but also trustworthy and ready for responsible translation into clinical practice.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15913"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b135eab5aac8789e5da0ae4e560f7bc1da141531a83096a01c1b0e84795e3175",
      "title": "MaS-VQA: A Mask-and-Select Framework for Knowledge-Based Visual Question Answering",
      "url": "https://arxiv.org/abs/2602.15915",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15915v1 Announce Type: cross Abstract: Knowledge-based Visual Question Answering (KB-VQA) requires models to answer questions by integrating visual information with external knowledge. However, retrieved knowledge is often noisy, partially irrelevant, or misaligned with the visual content, while internal model knowledge is difficult to control and interpret. Naive aggregation of these sources limits reasoning effectiveness and reduces answer accuracy. To address this, we propose MaS-VQA, a selection-driven framework that tightly couples explicit knowledge filtering with implicit knowledge reasoning. MaS-VQA first retrieves candidate passages and applies a Mask-and-Select mechanism to jointly prune irrelevant image regions and weakly relevant knowledge fragments, producing compact, high-signal multimodal knowledge . This filtered knowledge then guides the activation of internal knowledge in a constrained semantic space, enabling complementary co-modeling of explicit and implicit knowledge for robust answer prediction. Experiments on Encyclopedic-VQA and InfoSeek demonstrate consistent performance gains across multiple MLLM backbones, and ablations verify that the selection mechanism effectively reduces noise and enhances knowledge utilization.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15915"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9cf7a49b32c5089ce18b709827cdb2fa349876d7f5d7cfc0434d30e79b061cde",
      "title": "EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery",
      "url": "https://arxiv.org/abs/2602.15918",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15918v1 Announce Type: cross Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \\textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15918"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "72cd5bb944220aeec0450a63e9be0b31f6024fb48c24444fac181b2115bfc555",
      "title": "Generalized Leverage Score for Scalable Assessment of Privacy Vulnerability",
      "url": "https://arxiv.org/abs/2602.15919",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15919v1 Announce Type: cross Abstract: Can the privacy vulnerability of individual data points be assessed without retraining models or explicitly simulating attacks? We answer affirmatively by showing that exposure to membership inference attack (MIA) is fundamentally governed by a data point's influence on the learned model. We formalize this in the linear setting by establishing a theoretical correspondence between individual MIA risk and the leverage score, identifying it as a principled metric for vulnerability. This characterization explains how data-dependent sensitivity translates into exposure, without the computational burden of training shadow models. Building on this, we propose a computationally efficient generalization of the leverage score for deep learning. Empirical evaluations confirm a strong correlation between the proposed score and MIA success, validating this metric as a practical surrogate for individual privacy risk assessment.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15919"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bf70abcf1a8d7fda1cecd93eb6f80d552abf826d0aee7639b3742471d33c9c6d",
      "title": "A fully differentiable framework for training proxy Exchange Correlation Functionals for periodic systems",
      "url": "https://arxiv.org/abs/2602.15923",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15923v1 Announce Type: cross Abstract: Density Functional Theory (DFT) is widely used for first-principles simulations in chemistry and materials science, but its computational cost remains a key limitation for large systems. Motivated by recent advances in ML-based exchange-correlation (XC) functionals, this paper introduces a differentiable framework that integrates machine learning models into density functional theory (DFT) for solids and other periodic systems. The framework defines a clean API for neural network models that can act as drop in replacements for conventional exchange-correlation (XC) functionals and enables gradients to flow through the full self-consistent DFT workflow. The framework is implemented in Python using a PyTorch backend, making it fully differentiable and easy to use with standard deep learning tools. We integrate the implementation with the DeepChem library to promote the reuse of established models and to lower the barrier for experimentation. In initial benchmarks against established electronic structure packages (GPAW and PySCF), our models achieve relative errors on the order of 5-10%.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15923"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0de915df7724416afb4d091bbfd4fa74f5a2e146617d2cc08303bd5410101c9a",
      "title": "From Tool Orchestration to Code Execution: A Study of MCP Design Choices",
      "url": "https://arxiv.org/abs/2602.15945",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15945v1 Announce Type: cross Abstract: Model Context Protocols (MCPs) provide a unified platform for agent systems to discover, select, and orchestrate tools across heterogeneous execution environments. As MCP-based systems scale to incorporate larger tool catalogs and multiple concurrently connected MCP servers, traditional tool-by-tool invocation increases coordination overhead, fragments state management, and limits support for wide-context operations. To address these scalability challenges, recent MCP designs have incorporated code execution as a first-class capability, an approach called Code Execution MCP (CE-MCP). This enables agents to consolidate complex workflows, such as SQL querying, file analysis, and multi-step data transformations, into a single program that executes within an isolated runtime environment. In this work, we formalize the architectural distinction between context-coupled (traditional) and context-decoupled (CE-MCP) models, analyzing their fundamental scalability trade-offs. Using the MCP-Bench framework across 10 representative servers, we empirically evaluate task behavior, tool utilization patterns, execution latency, and protocol efficiency as the scale of connected MCP servers and available tools increases, demonstrating that while CE-MCP significantly reduces token usage and execution latency, it introduces a vastly expanded attack surface. We address this security gap by applying the MAESTRO framework, identifying sixteen attack classes across five execution phases-including specific code execution threats such as exception-mediated code injection and unsafe capability synthesis. We validate these vulnerabilities through adversarial scenarios across multiple LLMs and propose a layered defense architecture comprising containerized sandboxing and semantic gating. Our findings provide a rigorous roadmap for balancing scalability and security in production-ready executable agent workflows.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15945"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "006bd3f6caa405682c66189a1fd11044a10a8a349ddc32b10cc10171a2e1ae00",
      "title": "Hybrid Model Predictive Control with Physics-Informed Neural Network for Satellite Attitude Control",
      "url": "https://arxiv.org/abs/2602.15954",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15954v1 Announce Type: cross Abstract: Reliable spacecraft attitude control depends on accurate prediction of attitude dynamics, particularly when model-based strategies such as Model Predictive Control (MPC) are employed, where performance is limited by the quality of the internal system model. For spacecraft with complex dynamics, obtaining accurate physics-based models can be difficult, time-consuming, or computationally heavy. Learning-based system identification presents a compelling alternative; however, models trained exclusively on data frequently exhibit fragile stability properties and limited extrapolation capability. This work explores Physics-Informed Neural Networks (PINNs) for modeling spacecraft attitude dynamics and contrasts it with a conventional data-driven approach. A comprehensive dataset is generated using high-fidelity numerical simulations, and two learning methodologies are investigated: a purely data-driven pipeline and a physics-regularized approach that incorporates prior knowledge into the optimization process. The results indicate that embedding physical constraints during training leads to substantial improvements in predictive reliability, achieving a 68.17% decrease in mean relative error relative. When deployed within an MPC architecture, the physics-informed models yield superior closed-loop tracking performance and improved robustness to uncertainty. Furthermore, a hybrid control formulation that merges the learned nonlinear dynamics with a nominal linear model enables consistent steady-state convergence and significantly faster response, reducing settling times by 61.52%-76.42% under measurement noise and reaction wheel friction.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15954"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ef673f8ddb83e78a20ac15d69b542d9beda14a078d9afd4fbafde13962331abe",
      "title": "DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting",
      "url": "https://arxiv.org/abs/2602.15958",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15958v1 Announce Type: cross Abstract: Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separating a document packet into individual units, remains largely unaddressed. We present the first comprehensive benchmark dataset, DocSplit, along with novel evaluation metrics for assessing the document packet splitting capabilities of large language models. DocSplit comprises five datasets of varying complexity, covering diverse document types, layouts, and multimodal settings. We formalize the DocSplit task, which requires models to identify document boundaries, classify document types, and maintain correct page ordering within a document packet. The benchmark addresses real-world challenges, including out-of-order pages, interleaved documents, and documents lacking clear demarcations. We conduct extensive experiments evaluating multimodal LLMs on our datasets, revealing significant performance gaps in current models' ability to handle complex document splitting tasks. The DocSplit benchmark datasets and proposed novel evaluation metrics provide a systematic framework for advancing document understanding capabilities essential for legal, financial, healthcare, and other document-intensive domains. We release the datasets to facilitate future research in document packet processing.",
      "tags": [
        "papers",
        "eval",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15958"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "698256fc82b07d3a41ef01522db8f06abe78e697f8155afe677b53d8e55596e8",
      "title": "Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration",
      "url": "https://arxiv.org/abs/2602.15959",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15959v1 Announce Type: cross Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing registration methods, constrained by brightness constancy assumptions, achieve limited alignment quality, while recent generative approaches address domain shift through complex architectures that lack temporal awareness across frames. We propose GPEReg-Net, a scene-appearance disentanglement framework that separates domain-invariant scene features from domain-specific appearance codes via Adaptive Instance Normalization (AdaIN), enabling direct image-to-image registration without explicit deformation field estimation. To exploit temporal structure in sequential acquisitions, we introduce a Global Position Encoding (GPE) module that combines learnable position embeddings with sinusoidal encoding and cross-frame attention, allowing the network to leverage context from neighboring frames for improved temporal coherence. On the OR-PAM-Reg-4K benchmark (432 test samples), GPEReg-Net achieves NCC of 0.953, SSIM of 0.932, and PSNR of 34.49dB, surpassing the state-of-the-art by 3.8% in SSIM and 1.99dB in PSNR while maintaining competitive NCC. Code is available at https://github.com/JiahaoQin/GPEReg-Net.",
      "tags": [
        "papers",
        "eval",
        "safety"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15959"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "69e1f8736f4444b0a02d4ffd963bb2e02ccab49e808b3e47cd7eb53d705a9c3a",
      "title": "From Reflection to Repair: A Scoping Review of Dataset Documentation Tools",
      "url": "https://arxiv.org/abs/2602.15968",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15968v1 Announce Type: cross Abstract: Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their adoption. We present a systematic review supported by mixed-methods analysis of 59 dataset documentation publications to examine the motivations behind building documentation tools, how authors conceptualize documentation practices, and how these tools connect to existing systems, regulations, and cultural norms. Our analysis shows four persistent patterns in dataset documentation conceptualization that potentially impede adoption and standardization: unclear operationalizations of documentation's value, decontextualized designs, unaddressed labor demands, and a tendency to treat integration as future work. Building on these findings, we propose a shift in Responsible AI tool design toward institutional rather than individual solutions, and outline actions the HCI community can take to enable sustainable documentation practices.",
      "tags": [
        "papers",
        "policy",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15968"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6cfff8d1c28b455eecdf8886ea85e55d6d01aa3af62618549f559f73262e3976",
      "title": "B-DENSE: Branching For Dense Ensemble Network Learning",
      "url": "https://arxiv.org/abs/2602.15971",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15971v1 Announce Type: cross Abstract: Inspired by non-equilibrium thermodynamics, diffusion models have achieved state-of-the-art performance in generative modeling. However, their iterative sampling nature results in high inference latency. While recent distillation techniques accelerate sampling, they discard intermediate trajectory steps. This sparse supervision leads to a loss of structural information and introduces significant discretization errors. To mitigate this, we propose B-DENSE, a novel framework that leverages multi-branch trajectory alignment. We modify the student architecture to output $K$-fold expanded channels, where each subset corresponds to a specific branch representing a discrete intermediate step in the teacher's trajectory. By training these branches to simultaneously map to the entire sequence of the teacher's target timesteps, we enforce dense intermediate trajectory alignment. Consequently, the student model learns to navigate the solution space from the earliest stages of training, demonstrating superior image generation quality compared to baseline distillation frameworks.",
      "tags": [
        "papers",
        "safety",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15971"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "17e9129d5f0e511beba055f295247f905b11edb5475c624537545129dc004de4",
      "title": "ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization",
      "url": "https://arxiv.org/abs/2602.15983",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15983v1 Announce Type: cross Abstract: Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15983"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5754b3b5081d4b2c168072e2cff481ebe64935f2a9501bfe8d95e9e89428f50d",
      "title": "Anatomy of Capability Emergence: Scale-Invariant Representation Collapse and Top-Down Reorganization in Neural Networks",
      "url": "https://arxiv.org/abs/2602.15997",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15997v1 Announce Type: cross Abstract: Capability emergence during neural network training remains mechanistically opaque. We track five geometric measures across five model scales (405K-85M parameters), 120+ emergence events in eight algorithmic tasks, and three Pythia language models (160M-2.8B). We find: (1) training begins with a universal representation collapse to task-specific floors that are scale-invariant across a 210X parameter range (e.g., modular arithmetic collapses to RANKME ~ 2.0 regardless of model size); (2) collapse propagates top-down through layers (32/32 task X model consistency), contradicting bottom-up feature-building intuition; (3) a geometric hierarchy in which representation geometry leads emergence (75-100% precursor rate for hard tasks), while the local learning coefficient is synchronous (0/24 precursor) and Hessian measures lag. We also delineate prediction limits: geometric measures encode coarse task difficulty but not fine-grained timing (within-class concordance 27%; when task ordering reverses across scales, prediction fails at 26%). On Pythia, global geometric patterns replicate but per-task precursor signals do not -- the precursor relationship requires task-training alignment that naturalistic pre-training does not provide. Our contribution is the geometric anatomy of emergence and its boundary conditions, not a prediction tool.",
      "tags": [
        "papers",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15997"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1297d6b11177963b346a7de2391886777c3f48d9857d6320470d9f63f5bfb93b",
      "title": "ODYN: An All-Shifted Non-Interior-Point Method for Quadratic Programming in Robotics and AI",
      "url": "https://arxiv.org/abs/2602.16005",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16005v1 Announce Type: cross Abstract: We introduce ODYN, a novel all-shifted primal-dual non-interior-point quadratic programming (QP) solver designed to efficiently handle challenging dense and sparse QPs. ODYN combines all-shifted nonlinear complementarity problem (NCP) functions with proximal method of multipliers to robustly address ill-conditioned and degenerate problems, without requiring linear independence of the constraints. It exhibits strong warm-start performance and is well suited to both general-purpose optimization, and robotics and AI applications, including model-based control, estimation, and kernel-based learning methods. We provide an open-source implementation and benchmark ODYN on the Maros-M\\'esz\\'aros test set, demonstrating state-of-the-art convergence performance in small-to-high-scale problems. The results highlight ODYN's superior warm-starting capabilities, which are critical in sequential and real-time settings common in robotics and AI. These advantages are further demonstrated by deploying ODYN as the backend of an SQP-based predictive control framework (OdynSQP), as the implicitly differentiable optimization layer for deep learning (ODYNLayer), and the optimizer of a contact-dynamics simulation (ODYNSim).",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16005"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "92c721828ec03bdd2e1ed44d48cc38dd2d6aed3b229aaf83f3479fb426e7f69d",
      "title": "MAEB: Massive Audio Embedding Benchmark",
      "url": "https://arxiv.org/abs/2602.16008",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16008v1 Announce Type: cross Abstract: We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16008"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f83f9ccaad04b644755476799b81ee77b67163701b2126938740ed6c1917e253",
      "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
      "url": "https://arxiv.org/abs/2602.16019",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16019v1 Announce Type: cross Abstract: Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.",
      "tags": [
        "papers",
        "safety",
        "inference",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16019"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "865d13f7ff1a7afae56e1b9900fb27ed13741ad75dc86e57c4d06bfd8fc77a57",
      "title": "Transforming GenAI Policy to Prompting Instruction: An RCT of Scalable Prompting Interventions in a CS1 Course",
      "url": "https://arxiv.org/abs/2602.16033",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16033v1 Announce Type: cross Abstract: Despite universal GenAI adoption, students cannot distinguish task performance from actual learning and lack skills to leverage AI for learning, leading to worse exam performance when AI use remains unreflective. Yet few interventions teaching students to prompt AI as a tutor rather than solution provider have been validated at scale through randomized controlled trials (RCTs). To bridge this gap, we conducted a semester-long RCT (N=979) with four ICAP framework-based instructional conditions varying in engagement intensity with a pre-test, immediate and delayed post-test and surveys. Mixed methods analysis results showed: (1) All conditions significantly improved prompting skills, with gains increasing progressively from Condition 1 to Condition 4, validating ICAP's cognitive engagement hierarchy; (2) for students with similar pre-test scores, higher learning gain in immediate post-test predict higher final exam score, though no direct between-group differences emerged; (3) Our interventions are suitable and scalable solutions for diverse educational contexts, resources and learners. Together, this study makes empirical and theoretical contributions: (1) theoretically, we provided one of the first large-scale RCTs examining how cognitive engagement shapes learning in prompting literacy and clarifying the relationship between learning-oriented prompting skills and broader academic performance; (2) empirically, we offered timely design guidance for transforming GenAI classroom policies into scalable, actionable prompting literacy instruction to advance learning in the era of Generative AI.",
      "tags": [
        "papers",
        "policy"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16033"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a4dd8d1c6d5680b599aebb7cd656faede237fd1a8a5bd33f0fb07245542b4b2e",
      "title": "AI-CARE: Carbon-Aware Reporting Evaluation Metric for AI Models",
      "url": "https://arxiv.org/abs/2602.16042",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16042v1 Announce Type: cross Abstract: As machine learning (ML) continues its rapid expansion, the environmental cost of model training and inference has become a critical societal concern. Existing benchmarks overwhelmingly focus on standard performance metrics such as accuracy, BLEU, or mAP, while largely ignoring energy consumption and carbon emissions. This single-objective evaluation paradigm is increasingly misaligned with the practical requirements of large-scale deployment, particularly in energy-constrained environments such as mobile devices, developing regions, and climate-aware enterprises. In this paper, we propose AI-CARE, an evaluation tool for reporting energy consumption, and carbon emissions of ML models. In addition, we introduce the carbon-performance tradeoff curve, an interpretable tool that visualizes the Pareto frontier between performance and carbon cost. We demonstrate, through theoretical analysis and empirical validation on representative ML workloads, that carbon-aware benchmarking changes the relative ranking of models and encourages architectures that are simultaneously accurate and environmentally responsible. Our proposal aims to shift the research community toward transparent, multi-objective evaluation and align ML progress with global sustainability goals. The tool and documentation are available at https://github.com/USD-AI-ResearchLab/ai-care.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16042"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "03f7dcc82a1d7abe0c149df1984cac10e456f02ceffb265cbe02ad4f8b43722c",
      "title": "Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training",
      "url": "https://arxiv.org/abs/2602.16065",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16065v1 Announce Type: cross Abstract: Generative Artificial Intelligence (AI), such as large language models (LLMs), has become a transformative force across science, industry, and society. As these systems grow in popularity, web data becomes increasingly interwoven with this AI-generated material and it is increasingly difficult to separate them from naturally generated content. As generative models are updated regularly, later models will inevitably be trained on mixtures of human-generated data and AI-generated data from earlier versions, creating a recursive training process with data contamination. Existing theoretical work has examined only highly simplified settings, where both the real data and the generative model are discrete or Gaussian, where it has been shown that such recursive training leads to model collapse. However, real data distributions are far more complex, and modern generative models are far more flexible than Gaussian and linear mechanisms. To fill this gap, we study recursive training in a general framework with minimal assumptions on the real data distribution and allow the underlying generative model to be a general universal approximator. In this framework, we show that contaminated recursive training still converges, with a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data used in each iteration. To the best of our knowledge, this is the first (positive) theoretical result on recursive training without distributional assumptions on the data. We further extend the analysis to settings where sampling bias is present in data collection and support all theoretical results with empirical studies.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16065"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "784224f0fc17d07e285f1f4e43391c5f26d09c34cbfe96867659ee7ba38181c0",
      "title": "Omni-iEEG: A Large-Scale, Comprehensive iEEG Dataset and Benchmark for Epilepsy Research",
      "url": "https://arxiv.org/abs/2602.16072",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16072v1 Announce Type: cross Abstract: Epilepsy affects over 50 million people worldwide, and one-third of patients suffer drug-resistant seizures where surgery offers the best chance of seizure freedom. Accurate localization of the epileptogenic zone (EZ) relies on intracranial EEG (iEEG). Clinical workflows, however, remain constrained by labor-intensive manual review. At the same time, existing data-driven approaches are typically developed on single-center datasets that are inconsistent in format and metadata, lack standardized benchmarks, and rarely release pathological event annotations, creating barriers to reproducibility, cross-center validation, and clinical relevance. With extensive efforts to reconcile heterogeneous iEEG formats, metadata, and recordings across publicly available sources, we present $\\textbf{Omni-iEEG}$, a large-scale, pre-surgical iEEG resource comprising $\\textbf{302 patients}$ and $\\textbf{178 hours}$ of high-resolution recordings. The dataset includes harmonized clinical metadata such as seizure onset zones, resections, and surgical outcomes, all validated by board-certified epileptologists. In addition, Omni-iEEG provides over 36K expert-validated annotations of pathological events, enabling robust biomarker studies. Omni-iEEG serves as a bridge between machine learning and epilepsy research. It defines clinically meaningful tasks with unified evaluation metrics grounded in clinical priors, enabling systematic evaluation of models in clinically relevant settings. Beyond benchmarking, we demonstrate the potential of end-to-end modeling on long iEEG segments and highlight the transferability of representations pretrained on non-neurophysiological domains. Together, these contributions establish Omni-iEEG as a foundation for reproducible, generalizable, and clinically translatable epilepsy research. The project page with dataset and code links is available at omni-ieeg.github.io/omni-ieeg.",
      "tags": [
        "papers",
        "eval",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16072"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ee0a74344e6f59943b187c802bdc74747f63c6f6ee0ba430634e8ad8bea500bb",
      "title": "ScenicRules: An Autonomous Driving Benchmark with Multi-Objective Specifications and Abstract Scenarios",
      "url": "https://arxiv.org/abs/2602.16073",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16073v1 Announce Type: cross Abstract: Developing autonomous driving systems for complex traffic environments requires balancing multiple objectives, such as avoiding collisions, obeying traffic rules, and making efficient progress. In many situations, these objectives cannot be satisfied simultaneously, and explicit priority relations naturally arise. Also, driving rules require context, so it is important to formally model the environment scenarios within which such rules apply. Existing benchmarks for evaluating autonomous vehicles lack such combinations of multi-objective prioritized rules and formal environment models. In this work, we introduce ScenicRules, a benchmark for evaluating autonomous driving systems in stochastic environments under prioritized multi-objective specifications. We first formalize a diverse set of objectives to serve as quantitative evaluation metrics. Next, we design a Hierarchical Rulebook framework that encodes multiple objectives and their priority relations in an interpretable and adaptable manner. We then construct a compact yet representative collection of scenarios spanning diverse driving contexts and near-accident situations, formally modeled in the Scenic language. Experimental results show that our formalized objectives and Hierarchical Rulebooks align well with human driving judgments and that our benchmark effectively exposes agent failures with respect to the prioritized objectives. Our benchmark can be accessed at https://github.com/BerkeleyLearnVerify/ScenicRules/.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16073"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "90d10c05ac72bb09cb0ae5e557f6b3ad321b1bec333e327cd383f7f8411d80b8",
      "title": "Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs",
      "url": "https://arxiv.org/abs/2602.16085",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16085v1 Announce Type: cross Abstract: Research on mental state reasoning in language models (LMs) has the potential to inform theories of human social cognition--such as the theory that mental state reasoning emerges in part from language exposure--and our understanding of LMs themselves. Yet much published work on LMs relies on a relatively small sample of closed-source LMs, limiting our ability to rigorously test psychological theories and evaluate LM capacities. Here, we replicate and extend published work on the false belief task by assessing LM mental state reasoning behavior across 41 open-weight models (from distinct model families). We find sensitivity to implied knowledge states in 34% of the LMs tested; however, consistent with prior work, none fully ``explain away'' the effect in humans. Larger LMs show increased sensitivity and also exhibit higher psychometric predictive power. Finally, we use LM behavior to generate and test a novel hypothesis about human cognition: both humans and LMs show a bias towards attributing false beliefs when knowledge states are cued using a non-factive verb (``John thinks...'') than when cued indirectly (``John looks in the...''). Unlike the primary effect of knowledge states, where human sensitivity exceeds that of LMs, the magnitude of the human knowledge cue effect falls squarely within the distribution of LM effect sizes-suggesting that distributional statistics of language can in principle account for the latter but not the former in humans. These results demonstrate the value of using larger samples of open-weight LMs to test theories of human cognition and evaluate LM capacities.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16085"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "825fd051dc6c3ff259530acde5ca575d02c0d14a9f64ec39ada4312d15f488fb",
      "title": "Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities",
      "url": "https://arxiv.org/abs/2602.16093",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16093v1 Announce Type: cross Abstract: Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \\methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.",
      "tags": [
        "papers",
        "reasoning",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16093"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5c6cf9a592cfaf2a91e89054975fc5883e8dc304169fb89354c3a823c711d9ca",
      "title": "Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes",
      "url": "https://arxiv.org/abs/2602.16109",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16109v1 Announce Type: cross Abstract: Cross-border insider threats pose a critical challenge to government financial schemes, particularly when dealing with distributed, privacy-sensitive data across multiple jurisdictions. Existing approaches face fundamental limitations: they cannot effectively share intelligence across borders due to privacy constraints, lack reasoning capabilities to understand complex multi-step attack patterns, and fail to capture intricate graph-structured relationships in financial networks. We introduce FedGraph-AGI, a novel federated learning framework integrating Artificial General Intelligence (AGI) reasoning with graph neural networks for privacy-preserving cross-border insider threat detection. Our approach combines: (1) federated graph neural networks preserving data sovereignty; (2) Mixture-of-Experts (MoE) aggregation for heterogeneous jurisdictions; and (3) AGI-powered reasoning via Large Action Models (LAM) performing causal inference over graph data. Through experiments on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves 92.3% accuracy, significantly outperforming federated baselines (86.1%) and centralized approaches (84.7%). Our ablation studies reveal AGI reasoning contributes 6.8% improvement, while MoE adds 4.4%. The system maintains epsilon = 1.0 differential privacy while achieving near-optimal performance and scales efficiently to 50+ clients. This represents the first integration of AGI reasoning with federated graph learning for insider threat detection, opening new directions for privacy-preserving cross-border intelligence sharing.",
      "tags": [
        "papers",
        "reasoning",
        "inference",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16109"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "73ad08524d838b57f202365c95ccc206d140884e6589c2f232af974445877805",
      "title": "OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis",
      "url": "https://arxiv.org/abs/2602.16110",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16110v1 Announce Type: cross Abstract: Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "compute",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16110"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "376c85b35e02aa8f7270f67c3f108151dc8ce07a31766a488847ca3aa69ec13f",
      "title": "Surrogate-Based Prevalence Measurement for Large-Scale A/B Testing",
      "url": "https://arxiv.org/abs/2602.16111",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16111v1 Announce Type: cross Abstract: Online media platforms often need to measure how frequently users are exposed to specific content attributes in order to evaluate trade-offs in A/B experiments. A direct approach is to sample content, label it using a high-quality rubric (e.g., an expert-reviewed LLM prompt), and estimate impression-weighted prevalence. However, repeatedly running such labeling for every experiment arm and segment is too costly and slow to serve as a default measurement at scale. We present a scalable \\emph{surrogate-based prevalence measurement} framework that decouples expensive labeling from per-experiment evaluation. The framework calibrates a surrogate signal to reference labels offline and then uses only impression logs to estimate prevalence for arbitrary experiment arms and segments. We instantiate this framework using \\emph{score bucketing} as the surrogate: we discretize a model score into buckets, estimate bucket-level prevalences from an offline labeled sample, and combine these calibrated bucket level prevalences with the bucket distribution of impressions in each arm to obtain fast, log-based estimates. Across multiple large-scale A/B tests, we validate that the surrogate estimates closely match the reference estimates for both arm-level prevalence and treatment--control deltas. This enables scalable, low-latency prevalence measurement in experimentation without requiring per-experiment labeling jobs.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16111"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6e315d5d97a9b1f7a8e6cda605ccb6564770c6f5b7ca4f01bac7584d62269ac6",
      "title": "Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System",
      "url": "https://arxiv.org/abs/2602.16124",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16124v1 Announce Type: cross Abstract: Approximate nearest neighbor (ANN) search is widely used in the retrieval stage of large-scale recommendation systems. In this stage, candidate items are indexed using their learned embedding vectors, and ANN search is executed for each user (or item) query to retrieve a set of relevant items. However, ANN-based retrieval has two key limitations. First, item embeddings and their indices are typically learned in separate stages: indexing is often performed offline after embeddings are trained, which can yield suboptimal retrieval quality-especially for newly created items. Second, although ANN offers sublinear query time, it must still be run for every request, incurring substantial computation cost at industry scale. In this paper, we propose MultiFaceted Learnable Index (MFLI), a scalable, real-time retrieval paradigm that learns multifaceted item embeddings and indices within a unified framework and eliminates ANN search at serving time. Specifically, we construct a multifaceted hierarchical codebook via residual quantization of item embeddings and co-train the codebook with the embeddings. We further introduce an efficient multifaceted indexing structure and mechanisms that support real-time updates. At serving time, the learned hierarchical indices are used directly to identify relevant items, avoiding ANN search altogether. Extensive experiments on real-world data with billions of users show that MFLI improves recall on engagement tasks by up to 11.8\\%, cold-content delivery by up to 57.29\\%, and semantic relevance by 13.5\\% compared with prior state-of-the-art methods. We also deploy MFLI in the system and report online experimental results demonstrating improved engagement, less popularity bias, and higher serving efficiency.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16124"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e09d4048dcc50ee5ac725307f418191323f45a70cbfa2e8fd5a29ffe9ae8b804",
      "title": "Retrieval Collapses When AI Pollutes the Web",
      "url": "https://arxiv.org/abs/2602.16136",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16136v1 Announce Type: cross Abstract: The rapid proliferation of AI-generated content on the Web presents a structural risk to information retrieval, as search engines and Retrieval-Augmented Generation (RAG) systems increasingly consume evidence produced by the Large Language Models (LLMs). We characterize this ecosystem-level failure mode as Retrieval Collapse, a two-stage process where (1) AI-generated content dominates search results, eroding source diversity, and (2) low-quality or adversarial content infiltrates the retrieval pipeline. We analyzed this dynamic through controlled experiments involving both high-quality SEO-style content and adversarially crafted content. In the SEO scenario, a 67\\% pool contamination led to over 80\\% exposure contamination, creating a homogenized yet deceptively healthy state where answer accuracy remains stable despite the reliance on synthetic sources. Conversely, under adversarial contamination, baselines like BM25 exposed $\\sim$19\\% of harmful content, whereas LLM-based rankers demonstrated stronger suppression capabilities. These findings highlight the risk of retrieval pipelines quietly shifting toward synthetic evidence and the need for retrieval-aware strategies to prevent a self-reinforcing cycle of quality decline in Web-grounded systems.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16136"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c306cc23175473494472587a90cd100deadfa5a24c2a4149c6b88c90a0ee784f",
      "title": "Human-AI Collaboration in Large Language Model-Integrated Building Energy Management Systems: The Role of User Domain Knowledge and AI Literacy",
      "url": "https://arxiv.org/abs/2602.16140",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16140v1 Announce Type: cross Abstract: This study aimed to comprehend how user domain knowledge and artificial intelligence (AI) literacy impact the effective use of human-AI interactive building energy management system (BEMS). While prior studies have investigated the potential of integrating large language models (LLMs) into BEMS or building energy modeling, very few studies have examined how user interact with such systems. We conducted a systematic role-playing experiment, where 85 human subjects interacted with an advanced generative pre-trained transformer (OpenAI GPT-4o). Participants were tasked with identifying the top five behavioral changes that could reduce home energy use with the GPT model that functioned as an LLM-integrated BEMS. Then, the collected prompt-response data and participant conclusions were analyzed using an analytical framework that hierarchically assessed and scored human-AI interactions and their home energy analysis approaches. Also, participants were classified into four groups based on their self-evaluated domain knowledge of building energy use and AI literacy, and Kruskal-Wallis H tests with post-hoc pairwise comparisons were conducted across 20 quantifiable metrics. Key takeaways include: most participants employed concise prompts (median: 16.2 words) and relied heavily on GPT's analytical capabilities; and notably, only 1 of 20 metrics, appliance identification rate, showed statistically significant group differences (p=0.037), driven by AI literacy rather than domain knowledge, suggesting an equalizing effect of LLMs across expertise levels. This study provides foundational insights into human-AI collaboration dynamics and promising development directions in the context of LLM-integrated BEMS and contributes to realizing human-centric LLM-integrated energy systems.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16140"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "68ae39389d3ea135fb80a7e4ad6b8e3435d64a36291c8ec6c0cdad46a2c00b6e",
      "title": "ASPEN: Spectral-Temporal Fusion for Cross-Subject Brain Decoding",
      "url": "https://arxiv.org/abs/2602.16147",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16147v1 Announce Type: cross Abstract: Cross-subject generalization in EEG-based brain-computer interfaces (BCIs) remains challenging due to individual variability in neural signals. We investigate whether spectral representations offer more stable features for cross-subject transfer than temporal waveforms. Through correlation analyses across three EEG paradigms (SSVEP, P300, and Motor Imagery), we find that spectral features exhibit consistently higher cross-subject similarity than temporal signals. Motivated by this observation, we introduce ASPEN, a hybrid architecture that combines spectral and temporal feature streams via multiplicative fusion, requiring cross-modal agreement for features to propagate. Experiments across six benchmark datasets reveal that ASPEN is able to dynamically achieve the optimal spectral-temporal balance depending on the paradigm. ASPEN achieves the best unseen-subject accuracy on three of six datasets and competitive performance on others, demonstrating that multiplicative multimodal fusion enables effective cross-subject generalization.",
      "tags": [
        "papers",
        "eval",
        "compute",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16147"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1b7623431fbbf267594c55814bb1c0d1d49fda46a426a339dbcca27920121047",
      "title": "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution",
      "url": "https://arxiv.org/abs/2602.16154",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16154v1 Announce Type: cross Abstract: Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who \"execute\" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16154"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "cd4429020692a32f555e73257f2d21040d882ddba1070e9fc1f3bdbdb91fd9e2",
      "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents",
      "url": "https://arxiv.org/abs/2602.16165",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16165v1 Announce Type: cross Abstract: Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment. We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation. Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\\% success on ALFWorld and 83.3\\% on WebShop with Qwen2.5-7B-Instruct (+6.6\\% and +8.3\\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "policy",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16165"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3734521c96ddc48df499bd13886a58b7666edc618d35998aa92009b6272c62cc",
      "title": "Edge Learning via Federated Split Decision Transformers for Metaverse Resource Allocation",
      "url": "https://arxiv.org/abs/2602.16174",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16174v1 Announce Type: cross Abstract: Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource allocation for virtual reality users needs to be supported by coordination across MEC servers to harness distributed data. Federated learning (FL) is a promising solution, and can be combined with reinforcement learning (RL) to develop generalized policies across MEC-servers. However, conventional FL incurs transmitting the full model parameters across the MEC-servers and the cloud, and suffer performance degradation due to naive global aggregation, especially in heterogeneous multi-radio access technology environments. To address these challenges, this paper proposes Federated Split Decision Transformer (FSDT), an offline RL framework where the transformer model is partitioned between MEC servers and the cloud. Agent-specific components (e.g., MEC-based embedding and prediction layers) enable local adaptability, while shared global layers in the cloud facilitate cooperative training across MEC servers. Experimental results demonstrate that FSDT enhances QoE for up to 10% in heterogeneous environments compared to baselines, while offloadingnearly 98% of the transformer model parameters to the cloud, thereby reducing the computational burden on MEC servers.",
      "tags": [
        "papers",
        "agents",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16174"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "84cdb14043b3f6951f1f41cfb14e867281368c88f23a61c886acb441861e4899",
      "title": "Conjugate Learning Theory: Uncovering the Mechanisms of Trainability and Generalization in Deep Neural Networks",
      "url": "https://arxiv.org/abs/2602.16177",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16177v1 Announce Type: cross Abstract: In this work, we propose a notion of practical learnability grounded in finite sample settings, and develop a conjugate learning theoretical framework based on convex conjugate duality to characterize this learnability property. Building on this foundation, we demonstrate that training deep neural networks (DNNs) with mini-batch stochastic gradient descent (SGD) achieves global optima of empirical risk by jointly controlling the extreme eigenvalues of a structure matrix and the gradient energy, and we establish a corresponding convergence theorem. We further elucidate the impact of batch size and model architecture (including depth, parameter count, sparsity, skip connections, and other characteristics) on non-convex optimization. Additionally, we derive a model-agnostic lower bound for the achievable empirical risk, theoretically demonstrating that data determines the fundamental limit of trainability. On the generalization front, we derive deterministic and probabilistic bounds on generalization error based on generalized conditional entropy measures. The former explicitly delineates the range of generalization error, while the latter characterizes the distribution of generalization error relative to the deterministic bounds under independent and identically distributed (i.i.d.) sampling conditions. Furthermore, these bounds explicitly quantify the influence of three key factors: (i) information loss induced by irreversibility in the model, (ii) the maximum attainable loss value, and (iii) the generalized conditional entropy of features with respect to labels. Moreover, they offer a unified theoretical lens for understanding the roles of regularization, irreversible transformations, and network depth in shaping the generalization behavior of deep neural networks. Extensive experiments validate all theoretical predictions, confirming the framework's correctness and consistency.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16177"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "aa0a72e06425cd3b40abeab59d0226822ee266eb71089784533ddeeafb4cc87e",
      "title": "SIT-LMPC: Safe Information-Theoretic Learning Model Predictive Control for Iterative Tasks",
      "url": "https://arxiv.org/abs/2602.16187",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16187v1 Announce Type: cross Abstract: Robots executing iterative tasks in complex, uncertain environments require control strategies that balance robustness, safety, and high performance. This paper introduces a safe information-theoretic learning model predictive control (SIT-LMPC) algorithm for iterative tasks. Specifically, we design an iterative control framework based on an information-theoretic model predictive control algorithm to address a constrained infinite-horizon optimal control problem for discrete-time nonlinear stochastic systems. An adaptive penalty method is developed to ensure safety while balancing optimality. Trajectories from previous iterations are utilized to learn a value function using normalizing flows, which enables richer uncertainty modeling compared to Gaussian priors. SIT-LMPC is designed for highly parallel execution on graphics processing units, allowing efficient real-time optimization. Benchmark simulations and hardware experiments demonstrate that SIT-LMPC iteratively improves system performance while robustly satisfying system constraints.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16187"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f75aeacf53bc3a2a05e167f781c2c6402f4657eb52a6e6cfa1ae33d5e22f635b",
      "title": "Beyond Learning: A Training-Free Alternative to Model Adaptation",
      "url": "https://arxiv.org/abs/2602.16189",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16189v1 Announce Type: cross Abstract: Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.",
      "tags": [
        "papers",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16189"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0dcba1aedb8d7e80875d4b9eaa2e95aec6d2fc8ac50504cf093da8c8f299ff3c",
      "title": "Rethinking Input Domains in Physics-Informed Neural Networks via Geometric Compactification Mappings",
      "url": "https://arxiv.org/abs/2602.16193",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16193v1 Announce Type: cross Abstract: Several complex physical systems are governed by multi-scale partial differential equations (PDEs) that exhibit both smooth low-frequency components and localized high-frequency structures. Existing physics-informed neural network (PINN) methods typically train with fixed coordinate system inputs, where geometric misalignment with these structures induces gradient stiffness and ill-conditioning that hinder convergence. To address this issue, we introduce a mapping paradigm that reshapes the input coordinates through differentiable geometric compactification mappings and couples the geometric structure of PDEs with the spectral properties of residual operators. Based on this paradigm, we propose Geometric Compactification (GC)-PINN, a framework that introduces three mapping strategies for periodic boundaries, far-field scale expansion, and localized singular structures in the input domain without modifying the underlying PINN architecture. Extensive empirical evaluation demonstrates that this approach yields more uniform residual distributions and higher solution accuracy on representative 1D and 2D PDEs, while improving training stability and convergence speed.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16193"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "26e568c2b901c931030162e62c65944760282e2d999058262ec049dcc62c97b5",
      "title": "Temporal Panel Selection in Ongoing Citizens' Assemblies",
      "url": "https://arxiv.org/abs/2602.16194",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16194v1 Announce Type: cross Abstract: Permanent citizens' assemblies are ongoing deliberative bodies composed of randomly selected citizens, organized into panels that rotate over time. Unlike one-off panels, which represent the population in a single snapshot, permanent assemblies enable shifting participation across multiple rounds. This structure offers a powerful framework for ensuring that different groups of individuals are represented over time across successive panels. In particular, it allows smaller groups of individuals that may not warrant representation in every individual panel to be represented across a sequence of them. We formalize this temporal sortition framework by requiring proportional representation both within each individual panel and across the sequence of panels. Building on the work of Ebadian and Micha (2025), we consider a setting in which the population lies in a metric space, and the goal is to achieve both proportional representation, ensuring that every group of citizens receives adequate representation, and individual fairness, ensuring that each individual has an equal probability of being selected. We extend the notion of representation to a temporal setting by requiring that every initial segment of the panel sequence, viewed as a cumulative whole, proportionally reflects the structure of the population. We present algorithms that provide varying guarantees of proportional representation, both within individual panels and across any sequence of panels, while also maintaining individual fairness over time.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16194"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "bb1d6a0b90cc4180ff33ce705fdd73a8c8b6419a4c23b1b045c6b31814b90e32",
      "title": "Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.16196",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16196v1 Announce Type: cross Abstract: Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\\texttt{GMFS}$, a $\\textbf{G}$raphon $\\textbf{M}$ean-$\\textbf{F}$ield $\\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $\\kappa$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\\mathrm{poly}(\\kappa)$ and optimality gap $O(1/\\sqrt{\\kappa})$. We verify our theory with numerical simulations in robotic coordination, showing that $\\texttt{GMFS}$ achieves near-optimal performance.",
      "tags": [
        "papers",
        "agents",
        "policy"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16196"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "717b7415be774b7575fdec2af24afd075c68083525703fe8bb5634e3f502a3b1",
      "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
      "url": "https://arxiv.org/abs/2602.16201",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16201v1 Announce Type: cross Abstract: Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives. We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.",
      "tags": [
        "papers",
        "eval",
        "policy",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16201"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7162d709bdb79a4dddb45f78575badf2582c915b8687c091822bbc95144345a2",
      "title": "Geometric Neural Operators via Lie Group-Constrained Latent Dynamics",
      "url": "https://arxiv.org/abs/2602.16209",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16209v1 Announce Type: cross Abstract: Neural operators offer an effective framework for learning solutions of partial differential equations for many physical systems in a resolution-invariant and data-driven manner. Existing neural operators, however, often suffer from instability in multi-layer iteration and long-horizon rollout, which stems from the unconstrained Euclidean latent space updates that violate the geometric and conservation laws. To address this challenge, we propose to constrain manifolds with low-rank Lie algebra parameterization that performs group action updates on the latent representation. Our method, termed Manifold Constraining based on Lie group (MCL), acts as an efficient \\emph{plug-and-play} module that enforces geometric inductive bias to existing neural operators. Extensive experiments on various partial differential equations, such as 1-D Burgers and 2-D Navier-Stokes, over a wide range of parameters and steps demonstrate that our method effectively lowers the relative prediction error by 30-50\\% at the cost of 2.26\\% of parameter increase. The results show that our approach provides a scalable solution for improving long-term prediction fidelity by addressing the principled geometric constraints absent in the neural operator updates.",
      "tags": [
        "papers",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16209"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9cebf1c826ea13ba4ea5c9d3970690516553639b2bb35114a2c1f81262c5518a",
      "title": "Graph neural network for colliding particles with an application to sea ice floe modeling",
      "url": "https://arxiv.org/abs/2602.16213",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16213v1 Announce Type: cross Abstract: This paper introduces a novel approach to sea ice modeling using Graph Neural Networks (GNNs), utilizing the natural graph structure of sea ice, where nodes represent individual ice pieces, and edges model the physical interactions, including collisions. This concept is developed within a one-dimensional framework as a foundational step. Traditional numerical methods, while effective, are computationally intensive and less scalable. By utilizing GNNs, the proposed model, termed the Collision-captured Network (CN), integrates data assimilation (DA) techniques to effectively learn and predict sea ice dynamics under various conditions. The approach was validated using synthetic data, both with and without observed data points, and it was found that the model accelerates the simulation of trajectories without compromising accuracy. This advancement offers a more efficient tool for forecasting in marginal ice zones (MIZ) and highlights the potential of combining machine learning with data assimilation for more effective and efficient modeling.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16213"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b711e769834ca2a134183c5900a8685c8b9e0c5213396dfaf613befebc05dcf5",
      "title": "UCTECG-Net: Uncertainty-aware Convolution Transformer ECG Network for Arrhythmia Detection",
      "url": "https://arxiv.org/abs/2602.16216",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16216v1 Announce Type: cross Abstract: Deep learning has improved automated electrocardiogram (ECG) classification, but limited insight into prediction reliability hinders its use in safety-critical settings. This paper proposes UCTECG-Net, an uncertainty-aware hybrid architecture that combines one-dimensional convolutions and Transformer encoders to process raw ECG signals and their spectrograms jointly. Evaluated on the MIT-BIH Arrhythmia and PTB Diagnostic datasets, UCTECG-Net outperforms LSTM, CNN1D, and Transformer baselines in terms of accuracy, precision, recall and F1 score, achieving up to 98.58% accuracy on MIT-BIH and 99.14% on PTB. To assess predictive reliability, we integrate three uncertainty quantification methods (Monte Carlo Dropout, Deep Ensembles, and Ensemble Monte Carlo Dropout) into all models and analyze their behavior using an uncertainty-aware confusion matrix and derived metrics. The results show that UCTECG-Net, particularly with Ensemble or EMCD, provides more reliable and better-aligned uncertainty estimates than competing architectures, offering a stronger basis for risk-aware ECG decision support.",
      "tags": [
        "papers",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16216"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ade8183330686c242f5b9333a323188c684fcab7256765c338cdebd073f33a09",
      "title": "Are LLMs Ready to Replace Bangla Annotators?",
      "url": "https://arxiv.org/abs/2602.16241",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16241v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16241"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e5df81e4aaf62933b558396f46d1a3e19bbccae295ab461c58efc00e5c609ff9",
      "title": "Color-based Emotion Representation for Speech Emotion Recognition",
      "url": "https://arxiv.org/abs/2602.16256",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16256v1 Announce Type: cross Abstract: Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16256"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "825e9469ae1912434e3aa1d771e742d6a940eb3be4ba3ffdd038d941ec075c08",
      "title": "Generative AI Usage of University Students: Navigating Between Education and Business",
      "url": "https://arxiv.org/abs/2602.16307",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16307v1 Announce Type: cross Abstract: This study investigates generative artificial intelligence (GenAI) usage of university students who study alongside their professional career. Previous literature has paid little attention to part-time students and the intersectional use of GenAI between education and business. This study examines with a grounded theory approach the characteristics of GenAI usage of part-time students. Eleven students from a distance learning university were interviewed. Three causal and four intervening conditions, as well as strategies were identified, to influence the use of GenAI. The study highlights both the potential and challenges of GenAI usage in education and business. While GenAI can significantly enhance productivity and learning outcomes, concerns about ethical implications, reliability, and the risk of academic misconduct persist. The developed grounded model offers a comprehensive understanding of GenAI usage among students, providing valuable insights for educators, policymakers, and developers of GenAI tools seeking to bridge the gap between education and business.",
      "tags": [
        "papers",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16307"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2b1835e59678fdecfd105dcca0b6df523c3f74fc79138efd2654fc9c75fae3f5",
      "title": "The Weight of a Bit: EMFI Sensitivity Analysis of Embedded Deep Learning Models",
      "url": "https://arxiv.org/abs/2602.16309",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16309v1 Announce Type: cross Abstract: Fault injection attacks on embedded neural network models have been shown as a potent threat. Numerous works studied resilience of models from various points of view. As of now, there is no comprehensive study that would evaluate the influence of number representations used for model parameters against electromagnetic fault injection (EMFI) attacks. In this paper, we investigate how four different number representations influence the success of an EMFI attack on embedded neural network models. We chose two common floating-point representations (32-bit, and 16-bit), and two integer representations (8-bit, and 4-bit). We deployed four common image classifiers, ResNet-18, ResNet-34, ResNet-50, and VGG-11, on an embedded memory chip, and utilized a low-cost EMFI platform to trigger faults. Our results show that while floating-point representations exhibit almost a complete degradation in accuracy (Top-1 and Top-5) after a single fault injection, integer representations offer better resistance overall. Especially, when considering the the 8-bit representation on a relatively large network (VGG-11), the Top-1 accuracies stay at around 70% and the Top-5 at around 90%.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16309"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1c2ce41ed2024c0eb1fc2df364731a4a80d2fc426c5f4613e8bbe8367add422f",
      "title": "The Diversity Paradox revisited: Systemic Effects of Feedback Loops in Recommender Systems",
      "url": "https://arxiv.org/abs/2602.16315",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16315v1 Announce Type: cross Abstract: Recommender systems shape individual choices through feedback loops in which user behavior and algorithmic recommendations coevolve over time. The systemic effects of these loops remain poorly understood, in part due to unrealistic assumptions in existing simulation studies. We propose a feedback-loop model that captures implicit feedback, periodic retraining, probabilistic adoption of recommendations, and heterogeneous recommender systems. We apply the framework on online retail and music streaming data and analyze systemic effects of the feedback loop. We find that increasing recommender adoption may lead to a progressive diversification of individual consumption, while collective demand is redistributed in model- and domain-dependent ways, often amplifying popularity concentration. Temporal analyses further reveal that apparent increases in individual diversity observed in static evaluations are illusory: when adoption is fixed and time unfolds, individual diversity consistently decreases across all models. Our results highlight the need to move beyond static evaluations and explicitly account for feedback-loop dynamics when designing recommender systems.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16315"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "44f28057806d01fc040b79793159572f7ea7b3359824153a5207cb78fc868f28",
      "title": "A Graph Meta-Network for Learning on Kolmogorov-Arnold Networks",
      "url": "https://arxiv.org/abs/2602.16316",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16316v1 Announce Type: cross Abstract: Weight-space models learn directly from the parameters of neural networks, enabling tasks such as predicting their accuracy on new datasets. Naive methods -- like applying MLPs to flattened parameters -- perform poorly, making the design of better weight-space architectures a central challenge. While prior work leveraged permutation symmetries in standard networks to guide such designs, no analogous analysis or tailored architecture yet exists for Kolmogorov-Arnold Networks (KANs). In this work, we show that KANs share the same permutation symmetries as MLPs, and propose the KAN-graph, a graph representation of their computation. Building on this, we develop WS-KAN, the first weight-space architecture that learns on KANs, which naturally accounts for their symmetry. We analyze WS-KAN's expressive power, showing it can replicate an input KAN's forward pass - a standard approach for assessing expressiveness in weight-space architectures. We construct a comprehensive ``zoo'' of trained KANs spanning diverse tasks, which we use as benchmarks to empirically evaluate WS-KAN. Across all tasks, WS-KAN consistently outperforms structure-agnostic baselines, often by a substantial margin. Our code is available at https://github.com/BarSGuy/KAN-Graph-Metanetwork.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16316"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "39fd6a5eb57c7c97a27e5004bea8a0d1d08427ded6aa80d3cc327958c28c9386",
      "title": "A Self-Supervised Approach for Enhanced Feature Representations in Object Detection Tasks",
      "url": "https://arxiv.org/abs/2602.16322",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16322v1 Announce Type: cross Abstract: In the fast-evolving field of artificial intelligence, where models are increasingly growing in complexity and size, the availability of labeled data for training deep learning models has become a significant challenge. Addressing complex problems like object detection demands considerable time and resources for data labeling to achieve meaningful results. For companies developing such applications, this entails extensive investment in highly skilled personnel or costly outsourcing. This research work aims to demonstrate that enhancing feature extractors can substantially alleviate this challenge, enabling models to learn more effective representations with less labeled data. Utilizing a self-supervised learning strategy, we present a model trained on unlabeled data that outperforms state-of-the-art feature extractors pre-trained on ImageNet and particularly designed for object detection tasks. Moreover, the results demonstrate that our approach encourages the model to focus on the most relevant aspects of an object, thus achieving better feature representations and, therefore, reinforcing its reliability and robustness.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16322"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b77a77b84f4cbc0155ebe1f19f365f23a6cdaad087d6fdaecf5c54a07034c054",
      "title": "Guide-Guard: Off-Target Predicting in CRISPR Applications",
      "url": "https://arxiv.org/abs/2602.16327",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16327v1 Announce Type: cross Abstract: With the introduction of cyber-physical genome sequencing and editing technologies, such as CRISPR, researchers can more easily access tools to investigate and create remedies for a variety of topics in genetics and health science (e.g. agriculture and medicine). As the field advances and grows, new concerns present themselves in the ability to predict the off-target behavior. In this work, we explore the underlying biological and chemical model from a data driven perspective. Additionally, we present a machine learning based solution named \\textit{Guide-Guard} to predict the behavior of the system given a gRNA in the CRISPR gene-editing process with 84\\% accuracy. This solution is able to be trained on multiple different genes at the same time while retaining accuracy.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16327"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d1ea504d20892ad09084929efb1984a6def07da9672d5dabc0602e3a4e056b78",
      "title": "Spatial Audio Question Answering and Reasoning on Dynamic Source Movements",
      "url": "https://arxiv.org/abs/2602.16334",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16334v1 Announce Type: cross Abstract: Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and directional changes directly from stereo audio. First, we introduce a movement-centric spatial audio augmentation framework that synthesizes diverse motion patterns from isolated mono audio events, enabling controlled and scalable training data generation. Second, we propose an end-to-end multimodal finetuning approach with a thinking mode, which allows audio-language models to produce explicit intermediate reasoning steps before predicting an answer. Third, we investigate the impact of query-conditioned source separation as a preprocessing stage and compare three inference regimes: no masking, an audio grounding model (AGM), and ground-truth masks. Our results show that reasoning amplifies the benefits of source separation, with thinking mode showing significant improvement of +5.1% when a single event is present in the question. These findings highlight the interplay between movement modeling, reasoning, and separation quality, offering new insights for advancing spatial audio understanding.",
      "tags": [
        "papers",
        "reasoning",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16334"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "fd7598ac1953e1528164c0d03a4159374e386aef0009e004766ca1179c391064",
      "title": "HAWX: A Hardware-Aware FrameWork for Fast and Scalable ApproXimation of DNNs",
      "url": "https://arxiv.org/abs/2602.16336",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16336v1 Announce Type: cross Abstract: This work presents HAWX, a hardware-aware scalable exploration framework that employs multi-level sensitivity scoring at different DNN abstraction levels (operator, filter, layer, and model) to guide selective integration of heterogeneous AxC blocks. Supported by predictive models for accuracy, power, and area, HAWX accelerates the evaluation of candidate configurations, achieving over 23* speedup in a layer-level search with two candidate approximate blocks and more than (3*106)* speedup at the filter-level search only for LeNet-5, while maintaining accuracy comparable to exhaustive search. Experiments across state-of-the-art DNN benchmarks such as VGG-11, ResNet-18, and EfficientNetLite demonstrate that the efficiency benefits of HAWX scale exponentially with network size. The HAWX hardware-aware search algorithm supports both spatial and temporal accelerator architectures, leveraging either off-the-shelf approximate components or customized designs.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16336"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1a7018086b231ee81f2d3d3e44beb99018b02ee7b139264443adc254bb17aa6a",
      "title": "Articulated 3D Scene Graphs for Open-World Mobile Manipulation",
      "url": "https://arxiv.org/abs/2602.16356",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16356v1 Announce Type: cross Abstract: Semantics has enabled 3D scene understanding and affordance-driven object interaction. However, robots operating in real-world environments face a critical limitation: they cannot anticipate how objects move. Long-horizon mobile manipulation requires closing the gap between semantics, geometry, and kinematics. In this work, we present MoMa-SG, a novel framework for building semantic-kinematic 3D scene graphs of articulated scenes containing a myriad of interactable objects. Given RGB-D sequences containing multiple object articulations, we temporally segment object interactions and infer object motion using occlusion-robust point tracking. We then lift point trajectories into 3D and estimate articulation models using a novel unified twist estimation formulation that robustly estimates revolute and prismatic joint parameters in a single optimization pass. Next, we associate objects with estimated articulations and detect contained objects by reasoning over parent-child relations at identified opening states. We also introduce the novel Arti4D-Semantic dataset, which uniquely combines hierarchical object semantics including parent-child relation labels with object axis annotations across 62 in-the-wild RGB-D sequences containing 600 object interactions and three distinct observation paradigms. We extensively evaluate the performance of MoMa-SG on two datasets and ablate key design choices of our approach. In addition, real-world experiments on both a quadruped and a mobile manipulator demonstrate that our semantic-kinematic scene graphs enable robust manipulation of articulated objects in everyday home environments. We provide code and data at: https://momasg.cs.uni-freiburg.de.",
      "tags": [
        "papers",
        "reasoning",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16356"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0f3eeec560d71a4b3d46446b974687553676dccd302bf6412b7be83b45756066",
      "title": "AI-Driven Structure Refinement of X-ray Diffraction",
      "url": "https://arxiv.org/abs/2602.16372",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16372v1 Announce Type: cross Abstract: Artificial intelligence can rapidly propose candidate phases and structures from X-ray diffraction (XRD), but these hypotheses often fail in downstream refinement because peak intensities cannot be stably assigned under severe overlap and diffraction consistency is enforced only weakly. Here we introduce WPEM, a physics-constrained whole-pattern decomposition and refinement workflow that turns Bragg's law into an explicit constraint within a batch expectation--maximization framework. WPEM models the full profile as a probabilistic mixture density and iteratively infers component-resolved intensities while keeping peak centres Bragg-consistent, producing a continuous, physically admissible intensity representation that remains stable in heavily overlapped regions and in the presence of mixed radiation or multiple phases. We benchmark WPEM on standard reference patterns (\\ce{PbSO4} and \\ce{Tb2BaCoO5}), where it yields lower $R_{\\mathrm{p}}$/$R_{\\mathrm{wp}}$ than widely used packages (FullProf and TOPAS) under matched refinement conditions. We further demonstrate generality across realistic experimental scenarios, including phase-resolved decomposition of a multiphase Ti--15Nb thin film, quantitative recovery of \\ce{NaCl}--\\ce{Li2CO3} mixture compositions, separation of crystalline peaks from amorphous halos in semicrystalline polymers, high-throughput operando lattice tracking in layered cathodes, automated refinement of a compositionally disordered Ru--Mn oxide solid solution (CCDC 2530452), and quantitative phase-resolved deciphering of an ancient Egyptian make-up sample from synchrotron powder XRD. By providing Bragg-consistent, uncertainty-aware intensity partitioning as a refinement-ready interface, WPEM closes the gap between AI-generated hypotheses and diffraction-admissible structure refinement on challenging XRD data.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16372"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "771765bc07e7afcf0ad166c447677ac36ef3323f6b595bf4aa9a7dfbf9b2c0b0",
      "title": "Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model",
      "url": "https://arxiv.org/abs/2602.16422",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16422v1 Announce Type: cross Abstract: Generating diagnostic text from histopathology whole slide images (WSIs) is challenging due to the gigapixel scale of the input and the requirement for precise, domain specific language. We propose a hierarchical vision language framework that combines a frozen pathology foundation model with a Transformer decoder for report generation. To make WSI processing tractable, we perform multi resolution pyramidal patch selection (downsampling factors 2^3 to 2^6) and remove background and artifacts using Laplacian variance and HSV based criteria. Patch features are extracted with the UNI Vision Transformer and projected to a 6 layer Transformer decoder that generates diagnostic text via cross attention. To better represent biomedical terminology, we tokenize the output using BioGPT. Finally, we add a retrieval based verification step that compares generated reports with a reference corpus using Sentence BERT embeddings; if a high similarity match is found, the generated report is replaced with the retrieved ground truth reference to improve reliability.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16422"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a376034556925532dd216fd478555eee6b9cbee81e398e6ff09f45c2984d2478",
      "title": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems",
      "url": "https://arxiv.org/abs/2602.16430",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16430v1 Announce Type: cross Abstract: Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.",
      "tags": [
        "papers",
        "eval",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16430"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f22ac04567737f7db3ec3882e5a34d110d46e9bf3d482a2ebc36360e67a0f856",
      "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
      "url": "https://arxiv.org/abs/2602.16438",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16438v1 Announce Type: cross Abstract: Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16438"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d1d75f9ecf886b84748047c730fa581780e9280fb4611b842d5e0782c46f8048",
      "title": "Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA",
      "url": "https://arxiv.org/abs/2602.16442",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16442v1 Announce Type: cross Abstract: As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16442"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "35d73c7d019deb681a0290dd5b771b6d02c29eeb5679e13140e96891be2e12a0",
      "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
      "url": "https://arxiv.org/abs/2602.16444",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16444v1 Announce Type: cross Abstract: The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.",
      "tags": [
        "papers",
        "agents",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16444"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "86c45171295154da1b403ca7595ce043592c1363ec42152e403414af48d4b2ef",
      "title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation",
      "url": "https://arxiv.org/abs/2602.16449",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16449v1 Announce Type: cross Abstract: Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "compute",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16449"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c5f4cc92d57f6bbffa00909c0ab1e0c729c66c2ebf1f846df078f85555a611e0",
      "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
      "url": "https://arxiv.org/abs/2602.16467",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16467v1 Announce Type: cross Abstract: The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16467"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8c4e2e8ebac7c839b40798eff0fd814dc4e14ba1f58c6825140fd2143d934630",
      "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
      "url": "https://arxiv.org/abs/2602.16485",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16485v1 Announce Type: cross Abstract: Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16485"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "421c317eacac9d42ecf98fa50ecad288cfd8aa8ea196243acd020448d500be30",
      "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
      "url": "https://arxiv.org/abs/2602.16488",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16488v1 Announce Type: cross Abstract: Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16488"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1dfa490c1beb6567d06c8fd6a30a7fd481b66f60cf55f45f05f014396c3f65e7",
      "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
      "url": "https://arxiv.org/abs/2602.16490",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16490v1 Announce Type: cross Abstract: Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.",
      "tags": [
        "papers",
        "reasoning",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16490"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1a6a3536b47e0426593cf16da350750c42d282371c4790f8bd863fbbd3b64373",
      "title": "Fast and Scalable Analytical Diffusion",
      "url": "https://arxiv.org/abs/2602.16498",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16498v1 Announce Type: cross Abstract: Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\\bf 71 \\times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.",
      "tags": [
        "papers",
        "inference",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16498"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b91c8c09c27820bf75a987b756be6e9f9ad39558a49e59c08cf33b53dce87a09",
      "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
      "url": "https://arxiv.org/abs/2602.16503",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16503v1 Announce Type: cross Abstract: Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \\emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms. Overall, CALMs offer a compelling trade-off between predictive accuracy and interpretability.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16503"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9bb722e6f5bb9d4c2766f438f830965c362c511bbd1e4eb26a95fc0059e074a7",
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
      "url": "https://arxiv.org/abs/2602.16520",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16520v1 Announce Type: cross Abstract: Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16520"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "007d96e248bfa93079493fa9d995f584bcd76b7ae00ff62a2870d3c0409ee616",
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "url": "https://arxiv.org/abs/2602.16554",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16554v1 Announce Type: cross Abstract: We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three papers, reducing the verification burden to only the newly introduced definitions and axioms. Our results demonstrate that agentic autoformalization can scale to frontier research, offering both a practical tool for machine-verified peer review and a scalable engine for mining high-quality synthetic data to train future reasoning models. Our approach can also be generalized to any other rigorous research in mathematics and theoretical physics.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16554"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "226adeb3d55887b74ea4dbe857af8b13326e19160213f3c745076093a7a026df",
      "title": "AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS",
      "url": "https://arxiv.org/abs/2602.16579",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16579v1 Announce Type: cross Abstract: Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16579"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2fc0717a0450e05e5d90dfacdf7eb6331b63c618fdd1db5b4fa202f7a52a199f",
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
      "url": "https://arxiv.org/abs/2602.16585",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16585v1 Announce Type: cross Abstract: Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16585"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "07ce92b33467f44a1517070f5bc7cecea59c4690400c8142d9cb7ca2d859914a",
      "title": "A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification",
      "url": "https://arxiv.org/abs/2602.16590",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16590v1 Announce Type: cross Abstract: Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16590"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ea7373a52ab712bc21329e2488c991e5e69b7ec1a97a653cd867a067184f239d",
      "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
      "url": "https://arxiv.org/abs/2602.16603",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16603v1 Announce Type: cross Abstract: The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge. In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
      "tags": [
        "papers",
        "eval",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16603"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8d423a909ff624047557aa14912b266e2fd20745179181e8c915a107ff73fae3",
      "title": "Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models",
      "url": "https://arxiv.org/abs/2602.16608",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16608v1 Announce Type: cross Abstract: Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \\textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.",
      "tags": [
        "papers",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16608"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "dbdedf965ce9f2860546f45d79ac41cb186f898554250536c4c4c3daa4cd610f",
      "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
      "url": "https://arxiv.org/abs/2602.16610",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16610v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16610"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ef26a82a84504ba8c41e2bd2c59bfd17ccd6ec46624eff8a8b8944521f37bfff",
      "title": "Causal and Compositional Abstraction",
      "url": "https://arxiv.org/abs/2602.16612",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16612v1 Announce Type: cross Abstract: Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$\\tau$ consistency, abstractions based on interchange interventions, and `distributed' causal abstractions. Our approach is formalised in terms of category theory, and uses the general notion of a compositional model with a given set of queries and semantics in a monoidal, cd- or Markov category; causal models and their queries such as interventions being special cases. We identify two basic notions of abstraction: downward abstractions mapping queries from high to low level; and upward abstractions, mapping concrete queries such as Do-interventions from low to high. Although usually presented as the latter, we show how common causal abstractions may, more fundamentally, be understood in terms of the former. Our approach also leads us to consider a new stronger notion of `component-level' abstraction, applying to the individual components of a model. In particular, this yields a novel, strengthened form of constructive causal abstraction at the mechanism-level, for which we prove characterisation results. Finally, we show that abstraction can be generalised to further compositional models, including those with a quantum semantics implemented by quantum circuits, and we take first steps in exploring abstractions between quantum compositional circuit models and high-level classical causal models as a means to explainable quantum AI.",
      "tags": [
        "papers",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16612"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7bda50bf521b7bd13917d2b859d856d8eaacb3ad6aa86607d8ee0c5e0cccde1e",
      "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
      "url": "https://arxiv.org/abs/2602.16626",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16626v1 Announce Type: cross Abstract: Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16626"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "463583a679143d09fbdf67998698680d89ec96d0e09efb05eac1b2b967144027",
      "title": "Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes",
      "url": "https://arxiv.org/abs/2602.16629",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16629v1 Announce Type: cross Abstract: The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.",
      "tags": [
        "papers",
        "agents",
        "policy"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16629"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "53c59f8e37a939789b48625ef2f2ef28a72e307fe0ba1080c72b711cfd98e6eb",
      "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
      "url": "https://arxiv.org/abs/2602.16634",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16634v1 Announce Type: cross Abstract: The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $\\Delta$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.",
      "tags": [
        "papers",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16634"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "58e610b0766b29183f92a053e59055a49ef8f59f257e53d54ca81d4e1a05f083",
      "title": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System",
      "url": "https://arxiv.org/abs/2602.16650",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16650v1 Announce Type: cross Abstract: Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16650"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8137eb49a854bcfc7fee77d56cb0b2763ebd63873aedbc90f456506c4b402bba",
      "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
      "url": "https://arxiv.org/abs/2602.16660",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16660v1 Announce Type: cross Abstract: The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16660"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0f57051f8499d4eea6c6552976deb4df1399f40c7cb5f686e934c91cdf02dae1",
      "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
      "url": "https://arxiv.org/abs/2602.16671",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16671v1 Announce Type: cross Abstract: Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
      "tags": [
        "papers",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16671"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "45b9f1040e61ea0bfee68e70952d99a07800b596949d1c7beaa34fd72c69525e",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "url": "https://arxiv.org/abs/2602.16699",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16699v1 Announce Type: cross Abstract: LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "tags": [
        "papers",
        "agents",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16699"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b62f15fee7aac1405eb7946b8399677e0dc45463012faab43a36c16d753985d7",
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "url": "https://arxiv.org/abs/2602.16703",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16703v1 Announce Type: cross Abstract: Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16703"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "878133cdcd27c22a31a8cc842a6c521f3eb0b9eb46a2e073bf77fa4adb8e9c41",
      "title": "Policy Compiler for Secure Agentic Systems",
      "url": "https://arxiv.org/abs/2602.16708",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.16708v1 Announce Type: cross Abstract: LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement. Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning. PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.16708"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c5b0f5b15c4d77906f607f7382285d92f4a77f1403fd079b1621ea74951bca21",
      "title": "A Review of Fairness and A Practical Guide to Selecting Context-Appropriate Fairness Metrics in Machine Learning",
      "url": "https://arxiv.org/abs/2411.06624",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2411.06624v4 Announce Type: replace Abstract: Recent regulatory proposals for artificial intelligence emphasize fairness requirements for machine learning models. However, precisely defining the appropriate measure of fairness is challenging due to philosophical, cultural and political contexts. Biases can infiltrate machine learning models in complex ways depending on the model's context, rendering a single common metric of fairness insufficient. This ambiguity highlights the need for criteria to guide the selection of context-aware measures, an issue of increasing importance given the proliferation of ever tighter regulatory requirements. To address this, we developed a flowchart to guide the selection of contextually appropriate fairness measures. Twelve criteria were used to formulate the flowchart. This included consideration of model assessment criteria, model selection criteria, and data bias. We also review fairness literature in the context of machine learning and link it to core regulatory instruments to assist policymakers, AI developers, researchers, and other stakeholders in appropriately addressing fairness concerns and complying with relevant regulatory requirements.",
      "tags": [
        "papers",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2411.06624"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "acd924a4ae7c7939a9508295e5a8114113c97e8c37aac763679883eecf39c7e2",
      "title": "Scalable Precise Computation of Shannon Entropy",
      "url": "https://arxiv.org/abs/2502.01160",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2502.01160v3 Announce Type: replace Abstract: Quantitative information flow analyses (QIF) are a class of techniques for measuring the amount of confidential information leaked by a program to its public outputs. Shannon entropy is an important method to quantify the amount of leakage in QIF. This paper focuses on the programs modeled in Boolean constraints and optimizes the two stages of the Shannon entropy computation to implement a scalable precise tool PSE. In the first stage, we design a knowledge compilation language called \\ADDAND that combines Algebraic Decision Diagrams and conjunctive decomposition. \\ADDAND avoids enumerating possible outputs of a program and supports tractable entropy computation. In the second stage, we optimize the model counting queries that are used to compute the probabilities of outputs. We compare PSE with the state-of-the-art probabilistic approximately correct tool EntropyEstimation, which was shown to significantly outperform the previous precise tools. The experimental results demonstrate that PSE solved 56 more benchmarks compared to EntropyEstimation in a total of 459. For 98\\% of the benchmarks that both PSE and EntropyEstimation solved, PSE is at least $10\\times$ as efficient as EntropyEstimation.",
      "tags": [
        "papers",
        "eval",
        "compute",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2502.01160"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6e1235ec163e8c63ef41f5867102dfe7abde826656d32b7caa87b4ad52f6b435",
      "title": "SurgRAW: Multi-Agent Workflow with Chain of Thought Reasoning for Robotic Surgical Video Analysis",
      "url": "https://arxiv.org/abs/2503.10265",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2503.10265v2 Announce Type: replace Abstract: Robotic-assisted surgery (RAS) is central to modern surgery, driving the need for intelligent systems with accurate scene understanding. Most existing surgical AI methods rely on isolated, task-specific models, leading to fragmented pipelines with limited interpretability and no unified understanding of RAS scene. Vision-Language Models (VLMs) offer strong zero-shot reasoning, but struggle with hallucinations, domain gaps and weak task-interdependency modeling. To address the lack of unified data for RAS scene understanding, we introduce SurgCoTBench, the first reasoning-focused benchmark in RAS, covering 14256 QA pairs with frame-level annotations across five major surgical tasks. Building on SurgCoTBench, we propose SurgRAW, a clinically aligned Chain-of-Thought (CoT) driven agentic workflow for zero-shot multi-task reasoning in surgery. SurgRAW employs a hierarchical reasoning workflow where an orchestrator divides surgical scene understanding into two reasoning streams and directs specialized agents to generate task-level reasoning, while higher-level agents capture workflow interdependencies or ground output clinically. Specifically, we propose a panel discussion mechanism to ensure task-specific agents collaborate synergistically and leverage on task interdependencies. Similarly, we incorporate a retrieval-augmented generation module to enrich agents with surgical knowledge and alleviate domain gaps in general VLMs. We design task-specific CoT prompts grounded in surgical domain to ensure clinically aligned reasoning, reduce hallucinations and enhance interpretability. Extensive experiments show that SurgRAW surpasses mainstream VLMs and agentic systems and outperforms a supervised model by 14.61% accuracy. Dataset and code is available at https://github.com/jinlab-imvr/SurgRAW.git .",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2503.10265"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "16579ce0cc519ec2a522eb205afd94d3b289bb39852f91a906a4e9833dcd296b",
      "title": "Large Language Models for Water Distribution Systems Modeling and Decision-Making",
      "url": "https://arxiv.org/abs/2503.16191",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2503.16191v2 Announce Type: replace Abstract: The integration of Large Language Models (LLMs) into engineering workflows presents new opportunities for making computational tools more accessible. Especially where such tools remain underutilized due to technical or expertise barriers, such as water distribution system (WDS) management. This study introduces LLM-EPANET, an agent-based framework that enables natural language interaction with EPANET, the benchmark WDS simulator. The framework combines retrieval-augmented generation and multi-agent orchestration to automatically translate user queries into executable code, run simulations, and return structured results. A curated set of 69 benchmark queries is introduced to evaluate performance across state-of-the-art LLMs. Results show that LLMs can effectively support a wide range of modeling tasks, achieving 56-81% accuracy overall, and over 90% for simpler queries. These findings highlight the potential of LLM-based modeling to democratize data-driven decision-making in the water sector through transparent, interactive AI interfaces. The framework code and benchmark queries are shared as an open resource: https://github.com/yinon-gold/LLMs-in-WDS-Modeling.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2503.16191"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "cbd9b6bc675f8fcaa317ffe21744a75d6b067eff947a42e5ed8cbc7b4608a549",
      "title": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents",
      "url": "https://arxiv.org/abs/2503.18825",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2503.18825v4 Announce Type: replace Abstract: We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM's choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score, which measures an LLM's capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs' choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making.",
      "tags": [
        "papers",
        "agents",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2503.18825"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8ed4574f0525569b85e16fe6764473e9191e558b262189323190c5013a43c306",
      "title": "GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning",
      "url": "https://arxiv.org/abs/2507.03267",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2507.03267v2 Announce Type: replace Abstract: Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate structural, temporal, and textual attributes, are crucial for modeling complex real-world systems. However, most existing DyTAG datasets exhibit poor textual quality, which severely limits their utility for generative DyTAG tasks requiring semantically rich inputs. Additionally, prior work mainly focuses on discriminative tasks on DyTAGs, resulting in a lack of standardized task formulations and evaluation protocols tailored for DyTAG generation. To address these critical issues, we propose Generative DyTAG Benchmark (GDGB), which comprises eight meticulously curated DyTAG datasets with high-quality textual features for both nodes and edges, overcoming limitations of prior datasets. Building on GDGB, we define two novel DyTAG generation tasks: Transductive Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG). TDGG transductively generates a target DyTAG based on the given source and destination node sets, while the more challenging IDGG introduces new node generation to inductively model the dynamic expansion of real-world graph data. To enable holistic evaluation, we design multifaceted metrics that assess the structural, temporal, and textual quality of the generated DyTAGs. We further propose GAG-General, an LLM-based multi-agent generative framework tailored for reproducible and robust benchmarking of DyTAG generation. Experimental results demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key insights revealing the critical interplay of structural and textual features in DyTAG generation. These findings establish GDGB as a foundational resource for advancing generative DyTAG research and unlocking further practical applications in DyTAG generation. The dataset and source code are available at https://github.com/Lucas-PJ/GDGB-ALGO.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2507.03267"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "7311595f3c03143d67110febf23b183be215377e480b7ef54f3bea3d86fb5d93",
      "title": "Language and Experience: A Computational Model of Social Learning in Complex Tasks",
      "url": "https://arxiv.org/abs/2509.00074",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2509.00074v2 Announce Type: replace Abstract: The ability to combine linguistic guidance from others with direct experience is central to human development, enabling safe and rapid learning in new environments. How do people integrate these two sources of knowledge, and how might AI systems? We present a computational framework that models social learning as joint probabilistic inference over structured, executable world models given sensorimotor and linguistic data. We make this possible by turning a pretrained language model into a probabilistic model of how humans share advice conditioned on their beliefs, allowing our agents both to generate advice for others and to interpret linguistic input as evidence during Bayesian inference. Using behavioral experiments and simulations across 10 video games, we show how linguistic guidance can shape exploration and accelerate learning by reducing risky interactions and speeding up key discoveries in both humans and models. We further explore how knowledge can accumulate across generations through iterated learning experiments and demonstrate successful knowledge transfer between humans and models -- revealing how structured, language-compatible representations might enable human-machine collaborative learning.",
      "tags": [
        "papers",
        "agents",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.00074"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d75b49619b0af264438b3eeef6334f09cff26bff2e1290392b32e08446db6655",
      "title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models",
      "url": "https://arxiv.org/abs/2509.24803",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2509.24803v2 Announce Type: replace Abstract: Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.24803"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "98463210edee241cd775b31d452a5d37b8ae8d5097128c5a319637d8462791c3",
      "title": "Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing",
      "url": "https://arxiv.org/abs/2510.12121",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2510.12121v2 Announce Type: replace Abstract: Precise attribute intensity control--generating Large Language Model (LLM) outputs with specific, user-defined attribute intensities--is crucial for AI systems adaptable to diverse user expectations. Current LLM alignment methods, however, typically provide only directional or open-ended guidance, failing to reliably achieve exact attribute intensities. We address this limitation with three key designs: (1) reformulating precise attribute intensity control as a target-reaching problem, rather than simple maximization; (2) training a lightweight value function via temporal-difference learning to predict final attribute intensity scores from partial generations, thereby steering LLM outputs; and (3) employing gradient-based interventions on hidden representations to navigate the model precisely towards specific attribute intensity targets. Our method enables fine-grained, continuous control over attribute intensities, moving beyond simple directional alignment. Experiments on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text generation to user-specified attribute intensities with high accuracy. Finally, we demonstrate efficiency enhancements across three downstream tasks: preference data synthesis, Pareto frontier approximation and optimization, and distillation of aligned behaviors for intervention-free inference. Our code is available on https://github.com/Pre-Control/pre-control",
      "tags": [
        "papers",
        "safety",
        "inference",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.12121"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5c6fc226b230bb824e9288316b938874314fd3069243cd0f70e838ec095651b2",
      "title": "Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning",
      "url": "https://arxiv.org/abs/2510.18318",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2510.18318v4 Announce Type: replace Abstract: Geospatial data offers immense potential for understanding our planet. However, the sheer volume and diversity of this data along with its varied resolutions, timescales, and sparsity pose significant challenges for thorough analysis and interpretation. This paper introduces Earth AI, a family of geospatial AI models and agentic reasoning that enables significant advances in our ability to unlock novel and profound insights into our planet. This approach is built upon foundation models across three key domains--Planet-scale Imagery, Population, and Environment--and an intelligent Gemini-powered reasoning engine. We present rigorous benchmarks showcasing the power and novel capabilities of our foundation models and validate that when used together, they provide complementary value for geospatial inference and their synergies unlock superior predictive capabilities. To handle complex, multi-step queries, we developed a Gemini-powered agent that jointly reasons over our multiple foundation models along with large geospatial data sources and tools. On a new benchmark of real-world crisis scenarios, our agent demonstrates the ability to deliver critical and timely insights, effectively bridging the gap between raw geospatial data and actionable understanding.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.18318"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "41da86a2f23ef1a8e81053a214204fae702898500fc8ae87a2e3035ab4f800a1",
      "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators",
      "url": "https://arxiv.org/abs/2601.01569",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2601.01569v2 Announce Type: replace Abstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms that struggle with long-horizon tasks due to fragile multi-turn dependencies and context drift. We present CaveAgent, a framework that shifts tool use from ``LLM-as-Text-Generator'' to ``LLM-as-Runtime-Operator.'' CaveAgent introduces a dual-stream architecture that inverts the conventional paradigm: rather than treating the LLM's text context as the primary workspace with tools as auxiliary, CaveAgent elevates the persistent Python runtime as the central locus of state, with a lightweight semantic stream serving as its orchestrator. Beyond leveraging code generation to resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, CaveAgent introduces \\textit{Stateful Runtime Management}: it injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns, unlike existing code-based approaches that remain text-bound. CaveAgent further provides a runtime-integrated skill management system that extends the Agent Skills open standard, enabling ecosystem interoperability through executable skill injections. This persistence mechanism serves as a high-fidelity external memory that reduces context drift in multi-turn interactions and preserves processed data for downstream applications without information loss. Evaluations show consistent improvement across challenging benchmarks, enabling CaveAgent to handle data scales that cause context overflow in both JSON-based and code-based agents. The accessible runtime state further provides programmatically verifiable feedback, enabling automated evaluation and reward signal generation without human annotation and establishing a structural foundation for future research in Reinforcement Learning with Verifiable Rewards (RLVR).",
      "tags": [
        "papers",
        "agents",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.01569"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c42be73e3263950f167cdc37a021147edfd048b87c83bbc1ca5d8bf5eb5a97ed",
      "title": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning",
      "url": "https://arxiv.org/abs/2601.07611",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2601.07611v2 Announce Type: replace Abstract: Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.07611"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b569364d039cb9501eaaf242746f93a7f5a21d386e24b469541bbbb601a678c3",
      "title": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent",
      "url": "https://arxiv.org/abs/2602.00663",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.00663v2 Announce Type: replace Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "inference"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.00663"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "28e270891d535e44a2d2cef658da09e771e0972f99d3c03af0e7e022365db8bd",
      "title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents",
      "url": "https://arxiv.org/abs/2602.02050",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.02050v2 Announce Type: replace Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.02050"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "130d27734cff851a97c2c8298ba72434caa929e987c74d18a5a83a0c7bb48c80",
      "title": "VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health",
      "url": "https://arxiv.org/abs/2602.05088",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.05088v3 Announce Type: replace Abstract: Millions now use generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based, automated safety benchmark. This study aimed to examine the clinical validity and reliability of VERA-MH for evaluating AI safety in suicide risk detection and response. We first simulated a large set of conversations between large language model (LLM)-based users (user-agents) and general-purpose AI chatbots. Licensed mental health clinicians used a rubric (scoring guide) to independently rate the simulated conversations for safe and unsafe chatbot behaviors, as well as user-agent realism. An LLM-based judge used the same scoring rubric to evaluate the same set of simulated conversations. We then examined rating alignment (a) among individual clinicians and (b) between clinician consensus and the LLM judge, and (c) summarized clinicians' ratings of user-agent realism. Individual clinicians were generally consistent with one another in their safety ratings (chance-corrected inter-rater reliability [IRR] = 0.77), establishing a gold-standard clinical reference. The LLM judge was strongly aligned with this clinical consensus overall (IRR = 0.81) and within key conditions. Together, findings from this human evaluation study support the validity and reliability of VERA-MH: an open-source, automated AI safety evaluation for mental health. Future research will examine the generalizability and robustness of VERA-MH and expand the framework to target additional key areas of AI safety in mental health.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.05088"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "496bb5d8896f9d4cc6058e90fce74ddb1a36c695f2273ecb0950c8048651eb6f",
      "title": "AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition",
      "url": "https://arxiv.org/abs/2602.11348",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.11348v2 Announce Type: replace Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.11348"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1f6730b58cbd924a4cb721fc2cab571105b33e3bb2720645b1ce4fb3953c2a1f",
      "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
      "url": "https://arxiv.org/abs/2602.13912",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.13912v2 Announce Type: replace Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.",
      "tags": [
        "papers",
        "reasoning",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.13912"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e9bd5716e65c2dc4cda12516389845430ee3ea35eac8f1a61ae226677a23dbd9",
      "title": "ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI",
      "url": "https://arxiv.org/abs/2602.14135",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.14135v2 Announce Type: replace Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the \"ForesightSafety Bench\" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "safety",
        "policy",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.14135"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4d02deafe318d1b99d35a067847226a5d4f6289a97c40095567338c204b64403",
      "title": "Evaluating Language Model Agency through Negotiations",
      "url": "https://arxiv.org/abs/2401.04536",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2401.04536v3 Announce Type: replace-cross Abstract: We introduce an approach to evaluate language model (LM) agency using negotiation games. This approach better reflects real-world use cases and addresses some of the shortcomings of alternative LM benchmarks. Negotiation games enable us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental evaluation data leakage. We use our approach to test six widely used and publicly accessible LMs, evaluating performance and alignment in both self-play and cross-play settings. Noteworthy findings include: (i) only closed-source models tested here were able to complete these tasks; (ii) cooperative bargaining games proved to be most challenging to the models; and (iii) even the most powerful models sometimes \"lose\" to weaker opponents",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2401.04536"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "60eafffed62815b86a441fd67c31661f8e4b6f488009dae81731c3d7a3e00863",
      "title": "Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training",
      "url": "https://arxiv.org/abs/2405.05523",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2405.05523v2 Announce Type: replace-cross Abstract: Temporal grounding is crucial in multimodal learning, but it poses challenges when applied to animal behavior data due to the sparsity and uniform distribution of moments. To address these challenges, we propose a novel Positional Recovery Training framework (Port), which prompts the model with the start and end times of specific animal behaviors during training. Specifically, \\port{} enhances the baseline model with a Recovering branch to reconstruct corrupted label sequences and align distributions via a Dual-alignment method. This allows the model to focus on specific temporal regions prompted by ground-truth information. Extensive experiments on the Animal Kingdom dataset demonstrate the effectiveness of \\port{}, achieving an IoU@0.3 of 38.52. It emerges as one of the top performers in the sub-track of MMVRAC in ICME 2024 Grand Challenges.",
      "tags": [
        "papers",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2405.05523"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "67105f9ef50517661afe9fe7978f34fdd1b979571011f70585cc8cd9db918092",
      "title": "Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification",
      "url": "https://arxiv.org/abs/2409.17091",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2409.17091v3 Announce Type: replace-cross Abstract: In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing downstream medical recognition tasks. Nevertheless, existing works lack sufficient semantic and sequential steerability for challenging video/3D sequence generation, and neglect quality control of noisy synthesized samples, resulting in unreliable synthetic databases and severely limiting the performance of downstream tasks. In this work, we present Ctrl-GenAug, a novel and general generative augmentation framework that enables highly semantic- and sequential-customized sequence synthesis and suppresses incorrectly synthesized samples, to aid medical sequence classification. Specifically, we first design a multimodal conditions-guided sequence generator for controllably synthesizing diagnosis-promotive samples. A sequential augmentation module is integrated to enhance the temporal/stereoscopic coherence of generated samples. Then, we propose a noisy synthetic data filter to suppress unreliable cases at semantic and sequential levels. Extensive experiments on 3 medical datasets, using 11 networks trained on 3 paradigms, comprehensively analyze the effectiveness and generality of Ctrl-GenAug, particularly in underrepresented high-risk populations and out-domain conditions.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2409.17091"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0d58e81857f30c18ee8c7c1bf9751ede491c401b6e00f9ef0f53f06c32bca353",
      "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model",
      "url": "https://arxiv.org/abs/2411.11706",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2411.11706v4 Announce Type: replace-cross Abstract: Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies have investigated VLM personalization to understand user-provided concepts. However, they mainly focus on single concepts, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes MC-LLaVA, a multi-concept personalization paradigm. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the training costs, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location maps for enhanced recognition and grounding capabilities. To further push the performance upper bound, we incorporate an optional auxiliary loss, better enhancing the proposed personalized prompts. To decorate the VLM personalization research, we contribute a high-quality dataset. We carefully collect images with multiple characters and objects from movies and manually create question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive experiments demonstrate that MC-LLaVA achieves impressive multi-concept personalized responses, paving the way for VLMs to become better user assistants. The code and dataset will be released at \\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}.",
      "tags": [
        "papers",
        "inference",
        "training",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2411.11706"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ead3241c4dd4bfc3ba68fe75e5ebb5a6795b815e36bb1fedfbfb518b0dc65f5d",
      "title": "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics",
      "url": "https://arxiv.org/abs/2411.16537",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2411.16537v5 Announce Type: replace-cross Abstract: Spatial understanding is a crucial capability that enables robots to perceive their surroundings, reason about their environment, and interact with it meaningfully. In modern robotics, these capabilities are increasingly provided by vision-language models. However, these models face significant challenges in spatial reasoning tasks, as their training data are based on general-purpose image datasets that often lack sophisticated spatial understanding. For example, datasets frequently do not capture reference frame comprehension, yet effective spatial reasoning requires understanding whether to reason from ego-, world-, or object-centric perspectives. To address this issue, we introduce RoboSpatial, a large-scale dataset for spatial understanding in robotics. It consists of real indoor and tabletop scenes, captured as 3D scans and egocentric images, and annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial relationships, and the pairing of 2D egocentric images with 3D scans makes it both 2D- and 3D- ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robot manipulation.",
      "tags": [
        "papers",
        "reasoning",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2411.16537"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "4f426048e217ea34f985b025571c6ab071a1446074b26f9d11ad34bcc6696776",
      "title": "Cocoa: Co-Planning and Co-Execution with AI Agents",
      "url": "https://arxiv.org/abs/2412.10999",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2412.10999v4 Announce Type: replace-cross Abstract: As AI agents take on increasingly long-running tasks involving sophisticated planning and execution, there is a corresponding need for novel interaction designs that enable deeper human-agent collaboration. However, most prior works leverage human interaction to fix \"autonomous\" workflows that have yet to become fully autonomous or rigidly treat planning and execution as separate stages. Based on a formative study with 9 researchers using AI to support their work, we propose a design that affords greater flexibility in collaboration, so that users can 1) delegate agency to the user or agent via a collaborative plan where individual steps can be assigned; and 2) interleave planning and execution so that plans can adjust after partial execution. We introduce Cocoa, a system that takes design inspiration from computational notebooks to support complex research tasks. A lab study (n=16) found that Cocoa enabled steerability without sacrificing ease-of-use, and a week-long field deployment (n=7) showed how researchers collaborated with Cocoa to accomplish real-world tasks.",
      "tags": [
        "papers",
        "agents"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2412.10999"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "abef2fe9343c24e0c7f1fbdb536de5f770c4b12f8e6f33de50a82b8db71d8a30",
      "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models",
      "url": "https://arxiv.org/abs/2501.03544",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2501.03544v4 Announce Type: replace-cross Abstract: Recent text-to-image (T2I) models have exhibited remarkable performance in generating high-quality images from text descriptions. However, these models are vulnerable to misuse, particularly generating not-safe-for-work (NSFW) content, such as sexually explicit, violent, political, and disturbing images, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. We further enhance its reliability and helpfulness through a divide-and-conquer strategy, which optimizes category-specific soft prompts and combines them into holistic safety guidance. Extensive experiments across five datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 3.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%.",
      "tags": [
        "papers",
        "safety",
        "inference",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2501.03544"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "136eb289b869adfae4a25c397a65bb4a35bd88c2569cb0b3be4b3ed2def65cb2",
      "title": "Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models",
      "url": "https://arxiv.org/abs/2501.14406",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2501.14406v4 Announce Type: replace-cross Abstract: Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution to address privacy and efficiency challenges in distributed training for PLMs on resource-constrained local devices. However, our measurements reveal two key limitations of FedPEFT: heterogeneous data across devices exacerbates performance degradation of low-rank adaptation, and a fixed parameter configuration results in communication inefficiency. To overcome these limitations, we propose FedARA, a novel adaptive rank allocation framework for federated parameter-efficient fine-tuning of language models. Specifically, FedARA employs truncated Singular Value Decomposition (SVD) adaptation to enhance similar feature representation across clients, significantly mitigating the adverse effects of data heterogeneity. Subsequently, it utilizes dynamic rank allocation to progressively identify critical ranks, effectively improving communication efficiency. Lastly, it leverages rank-based module pruning to automatically remove inactive modules, steadily reducing local computational cost and memory usage in each federated learning round. Extensive experiments show that FedARA consistently outperforms baselines by an average of 6.95% to 8.49% across various datasets and models under heterogeneous data while significantly improving communication efficiency by 2.40$ \\times$. Moreover, experiments on various edge devices demonstrate substantial decreases in total training time and energy consumption by up to 48.90% and 46.95%, respectively.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2501.14406"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8bb92c94e35ba5e952c2e4aafff3f9b0d53723ece39f08c3c32890afa4e70d2a",
      "title": "Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs",
      "url": "https://arxiv.org/abs/2501.16534",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2501.16534v5 Announce Type: replace-cross Abstract: Alignment in large language models (LLMs) is used to enforce guidelines such as safety. Yet, alignment fails in the face of jailbreak attacks that modify inputs to induce unsafe outputs. In this paper, we introduce and evaluate a new technique for jailbreak attacks. We observe that alignment embeds a safety classifier in the LLM responsible for deciding between refusal and compliance, and seek to extract an approximation of this classifier: a surrogate classifier. To this end, we build candidate classifiers from subsets of the LLM. We first evaluate the degree to which candidate classifiers approximate the LLM's safety classifier in benign and adversarial settings. Then, we attack the candidates and measure how well the resulting adversarial inputs transfer to the LLM. Our evaluation shows that the best candidates achieve accurate agreement (an F1 score above 80%) using as little as 20% of the model architecture. Further, we find that attacks mounted on the surrogate classifiers can be transferred to the LLM with high success. For example, a surrogate using only 50% of the Llama 2 model achieved an attack success rate (ASR) of 70% with half the memory footprint and runtime -- a substantial improvement over attacking the LLM directly, where we only observed a 22% ASR. These results show that extracting surrogate classifiers is an effective and efficient means for modeling (and therein addressing) the vulnerability of aligned models to jailbreaking attacks. The code is available at https://github.com/jcnf0/targeting-alignment.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2501.16534"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9b6ca7fc853f8eff1b5e86cbfee0b79e462e589aecf3884a148ee3c7d509e914",
      "title": "Understanding Transformer Optimization via Gradient Heterogeneity",
      "url": "https://arxiv.org/abs/2502.00213",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2502.00213v4 Announce Type: replace-cross Abstract: Transformers are difficult to optimize with stochastic gradient descent (SGD) and largely rely on adaptive optimizers such as Adam. Despite their empirical success, the reasons behind Adam's superior performance over SGD remain poorly understood. In this study, we analyze the optimization of Transformer models through the lens of \\emph{gradient heterogeneity}, defined as the variation in gradient norms across parameter blocks. We provide a theoretical analysis showing that gradient heterogeneity, together with Hessian heterogeneity, degrades the convergence of gradient-based methods such as SGD, while sign-based methods are substantially less sensitive to this effect. Adam's coordinate-wise normalization makes its update directions depend mainly on gradient signs, so Adam can be interpreted as a soft variant of SignSGD. Our analysis uses the fact that SGD and SignSGD follow steepest descent directions under different norms, and derives upper bounds on the iteration complexity with implications for learning rate scaling in SignSGD. We further investigate the origin of gradient heterogeneity in Transformer architectures and show that it is strongly influenced by the placement of layer normalization, with Post-LN architectures exhibiting particularly pronounced heterogeneity. Experimental results from fine-tuning Transformers in both NLP and vision domains validate our theoretical analysis. Code is available at https://github.com/tom4649/gradient-heterogeneity.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2502.00213"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "9c0ec8641a3766c2d88c6e13ec9090386dabeac2f486fc9336b9194c0d710031",
      "title": "Forget Forgetting: Continual Learning in a World of Abundant Memory",
      "url": "https://arxiv.org/abs/2502.07274",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2502.07274v5 Announce Type: replace-cross Abstract: Continual learning (CL) has traditionally focused on minimizing exemplar memory, a constraint often misaligned with modern systems where GPU time, not storage, is the primary bottleneck. This paper challenges this paradigm by investigating a more realistic regime: one where memory is abundant enough to mitigate forgetting, but full retraining from scratch remains prohibitively expensive. In this practical \"middle ground\", we find that the core challenge shifts from stability to plasticity, as models become biased toward prior tasks and struggle to learn new ones. Conversely, improved stability allows simple replay baselines to outperform the state-of-the-art methods at a fraction of the GPU cost. To address this newly surfaced trade-off, we propose Weight Space Consolidation, a lightweight method that combines (1) rank-based parameter resets to restore plasticity with (2) weight averaging to enhance stability. Validated on both class-incremental learning with image classifiers and continual instruction tuning with large language models, our approach outperforms strong baselines while matching the low computational cost of replay, offering a scalable alternative to expensive full-retraining. These findings challenge long-standing CL assumptions and establish a new, cost-efficient baseline for real-world CL systems where exemplar memory is no longer the limiting factor.",
      "tags": [
        "papers",
        "compute",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2502.07274"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "d2c6a5f55ea887af3064965d65a95b557e3986cff71745f9d2040cb80c47a45b",
      "title": "FOCUS on Contamination: Hydrology-Informed Noise-Aware Learning for Geospatial PFAS Mapping",
      "url": "https://arxiv.org/abs/2502.14894",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2502.14894v4 Announce Type: replace-cross Abstract: Per- and polyfluoroalkyl substances (PFAS) are persistent environmental contaminants with significant public health impacts, yet large-scale monitoring remains severely limited due to the high cost and logistical challenges of field sampling. The lack of samples leads to difficulty simulating their spread with physical models and limited scientific understanding of PFAS transport in surface waters. Yet, rich geospatial and satellite-derived data describing land cover, hydrology, and industrial activity are widely available. We introduce FOCUS, a geospatial deep learning framework for PFAS contamination mapping that integrates sparse PFAS observations with large-scale environmental context, including priors derived from hydrological connectivity, land cover, source proximity, and sampling distance. These priors are integrated into a principled, noise-aware loss, yielding a robust training objective under sparse labels. Across extensive ablations, robustness analyses, and real-world validation, FOCUS consistently outperforms baselines including sparse segmentation, Kriging, and pollutant transport simulations, while preserving spatial coherence and scalability over large regions. Our results demonstrate how AI can support environmental science by providing screening-level risk maps that prioritize follow-up sampling and help connect potential sources to surface-water contamination patterns in the absence of complete physical models.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2502.14894"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "da3d31a0f6032994187213e26617af071a7c185177ca119a59ffae2549e6e274",
      "title": "A Survey: Spatiotemporal Consistency in Video Generation",
      "url": "https://arxiv.org/abs/2502.17863",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2502.17863v2 Announce Type: replace-cross Abstract: Video generation aims to produce temporally coherent sequences of visual frames, representing a pivotal advancement in Artificial Intelligence Generated Content (AIGC). Compared to static image generation, video generation poses unique challenges: it demands not only high-quality individual frames but also strong temporal coherence to ensure consistency throughout the spatiotemporal sequence. Although research addressing spatiotemporal consistency in video generation has increased in recent years, systematic reviews focusing on this core issue remain relatively scarce. To fill this gap, this paper views the video generation task as a sequential sampling process from a high-dimensional spatiotemporal distribution, and further discusses spatiotemporal consistency. We provide a systematic review of the latest advancements in the field. The content spans multiple dimensions including generation models, feature representations, generation frameworks, post-processing techniques, training strategies, benchmarks and evaluation metrics, with a particular focus on the mechanisms and effectiveness of various methods in maintaining spatiotemporal consistency. Finally, this paper explores future research directions and potential challenges in this field, aiming to provide valuable insights for advancing video generation technology. The project link is https://github.com/Yin-Z-Y/A-Survey-Spatiotemporal-Consistency-in-Video-Generation.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2502.17863"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0758770cf9c487ed600fc03acd33428f0c587f85f7e0a8d25459cc419a8bece0",
      "title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
      "url": "https://arxiv.org/abs/2503.12286",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2503.12286v2 Announce Type: replace-cross Abstract: Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes.",
      "tags": [
        "papers",
        "reasoning",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2503.12286"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a54c1be89df24369026d2087e88dc6a3a8bdc1acd81d5a7120c476d7fd34614f",
      "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models",
      "url": "https://arxiv.org/abs/2504.00869",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2504.00869v2 Announce Type: replace-cross Abstract: Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2504.00869"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "909cd08ca85b77e6cb5f28cbe52480f994a67cbf47dc9e6a0bbf6161c5f3bc68",
      "title": "FedEFC: Federated Learning Using Enhanced Forward Correction Against Noisy Labels",
      "url": "https://arxiv.org/abs/2504.05615",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2504.05615v3 Announce Type: replace-cross Abstract: Federated Learning (FL) is a powerful framework for privacy-preserving distributed learning. It enables multiple clients to collaboratively train a global model without sharing raw data. However, handling noisy labels in FL remains a major challenge due to heterogeneous data distributions and communication constraints, which can severely degrade model performance. To address this issue, we propose FedEFC, a novel method designed to tackle the impact of noisy labels in FL. FedEFC mitigates this issue through two key techniques: (1) prestopping, which prevents overfitting to mislabeled data by dynamically halting training at an optimal point, and (2) loss correction, which adjusts model updates to account for label noise. In particular, we develop an effective loss correction tailored to the unique challenges of FL, including data heterogeneity and decentralized training. Furthermore, we provide a theoretical analysis, leveraging the composite proper loss property, to demonstrate that the FL objective function under noisy label distributions can be aligned with the clean label distribution. Extensive experimental results validate the effectiveness of our approach, showing that it consistently outperforms existing FL techniques in mitigating the impact of noisy labels, particularly under heterogeneous data settings (e.g., achieving up to 41.64% relative performance improvement over the existing loss correction method).",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2504.05615"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "52516045af221e75862d13845640bec626d22a70ff454020d39a497e12c02bb1",
      "title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment",
      "url": "https://arxiv.org/abs/2504.08603",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2504.08603v3 Announce Type: replace-cross Abstract: Geometrically accurate and semantically expressive map representations have proven invaluable for robot deployment and task planning in unknown environments. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments still presents open challenges, mainly due to computational requirements. In this paper we present FindAnything, an open-world mapping framework that incorporates vision-language information into dense volumetric submaps. Thanks to the use of vision-language features, FindAnything combines pure geometric and open-vocabulary semantic information for a higher level of understanding. It proposes an efficient storage of open-vocabulary information through the aggregation of features at the object level. Pixelwise vision-language features are aggregated based on eSAM segments, which are in turn integrated into object-centric volumetric submaps, providing a mapping from open-vocabulary queries to 3D geometry that is scalable also in terms of memory usage. We demonstrate that FindAnything performs on par with the state-of-the-art in terms of semantic accuracy while being substantially faster and more memory-efficient, allowing its deployment in large-scale environments and on resourceconstrained devices, such as MAVs. We show that the real-time capabilities of FindAnything make it useful for downstream tasks, such as autonomous MAV exploration in a simulated Search and Rescue scenario. Project Page: https://ethz-mrl.github.io/findanything/.",
      "tags": [
        "papers"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2504.08603"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6d1dc6b747e84db3abd7e8ba2b5f1d8f764f8497bca49040b02d92eebd9ae7d6",
      "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis",
      "url": "https://arxiv.org/abs/2504.19223",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2504.19223v4 Announce Type: replace-cross Abstract: Spectral imaging offers promising applications across diverse domains, including medicine and urban scene understanding, and is already established as a critical modality in remote sensing. However, variability in channel dimensionality and captured wavelengths among spectral cameras impede the development of AI-driven methodologies, leading to camera-specific models with limited generalizability and inadequate cross-camera applicability. To address this bottleneck, we introduce CARL, a model for Camera-Agnostic Representation Learning across RGB, multispectral, and hyperspectral imaging modalities. To enable the conversion of a spectral image with any channel dimensionality to a camera-agnostic representation, we introduce a novel spectral encoder, featuring a self-attention-cross-attention mechanism, to distill salient spectral information into learned spectral representations. Spatio-spectral pre-training is achieved with a novel feature-based self-supervision strategy tailored to CARL. Large-scale experiments across the domains of medical imaging, autonomous driving, and satellite imaging demonstrate our model's unique robustness to spectral heterogeneity, outperforming on datasets with simulated and real-world cross-camera spectral variations. The scalability and versatility of the proposed approach position our model as a backbone for future spectral foundation models. Code and model weights are publicly available at https://github.com/IMSY-DKFZ/CARL.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2504.19223"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3a66e62a3ce8a694e38e8cf82e2dfc58968f76c210a70ec3c413df321d3ef2ba",
      "title": "Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics",
      "url": "https://arxiv.org/abs/2505.03795",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2505.03795v3 Announce Type: replace-cross Abstract: Human networks greatly impact important societal outcomes, including wealth and health inequality, poverty, and bullying. As such, understanding human networks is critical to learning how to promote favorable societal outcomes. As a step toward better understanding human networks, we compare and contrast several methods for learning models of human behavior in a strategic network game called the Junior High Game (JHG) [39]. These modeling methods differ with respect to the assumptions they use to parameterize human behavior (behavior matching vs. community-aware behavior) and the moments they model (mean vs. distribution). Results show that the highest-performing method, called hCAB, models the distribution of human behavior rather than the mean and assumes humans use community-aware behavior rather than behavior matching. When applied to small societies, the hCAB model closely mirrors the population dynamics of human groups (with notable differences). Additionally, in a user study, human participants had difficulty distinguishing hCAB agents from other humans, thus illustrating that the hCAB model also produces plausible (individual) behavior in this strategic network game.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.03795"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1f00cffd998c2352ed5e76c76a1e03efb10d036bb98d881b1d4b054160597838",
      "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI",
      "url": "https://arxiv.org/abs/2505.12707",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2505.12707v2 Announce Type: replace-cross Abstract: Advances in deep generative modeling have made it increasingly plausible to train human-level embodied agents. Yet progress has been limited by the absence of large-scale, real-time, multi-modal, and socially interactive datasets that reflect the sensory-motor complexity of natural environments. To address this, we present PLAICraft, a novel data collection platform and dataset capturing multiplayer Minecraft interactions across five time-aligned modalities: video, game output audio, microphone input audio, mouse, and keyboard actions. Each modality is logged with millisecond time precision, enabling the study of synchronous, embodied behaviour in a rich, open-ended world. The dataset comprises over 10,000 hours of gameplay from more than 10,000 global participants. Alongside the dataset, we provide an evaluation suite for benchmarking model capabilities in object recognition, spatial awareness, language grounding, and long-term memory. PLAICraft opens a path toward training and evaluating agents that act fluently and purposefully in real time, paving the way for truly embodied artificial intelligence.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.12707"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "8a6bd51394f5a3d86b4ed757b7da7f3d6c6f4c6dc32a2d20829f2a8f1038d0c8",
      "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models",
      "url": "https://arxiv.org/abs/2505.15801",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2505.15801v4 Announce Type: replace-cross Abstract: Large reasoning models such as OpenAI o1 and DeepSeek-R1 have demonstrated remarkable performance in complex reasoning tasks. A critical component of their training is the incorporation of reference-based reward systems within reinforcement learning (RL), where model outputs are evaluated against ground truth references. However, existing reward benchmarks focus on preference comparisons between responses rather than evaluating verification against ground truth references, leaving a critical gap in our ability to evaluate verification systems used in reasoning model training. In this paper, we introduce VerifyBench and its challenging variant VerifyBench-Hard, two benchmarks specifically designed to assess reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Our comprehensive evaluation reveals that while larger model-based verifiers show promise on standard cases, all current systems demonstrate substantial room for improvement on challenging instances. Through systematic analysis of performance patterns across reasoning tasks and error categories, we provide insights for advancing reference-based reward systems. These benchmarks establish a standardized framework for improving verification accuracy, ultimately enhancing reasoning capabilities in models trained via RL.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.15801"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f75d7dec7ba5e8560e90c77caa89b810a31beb2233997431a3fdc7db46f29efe",
      "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference",
      "url": "https://arxiv.org/abs/2505.19427",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2505.19427v2 Announce Type: replace-cross Abstract: The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise $\\ell_2$-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to $2.94\\%$ in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at https://github.com/microsoft/wina.",
      "tags": [
        "papers",
        "inference",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.19427"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "21ec351de47cbd401ba8f01b572b9c714a4823ba7a53586d8cf5171a3a3400be",
      "title": "Experience-based Knowledge Correction for Robust Planning in Minecraft",
      "url": "https://arxiv.org/abs/2505.24157",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2505.24157v3 Announce Type: replace-cross Abstract: Large Language Model (LLM)-based planning has advanced embodied agents in long-horizon environments such as Minecraft, where acquiring latent knowledge of goal (or item) dependencies and feasible actions is critical. However, LLMs often begin with flawed priors and fail to correct them through prompting, even with feedback. We present XENON (eXpErience-based kNOwledge correctioN), an agent that algorithmically revises knowledge from experience, enabling robustness to flawed priors and sparse binary feedback. XENON integrates two mechanisms: Adaptive Dependency Graph, which corrects item dependencies using past successes, and Failure-aware Action Memory, which corrects action knowledge using past failures. Together, these components allow XENON to acquire complex dependencies despite limited guidance. Experiments across multiple Minecraft benchmarks show that XENON outperforms prior agents in both knowledge learning and long-horizon planning. Remarkably, with only a 7B open-weight LLM, XENON surpasses agents that rely on much larger proprietary models. Project page: https://sjlee-me.github.io/XENON",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2505.24157"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "999e2da07d2a8fe49092ac3d19b51a0e33fffefdf944c3af0ecc265c3cd9b9b6",
      "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency",
      "url": "https://arxiv.org/abs/2506.08822",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2506.08822v2 Announce Type: replace-cross Abstract: Generative modeling-based visuomotor policies have been widely adopted in robotic manipulation, attributed to their ability to model multimodal action distributions. However, the high inference cost of multi-step sampling limits its applicability in real-time robotic systems. Existing approaches accelerate sampling in generative modeling-based visuomotor policies by adapting techniques originally developed to speed up image generation. However, a major distinction exists: image generation typically produces independent samples without temporal dependencies, while robotic manipulation requires generating action trajectories with continuity and temporal coherence. To this end, we propose FreqPolicy, a novel approach that first imposes frequency consistency constraints on flow-based visuomotor policies. Our work enables the action model to capture temporal structure effectively while supporting efficient, high-quality one-step action generation. Concretely, we introduce a frequency consistency constraint objective that enforces alignment of frequency-domain action features across different timesteps along the flow, thereby promoting convergence of one-step action generation toward the target distribution. In addition, we design an adaptive consistency loss to capture structural temporal variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53 tasks across 3 simulation benchmarks, proving its superiority over existing one-step action generators. We further integrate FreqPolicy into the vision-language-action (VLA) model and achieve acceleration without performance degradation on 40 tasks of LIBERO. Besides, we show efficiency and effectiveness in real-world robotic scenarios with an inference frequency of 93.5 Hz.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "policy",
        "inference",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.08822"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5833e1914b83317e080990b32a06fa0e536bb1100b186e5c0e49b63947a9482f",
      "title": "DiffusionBlocks: Block-wise Neural Network Training via Diffusion Interpretation",
      "url": "https://arxiv.org/abs/2506.14202",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2506.14202v3 Announce Type: replace-cross Abstract: End-to-end backpropagation requires storing activations throughout all layers, creating memory bottlenecks that limit model scalability. Existing block-wise training methods offer means to alleviate this problem, but they rely on ad-hoc local objectives and remain largely unexplored beyond classification tasks. We propose $\\textit{DiffusionBlocks}$, a principled framework for transforming transformer-based networks into genuinely independent trainable blocks that maintain competitive performance with end-to-end training. Our key insight leverages the fact that residual connections naturally correspond to updates in a dynamical system. With minimal modifications to this system, we can convert the updates to those of a denoising process, where each block can be learned independently by leveraging the score matching objective. This independence enables training with gradients for only one block at a time, thereby reducing memory requirements in proportion to the number of blocks. Our experiments on a range of transformer architectures (vision, diffusion, autoregressive, recurrent-depth, and masked diffusion) demonstrate that DiffusionBlocks training matches the performance of end-to-end training while enabling scalable block-wise training on practical tasks beyond small-scale classification. DiffusionBlocks provides a theoretically grounded approach that successfully scales to modern generative tasks across diverse architectures. Code is available at https://github.com/SakanaAI/DiffusionBlocks .",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.14202"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "091255ef4b38757a634bfb7ada46d9868082fa968522de439a632a56436fa08c",
      "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic",
      "url": "https://arxiv.org/abs/2506.23875",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2506.23875v3 Announce Type: replace-cross Abstract: The chain of thought, i.e., step-by-step reasoning, is one of the fundamental mechanisms of Transformers. While the design of intermediate reasoning steps has been extensively studied and shown to critically influence performance on mathematical, multi-step reasoning tasks, the ordering of these steps has received little attention, despite its significant effect on the difficulty of reasoning. This study addresses a novel task of unraveling the chain of thought -- reordering decoder input tokens into a learning-friendly sequence for Transformers, for learning arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture of target sequences arranged in different orders and then identifies benign orders as those with fast loss drops in the early stage. As the search space grows factorially in sequence length, we propose a two-stage hierarchical approach for inter- and intra-block reordering. Experiments on seven order-sensitive arithmetic tasks show that our method identifies a learning-friendly order out of a few billion candidates. Notably, it recovered the reverse-digit order reported in prior studies for the multiplication task.",
      "tags": [
        "papers",
        "reasoning"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2506.23875"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "082c7c9a43760c10700d0ed7d0664fd5ba176cc51b77fdd6712e7a9dbb74a90f",
      "title": "Expressive Power of Graph Transformers via Logic",
      "url": "https://arxiv.org/abs/2508.01067",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2508.01067v2 Announce Type: replace-cross Abstract: Transformers are the basis of modern large language models, but relatively little is known about their precise expressive power on graphs. We study the expressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and GPS-networks by Ramp\\'asek et al. (2022), both under soft-attention and average hard-attention. Our study covers two scenarios: the theoretical setting with real numbers and the more practical case with floats. With reals, we show that in restriction to vertex properties definable in first-order logic (FO), GPS-networks have the same expressive power as graded modal logic (GML) with the global modality. With floats, GPS-networks turn out to be equally expressive as GML with the counting global modality. The latter result is absolute, not restricting to properties definable in a background logic. We also obtain similar characterizations for GTs in terms of propositional logic with the global modality (for reals) and the counting global modality (for floats).",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2508.01067"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e0283b09f48bf267e010879393049beaae0ec2d93b0f723446e7978d9239f286",
      "title": "Model-Agnostic Dynamic Feature Selection with Uncertainty Quantification",
      "url": "https://arxiv.org/abs/2508.02566",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2508.02566v3 Announce Type: replace-cross Abstract: Dynamic feature selection (DFS) addresses budget constraints in decision-making by sequentially acquiring features for each instance, making it appealing for resource-limited scenarios. However, existing DFS methods require models specifically designed for the sequential acquisition setting, limiting compatibility with models already deployed in practice. Furthermore, they provide limited uncertainty quantification, undermining trust in high-stakes decisions. In this work, we show that DFS introduces new uncertainty sources compared to the static setting. We formalise how model adaptation to feature subsets induces epistemic uncertainty, how standard imputation strategies bias aleatoric uncertainty estimation, and why predictive confidence fails to discriminate between good and bad selection policies. We also propose a model-agnostic DFS framework compatible with pre-trained classifiers, including interpretable-by-design models, through efficient subset reparametrization strategies. Empirical evaluation on tabular and image datasets demonstrates competitive accuracy against state-of-the-art greedy and reinforcement learning-based DFS methods with both neural and rule-based classifiers. We further show that the identified uncertainty sources persist across most existing approaches, highlighting the need for uncertainty-aware DFS.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2508.02566"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "98ada29832ea4969c9f3a92016c37b6bfcfe0ef201f09623ed0b7c1454d12726",
      "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision",
      "url": "https://arxiv.org/abs/2508.08177",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2508.08177v3 Announce Type: replace-cross Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding.",
      "tags": [
        "papers",
        "reasoning",
        "safety",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2508.08177"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b17bd5ec3780654235fff2926cfbb0cdc90354d76dd2163e3733ea9c1a10b5c4",
      "title": "Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers",
      "url": "https://arxiv.org/abs/2508.10480",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2508.10480v2 Announce Type: replace-cross Abstract: We introduce an output layer for neural networks that ensures satisfaction of convex constraints. Our approach, $\\Pi$net, leverages operator splitting for rapid and reliable projections in the forward pass, and the implicit function theorem for backpropagation. We deploy $\\Pi$net as a feasible-by-design optimization proxy for parametric constrained optimization problems and obtain modest-accuracy solutions faster than traditional solvers when solving a single problem, and significantly faster for a batch of problems. We surpass state-of-the-art learning approaches by orders of magnitude in terms of training time, solution quality, and robustness to hyperparameter tuning, while maintaining similar inference times. Finally, we tackle multi-vehicle motion planning with non-convex trajectory preferences and provide $\\Pi$net as a GPU-ready package implemented in JAX.",
      "tags": [
        "papers",
        "compute",
        "inference",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2508.10480"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ae8d9ee2ab370c5b8b95d0e866713344897a2b1e05788765048bfcbd8e0042c0",
      "title": "FairTabGen: High-Fidelity and Fair Synthetic Health Data Generation from Limited Samples",
      "url": "https://arxiv.org/abs/2508.11810",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2508.11810v2 Announce Type: replace-cross Abstract: Synthetic healthcare data generation offers a promising solution to research limitations in clinical settings caused by privacy and regulatory constraints. However, current synthetic data generation approaches require specialized knowledge about training generative models and require high computational resources. In this paper, we propose FairTabGen, an LLM-based tabular data generation framework that produces high-quality synthetic healthcare data using only a small subset of the original dataset. Our method combines in-context learning, prompt curation and embedding structural constraints for data synthesis. We evaluate performance on MIMIC-IV dataset. Our method using 99% less data and achieving 50% improvement for fairness through unawareness while maintaining competitive predictive utility. However, we observe data distribution of racial groups is skewed affecting demographic parity. We thereafter apply bias mitigation algorithms in the pre-processing stage, improving overall fairness by 10% highlighting effectiveness of our approach.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2508.11810"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e823a453849bf5694cddab3935fc725dbcfa723fc68eb5c000190a1dbbf0232e",
      "title": "COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization",
      "url": "https://arxiv.org/abs/2509.05249",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2509.05249v2 Announce Type: replace-cross Abstract: The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments. It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties. This flexibility enables the creation of millions of unique task rules -- surpassing concurrent datasets by several orders of magnitude -- across a wide range of difficulties, while allowing virtually unlimited sample generation per rule. We provide baseline experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.05249"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "dae3c41c67a12039fd41e98c6b15d846f84fe0d588a5b05c3b5b64231236fec9",
      "title": "Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects",
      "url": "https://arxiv.org/abs/2509.06085",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2509.06085v2 Announce Type: replace-cross Abstract: Pre-trained models (PTMs) are machine learning models that have been trained in advance, often on large-scale data, and can be reused for new tasks, thereby reducing the need for costly training from scratch. Their widespread adoption introduces a new class of software dependency, which we term Software Dependencies 2.0, extending beyond conventional libraries to learned behaviors embodied in trained models and their associated artifacts. The integration of PTMs as software dependencies in real projects remains unclear, potentially threatening maintainability and reliability of modern software systems that increasingly rely on them. Objective: In this study, we investigate Software Dependencies 2.0 in open-source software (OSS) projects by examining the reuse of PTMs, with a focus on how developers manage and integrate these models. Specifically, we seek to understand: (1) how OSS projects structure and document their PTM dependencies; (2) what stages and organizational patterns emerge in the reuse pipelines of PTMs within these projects; and (3) the interactions among PTMs and other learned components across pipeline stages. We conduct a mixed-methods analysis of a statistically significant random sample of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM reuse by identifying patterns and qualitatively investigate how developers integrate and manage these models in practice.",
      "tags": [
        "papers",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.06085"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e9b5a4058aa566ec72491b5a717e21f5d14b7ecd4f2f5ef4aae35a8bef9b8ea4",
      "title": "PolicyPad: Collaborative Prototyping of LLM Policies",
      "url": "https://arxiv.org/abs/2509.19680",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2509.19680v2 Announce Type: replace-cross Abstract: As LLMs gain adoption in high-stakes domains like mental health, domain experts are increasingly consulted to provide input into policies governing their behavior. From an observation of 19 policymaking workshops with 9 experts over 15 weeks, we identified opportunities to better support rapid experimentation, feedback, and iteration for collaborative policy design processes. We present PolicyPad, an interactive system that facilitates the emerging practice of LLM policy prototyping by drawing from established UX prototyping practices, including heuristic evaluation and storyboarding. Using PolicyPad, policy designers can collaborate on drafting a policy in real time while independently testing policy-informed model behavior with usage scenarios. We evaluate PolicyPad through workshops with 8 groups of 22 domain experts in mental health and law, finding that PolicyPad enhanced collaborative dynamics during policy design, enabled tight feedback loops, and led to novel policy contributions. Overall, our work paves expert-informed paths for advancing AI alignment and safety.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "policy",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.19680"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b84b7faa13e9839cfde3bb92fe77c9c40079e9266ac4da505478e325b485ae0b",
      "title": "FeatBench: Towards More Realistic Evaluation of Feature-level Code Generation",
      "url": "https://arxiv.org/abs/2509.22237",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2509.22237v2 Announce Type: replace-cross Abstract: Evaluating Large Language Models (LLMs) on repository-level feature implementation is a critical frontier in software engineering. However, establishing a benchmark that faithfully mirrors realistic development scenarios remains a significant challenge. Existing feature-level benchmarks generally suffer from two primary limitations: unrealistic task inputs enriched with code hints and significant data leakage risks due to their static nature. To address these limitations, we propose a new benchmark - FeatBench, which introduces the following advances: (1) Realistic Task Inputs. Task inputs consist solely of natural language requirements, strictly devoid of code hints (e.g., function signatures). This format mirrors realistic software development by requiring agents to independently bridge the gap between abstract user intent and concrete code changes. (2) Evolving Data. FeatBench employs a fully automated pipeline to construct new benchmark versions from the latest repositories, effectively mitigating data contamination. The initial release comprises 157 tasks sourced from 27 actively maintained repositories. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. The results reveal that FeatBench poses a significant challenge, with the highest resolved rate reaching only 29.94%. Crucially, our analysis uncovers a prevalent behavioral pattern of aggressive implementation, which leads to \"scope creep\" and widespread regressions where agents break existing features by diverging from the user's explicit intent. We release FeatBench, our automated pipeline, and all experimental results to facilitate further community research.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.22237"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6abbb5382a13735b48c4bed96173b2e8b1195aeb2600fc39d5285797e67709f8",
      "title": "Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs",
      "url": "https://arxiv.org/abs/2509.25380",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2509.25380v2 Announce Type: replace-cross Abstract: Data curriculums have become central to successful LLM training, yet principles governing optimal data placement remain unclear. We introduce the *training re-evaluation curve (TREC)*, a diagnostic that retrospectively evaluates training batches *using the final model weights*. The TREC characterizes how well a trained model retains training data as a function of *when* the data was encountered during training. Analyzing TRECs for models from 111M to 3.9B parameters, we show that placing high-quality data at low points on the TREC significantly improves performance. Importantly, while a TREC is initially observable only after training, we demonstrate it can be *predicted in advance* from AdamW's implicit EMA coefficients, enabling proactive curriculum design. By predicting TRECs for published training recipes, we explain prior ablations and reveal suboptimal data placements. We also align high-quality data with TREC minima in order to improve continual pre-training of a 3.9B-parameter LLM trained on 900B tokens.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2509.25380"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "6c8715dfa4badc3c63e7ec8658558c13a5bb11ff5b02bdda0bc0de1fbcc70e4d",
      "title": "Multilingual Routing in Mixture-of-Experts",
      "url": "https://arxiv.org/abs/2510.04694",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2510.04694v2 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern LLMs, yet little is understood about how their sparse routing dynamics respond to multilingual data. In this work, we analyze expert routing patterns using parallel multilingual datasets and present highly interpretable layer-wise phenomena. We find that MoE models route tokens in language-specific ways in the early and late decoder layers but exhibit significant cross-lingual routing alignment in middle layers, mirroring parameter-sharing trends observed in dense LLMs. In particular, we reveal a clear, strong correlation between a model's performance in a given language and how similarly its tokens are routed to English in these layers. Extending beyond correlation, we explore inference-time interventions that induce higher cross-lingual routing alignment. We introduce a method that steers the router by promoting middle-layer task experts frequently activated in English, and it successfully increases multilingual performance. These 1-2% gains are remarkably consistent across two evaluation tasks, three models, and 15+ languages, especially given that these simple interventions override routers of extensively trained, state-of-the-art LLMs. In comparison, interventions outside of the middle layers or targeting multilingual-specialized experts only yield performance degradation. Altogether, we present numerous findings that explain how MoEs process non-English text and demonstrate that generalization is limited by the model's ability to leverage language-universal experts in all languages.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "inference",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.04694"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "3e06cc5486268544cb863bc895c943bb12a85a56480bee5d0833a6f129fd0e31",
      "title": "StarEmbed: Benchmarking Time Series Foundation Models on Astronomical Observations of Variable Stars",
      "url": "https://arxiv.org/abs/2510.06200",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2510.06200v3 Announce Type: replace-cross Abstract: Time series foundation models (TSFMs) are increasingly being adopted as highly-capable general-purpose time series representation learners. Although their training corpora are vast, they exclude astronomical time series data. Observations of stars produce peta-scale time series with unique challenges including irregular sampling and heteroskedasticity. We introduce StarEmbed, the first public benchmark for rigorous and standardized evaluation of state-of-the-art TSFMs on stellar time series observations (``light curves''). We benchmark on three scientifically-motivated downstream tasks: unsupervised clustering, supervised classification, and out-of-distribution source detection. StarEmbed integrates a catalog of expert-vetted labels with multi-variate light curves from the Zwicky Transient Facility, yielding ~40k hand-labeled light curves spread across seven astrophysical classes. We evaluate the zero-shot representation capabilities of three TSFMs (MOIRAI, Chronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against handcrafted feature extraction, the long-standing baseline in the astrophysics literature. Our results demonstrate that these TSFMs, especially the Chronos models, which are trained on data completely unlike the astronomical observations, can outperform established astrophysics-specific baselines in some tasks and effectively generalize to entirely new data. In particular, TSFMs deliver state-of-the-art performance on our out-of-distribution source detection benchmark. With the first benchmark of TSFMs on astronomical time series data, we test the limits of their generalization and motivate a paradigm shift in time-domain astronomy from using task-specific, fully supervised pipelines toward adopting generic foundation model representations for the analysis of peta-scale datasets from forthcoming observatories.",
      "tags": [
        "papers",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.06200"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "0b99ab9c0b5aa830486899653f126d1b6f5310adb954a8e3c98aaa3fef9f9530",
      "title": "Lossless Vocabulary Reduction for Auto-Regressive Language Models",
      "url": "https://arxiv.org/abs/2510.08102",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2510.08102v2 Announce Type: replace-cross Abstract: Tokenization -- the process of decomposing a given text into a sequence of subwords called tokens -- is one of the key components in the development of language models. Particularly, auto-regressive language models generate texts token by token, i.e., by predicting the next-token distribution given the previous ones, and thus tokenization directly affects their efficiency in text generation. Since each language model has their own vocabulary as a set of possible tokens, they struggle to cooperate with each other at the level of next-token distributions such as model ensemble. In this paper, we establish a theoretical framework of lossless vocabulary reduction, which efficiently converts a given auto-regressive language model into the one with an arbitrarily small vocabulary without any loss in accuracy. This framework allows language models with different tokenization to cooperate with each other efficiently by reduction to their maximal common vocabulary. Specifically, we empirically demonstrate its applicability to model ensemble with different tokenization.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.08102"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2315809c27c941c2234e28b142ceab643ced32049d43b2486bc576d40feb772e",
      "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction",
      "url": "https://arxiv.org/abs/2510.12768",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2510.12768v2 Announce Type: replace-cross Abstract: Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our approach estimates time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.",
      "tags": [
        "papers",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.12768"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a96954578eeb376c94dc95d783fd7fa0bf32b0bb12b250eaa9f6d17c7883dc44",
      "title": "GENESIS: A Generative Model of Episodic-Semantic Interaction",
      "url": "https://arxiv.org/abs/2510.15828",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2510.15828v2 Announce Type: replace-cross Abstract: A central challenge in cognitive neuroscience is to explain how semantic and episodic memory, two major forms of declarative memory, typically associated with cortical and hippocampal processing, interact to support learning, recall, and imagination. Despite significant advances, we still lack a unified computational framework that jointly accounts for core empirical phenomena across both semantic and episodic processing domains. Here, we introduce the Generative Episodic-Semantic Integration System (GENESIS), a computational model that formalizes memory as the interaction between two limited-capacity generative systems: a Cortical-VAE, supporting semantic learning and generalization, and a Hippocampal-VAE, supporting episodic encoding and retrieval within a retrieval-augmented generation (RAG) architecture. GENESIS reproduces hallmark behavioral findings, including generalization in semantic memory, recognition, serial recall effects and gist-based distortions in episodic memory, and constructive episodic simulation, while capturing their dynamic interactions. The model elucidates how capacity constraints shape the fidelity and memorability of experiences, how semantic processing introduces systematic distortions in episodic recall, and how episodic replay can recombine previous experiences. Together, these results provide a principled account of memory as an active, constructive, and resource-bounded process. GENESIS thus advances a unified theoretical framework that bridges semantic and episodic memory, offering new insights into the generative foundations of human cognition.",
      "tags": [
        "papers",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.15828"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c0dcbe53971891c71d557274acd5c79af0cd4d54b16922556540c2b80a19d7a4",
      "title": "CreativityPrism: A Holistic Evaluation Framework for Large Language Model Creativity",
      "url": "https://arxiv.org/abs/2510.20091",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2510.20091v2 Announce Type: replace-cross Abstract: Creativity is often seen as a hallmark of human intelligence. While large language models (LLMs) are increasingly perceived as generating creative text, there is still no holistic and scalable framework to evaluate their creativity across diverse scenarios. Existing methods of LLM creativity evaluation either heavily rely on humans, limiting speed and scalability, or are fragmented across different domains and different definitions of creativity. To address this gap, we propose CREATIVITYPRISM, an evaluation analysis framework that consolidates eight tasks from three domains, divergent thinking, creative writing, and logical reasoning, into a taxonomy of creativity that emphasizes three dimensions: quality, novelty, and diversity of LLM generations. The framework is designed to be scalable with reliable automatic evaluation judges that have been validated against human annotations. We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on CREATIVITYPRISM and find that while proprietary LLMs dominate creative writing and logical reasoning tasks by a 15% lead over open-sourced ones, they offer no significant advantage in divergent thinking, a domain much less explored in existing post-training regimes. Our analysis also shows that high performance in one creative dimension or domain rarely generalizes to others; specifically, novelty metrics often show weak or negative correlations with other metrics. This fragmentation confirms that a holistic, multi-dimensional framework like CREATIVITYPRISM is essential for meaningful assessment of LLM creativity.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.20091"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "346e8df13a1346e88c00d35552e3c3fe7fb2b141e3cfca838d0c365a0b0d2e34",
      "title": "Transformers can do Bayesian Clustering",
      "url": "https://arxiv.org/abs/2510.24318",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2510.24318v3 Announce Type: replace-cross Abstract: Bayesian clustering accounts for uncertainty but is computationally demanding at scale. Furthermore, real-world datasets often contain missing values, and simple imputation ignores the associated uncertainty, resulting in suboptimal results. We present Cluster-PFN, a Transformer-based model that extends Prior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained entirely on synthetic datasets generated from a finite Gaussian Mixture Model (GMM) prior, Cluster-PFN learns to estimate the posterior distribution over both the number of clusters and the cluster assignments. Our method estimates the number of clusters more accurately than handcrafted model selection procedures such as AIC, BIC and Variational Inference (VI), and achieves clustering quality competitive with VI while being orders of magnitude faster. Cluster-PFN can be trained on complex priors that include missing data, outperforming imputation-based baselines on real-world genomic datasets, at high missingness. These results show that the Cluster-PFN can provide scalable and flexible Bayesian clustering.",
      "tags": [
        "papers",
        "inference",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2510.24318"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "45f915adcfdef078628cf6a169fb75d4414105f891b9c489007d04a837cb7e33",
      "title": "Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training",
      "url": "https://arxiv.org/abs/2511.04485",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2511.04485v2 Announce Type: replace-cross Abstract: Parameter-efficient training based on low-rank optimization has become a highly successful tool for fine-tuning large deep learning models. However, these methods often fail for low-rank pre-training, where simultaneously maintaining low-rank weight structure and optimizing the task objective remains challenging. We propose the $\\textit{Quadratic Reweighted Rank Regularizer}$ ($\\texttt{Q3R}$), which leads to a novel low-rank-inducing training strategy inspired by the Iteratively Reweighted Least Squares (IRLS) framework. $\\texttt{Q3R}$ is based on a quadratic regularizer term that majorizes a smoothed log-determinant rank surrogate. Unlike other low-rank training techniques, $\\texttt{Q3R}$ can train weight matrices to prescribed low target ranks while achieving predictive performance comparable to dense models, with small computational overhead and full compatibility with existing architectures. For example, we demonstrate a $\\texttt{Q3R}$-regularized ViT-Tiny experiment where truncating the model to $60\\%$ and $80\\%$ of its parameters results in only minor absolute accuracy drops of $1.3\\%$ and $4\\%$, respectively, on CIFAR-10. We confirm the efficacy of $\\texttt{Q3R}$ on Transformers across both vision and language tasks, including low-rank fine-tuning.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.04485"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "5b7d2803953fa297abdbdaf9b5e417bf20bd093ffa8f5e55aa364800d6aa77e1",
      "title": "Reasoning Up the Instruction Ladder for Controllable Language Models",
      "url": "https://arxiv.org/abs/2511.04694",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2511.04694v4 Announce Type: replace-cross Abstract: As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first \"think\" about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises ~7K aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks, achieving roughly a 20% improvement on the IHEval conflict setup. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks, providing up to a 20% reduction in attack success rate (ASR). These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.04694"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "e65e8401643a4a9e3656154953ce516355483f51dcd8c5dfce9b10411c8afa2e",
      "title": "Mastering Olympiad-Level Physics with Artificial Intelligence",
      "url": "https://arxiv.org/abs/2511.10515",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2511.10515v2 Announce Type: replace-cross Abstract: Olympiad-level physics problem-solving significantly challenges both humans and artificial intelligence (AI), as it requires integrating appropriate modeling, application of physical principles, and precise calculation within long reasoning processes. In this paper, we introduce LOCA (LOgical Chain Augmentation), an AI agent framework designed for complex physics reasoning. LOCA decomposes long reasoning into serialized atomic and verifiable steps, refining the solution through an augment-review loop. We evaluate LOCA on the 2025 Chinese Physics Olympiad (CPhO) theory examination, a rigorous testbed renowned for its depth and complexity. The framework achieves a near-perfect score of 313 out of 320 points, significantly surpassing the top human competitor and other baseline methods. Furthermore, LOCA attains a near-perfect score of 28.6 out of 30 on the IPhO 2025 examination, demonstrating its strong generalizability across different contexts. Our work points toward the development of trustworthy AI partners in both research and education.",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.10515"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "10f24d1cfafe4263584a938b3386bd97935bc036182d87aec29b36b85a008671",
      "title": "Language-Guided Invariance Probing of Vision-Language Models",
      "url": "https://arxiv.org/abs/2511.13494",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2511.13494v2 Announce Type: replace-cross Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic. Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2511.13494"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "f5ff8c1d24505b030055a23e7729cd3e79dbb18e3f7ad060b158505906ffc244",
      "title": "Refined Bayesian Optimization for Efficient Beam Alignment in Intelligent Indoor Wireless Environments",
      "url": "https://arxiv.org/abs/2512.00036",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2512.00036v2 Announce Type: replace-cross Abstract: Future intelligent indoor wireless environments require fast and reliable beam alignment to sustain high-throughput links under mobility and blockage. Exhaustive beam training achieves optimal performance but is prohibitively costly. In indoor settings, dense scatterers and transceiver hardware imperfections introduce multipath and sidelobe leakage, producing measurable power across multiple angles and reducing the effectiveness of outdoor-oriented alignment algorithms. This paper presents a Refined Bayesian Optimization (R-BO) framework that exploits the inherent structure of mmWave transceiver patterns, where received power gradually increases as the transmit and receive beams converge toward the optimum. R-BO integrates a Gaussian Process (GP) surrogate with a Matern kernel and an Expected Improvement (EI) acquisition function, followed by a localized refinement around the predicted optimum. The GP hyperparameters are re-optimized online to adapt to irregular variations in the measured angular power field caused by reflections and sidelobe leakage. Experiments across 43 receiver positions in an indoor laboratory demonstrate 97.7% beam-alignment accuracy within 10 degrees, less than 0.3 dB average loss, and an 88% reduction in probing overhead compared to exhaustive search. These results establish R-BO as an efficient and adaptive beam-alignment solution for real-time intelligent indoor wireless environments.",
      "tags": [
        "papers",
        "safety",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2512.00036"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "1455cda98c9a6c292c6559e08f460ea19f1c404456b31085806d3a57f4ceadf2",
      "title": "Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective",
      "url": "https://arxiv.org/abs/2601.11616",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2601.11616v2 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) architectures are widely used for efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly understood. We study MoEs through a geometric lens, interpreting routing as soft partitioning into overlapping expert-local charts. We introduce a Dual Jacobian-PCA spectral probe that analyzes local function geometry via Jacobian singular value spectra and representation geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE setting with exact Jacobian computation, we compare dense, Top-k, and fully soft routing under matched capacity. Across random seeds, MoE routing consistently reduces local sensitivity: expert-local Jacobians show smaller leading singular values and faster spectral decay than dense baselines. Weighted PCA reveals that expert-local representations distribute variance across more principal directions, indicating higher effective rank. We further observe low alignment among expert Jacobians, suggesting decomposition into low-overlap expert-specific transformations. Routing sharpness modulates these effects: Top-k routing yields more concentrated, lower-rank expert structure, while fully soft routing produces broader, higher-rank representations. Experiments on a 3-layer transformer with WikiText confirm curvature reduction on natural language and show lower cross-expert alignment for Top-k routing. These findings support interpreting MoEs as soft partitionings of function space that flatten local curvature while redistributing representation variance, yielding testable predictions for expert scaling, hallucination reduction, and ensemble diversity.",
      "tags": [
        "papers",
        "safety"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.11616"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b4030b7c6b5cdb35f141ff8db938d1cc8c3e2ea6e7c36954a32ea815c4961e25",
      "title": "StableQAT: Stable Quantization-Aware Training at Ultra-Low Bitwidths",
      "url": "https://arxiv.org/abs/2601.19320",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2601.19320v2 Announce Type: replace-cross Abstract: Quantization-aware training (QAT) is essential for deploying large models under strict memory and latency constraints, yet achieving stable and robust optimization at ultra-low bitwidths remains challenging. Common approaches based on the straight-through estimator (STE) or soft quantizers often suffer from gradient mismatch, instability, or high computational overhead. As such, we propose StableQAT, a unified and efficient QAT framework that stabilizes training in ultra low-bit settings via a novel, lightweight, and theoretically grounded surrogate for backpropagation derived from a discrete Fourier analysis of the rounding operator. StableQAT strictly generalizes STE as the latter arises as a special case of our more expressive surrogate family, yielding smooth, bounded, and inexpensive gradients that improve QAT training performance and stability across various hyperparameter choices. In experiments, StableQAT exhibits stable and efficient QAT at 2-4 bit regimes, demonstrating improved training stability, robustness, and superior performance with negligible training overhead against standard QAT techniques. Our code is available at https://github.com/microsoft/StableQAT.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2601.19320"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "cd9c3640c6a179111c98d5c276a7e3679e6bc0ad072deedfcdc6bebca4199f01",
      "title": "Cardinality-Preserving Attention Channels for Graph Transformers in Molecular Property Prediction",
      "url": "https://arxiv.org/abs/2602.02201",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.02201v5 Announce Type: replace-cross Abstract: Molecular property prediction is crucial for drug discovery when labeled data are scarce. This work presents CardinalGraphFormer, a graph transformer augmented with a query-conditioned cardinality-preserving attention (CPA) channel that retains dynamic support-size signals complementary to static centrality embeddings. The approach combines structured sparse attention with Graphormer-inspired biases (shortest-path distance, centrality, direct-bond features) and unified dual-objective self-supervised pretraining (masked reconstruction and contrastive alignment of augmented views). Evaluation on 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET demonstrates consistent improvements over protocol-matched baselines under matched pretraining, optimization, and hyperparameter tuning. Rigorous ablations confirm CPA's contributions and rule out simple size shortcuts. Code and reproducibility artifacts are provided.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "training"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.02201"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "fc247a1ec3944c7db982cbc40fa920f44441c2ef4496c44cb247a01982d9900b",
      "title": "Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?",
      "url": "https://arxiv.org/abs/2602.05023",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.05023v2 Announce Type: replace-cross Abstract: Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.",
      "tags": [
        "papers",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.05023"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "c6bd3aa54cc4aa7cc472d51965cb58eb6cc1e027218e2a08384f06225abdaf8b",
      "title": "Protean Compiler: An Agile Framework to Drive Fine-grain Phase Ordering",
      "url": "https://arxiv.org/abs/2602.06142",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.06142v2 Announce Type: replace-cross Abstract: The phase ordering problem has been a long-standing challenge since the late 1970s, yet it remains an open problem due to having a vast optimization space and an unbounded nature, making it an open-ended problem without a finite solution, one can limit the scope by reducing the number and the length of optimizations. Traditionally, such locally optimized decisions are made by hand-coded algorithms tuned for a small number of benchmarks, often requiring significant effort to be retuned when the benchmark suite changes. In the past 20 years, Machine Learning has been employed to construct performance models to improve the selection and ordering of compiler optimizations, however, the approaches are not baked into the compiler seamlessly and never materialized to be leveraged at a fine-grained scope of code segments. This paper presents Protean Compiler: An agile framework to enable LLVM with built-in phase-ordering capabilities at a fine-grained scope. The framework also comprises a complete library of more than 140 handcrafted static feature collection methods at varying scopes, and the experimental results showcase speedup gains of up to 4.1% on average and up to 15.7% on select Cbench applications wrt LLVM's O3 by just incurring a few extra seconds of build time on Cbench. Additionally, Protean compiler allows for an easy integration with third-party ML frameworks and other Large Language Models, and this two-step optimization shows a gain of 10.1% and 8.5% speedup wrt O3 on Cbench's Susan and Jpeg applications. Protean compiler is seamlessly integrated into LLVM and can be used as a new, enhanced, full-fledged compiler. We plan to release the project to the open-source community in the near future.",
      "tags": [
        "papers",
        "eval",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.06142"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "b7a1de749f9c6903ec07aa3f1898ed2b49747e10838ee3517fa9fa4c51f949bc",
      "title": "Vision and Language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning",
      "url": "https://arxiv.org/abs/2602.07680",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.07680v2 Announce Type: replace-cross Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.",
      "tags": [
        "papers",
        "reasoning",
        "safety",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.07680"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "dc7f44b0f3f15679fc5e0b2cb4b968a467a085a9d4470525fe1457d55d09b043",
      "title": "When Models Examine Themselves: Vocabulary-Activation Correspondence in Self-Referential Processing",
      "url": "https://arxiv.org/abs/2602.11358",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.11358v2 Announce Type: replace-cross Abstract: Large language models produce rich introspective language when prompted for self-examination, but whether this language reflects internal computation or sophisticated confabulation has remained unclear. We show that self-referential vocabulary tracks concurrent activation dynamics, and that this correspondence is specific to self-referential processing. We introduce the Pull Methodology, a protocol that elicits extended self-examination through format engineering, and use it to identify a direction in activation space that distinguishes self-referential from descriptive processing in Llama 3.1. The direction is orthogonal to the known refusal direction, localised at 6.25% of model depth, and causally influences introspective output when used for steering. When models produce \"loop\" vocabulary, their activations exhibit higher autocorrelation (r = 0.44, p = 0.002); when they produce \"shimmer\" vocabulary under steering, activation variability increases (r = 0.36, p = 0.002). Critically, the same vocabulary in non-self-referential contexts shows no activation correspondence despite nine-fold higher frequency. Qwen 2.5-32B, with no shared training, independently develops different introspective vocabulary tracking different activation metrics, all absent in descriptive controls. The findings indicate that self-report in transformer models can, under appropriate conditions, reliably track internal computational states.",
      "tags": [
        "papers",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.11358"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "ba5a771e4af873e72f56c0bf402d701d3d3fdd73d25faf884020000768557ceb",
      "title": "VIRENA: Virtual Arena for Research, Education, and Democratic Innovation",
      "url": "https://arxiv.org/abs/2602.12207",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.12207v2 Announce Type: replace-cross Abstract: Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA's no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.12207"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "21ac722d982b3685a6663f0ce3a02b3d786e50e8e6d4f0a12d8234c00d81c502",
      "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
      "url": "https://arxiv.org/abs/2602.12281",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.12281v2 Announce Type: replace-cross Abstract: The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.\" We first characterize the test-time scaling laws for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce CoVer-VLA, a hierarchical test-time verification pipeline using the trained verifier. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses the verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer-VLA achieves 14% gains in task progress and 9% in success rate.",
      "tags": [
        "papers",
        "eval",
        "safety",
        "policy",
        "compute",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.12281"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "a151af76c724ff6cd19ab92cb7981356026370079aa04079de10783188e094f2",
      "title": "Knowledge-Based Design Requirements for Generative Social Robots in Higher Education",
      "url": "https://arxiv.org/abs/2602.12873",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.12873v2 Announce Type: replace-cross Abstract: Generative social robots (GSRs) powered by large language models enable adaptive, conversational tutoring but also introduce risks such as hallucinations, overreliance, and privacy violations. Existing frameworks for educational technologies and responsible AI primarily define desired behaviors, yet they rarely specify the knowledge prerequisites that enable generative systems to express these behaviors reliably. To address this gap, we adopt a knowledge-based design perspective and investigate what information tutoring-oriented GSRs require to function responsibly and effectively in higher education. Based on twelve semi-structured interviews with university students and lecturers, we identify twelve design requirements across three knowledge types: self-knowledge (assertive, conscientious and friendly personality with customizable role), user-knowledge (personalized information about student learning goals, learning progress, motivation type, emotional state and background), and context-knowledge (learning materials, educational strategies, course-related information, and physical learning environment). By identifying these knowledge requirements, this work provides a structured foundation for the design of tutoring GSRs and future evaluations, aligning generative system capabilities with pedagogical and ethical expectations.",
      "tags": [
        "papers",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.12873"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "db011f654f68befc8a2fd05bb2fe7fa519866bfb9604ee903befa35817008729",
      "title": "Semantic Chunking and the Entropy of Natural Language",
      "url": "https://arxiv.org/abs/2602.13194",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.13194v2 Announce Type: replace-cross Abstract: The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.",
      "tags": [
        "papers",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.13194"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "980551b7fb3f2349b501e2c4eaba9611e874632e84cdefe92455122e151f496e",
      "title": "Learning to Select Like Humans: Explainable Active Learning for Medical Imaging",
      "url": "https://arxiv.org/abs/2602.13308",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.13308v2 Announce Type: replace-cross Abstract: Medical image analysis requires substantial labeled data for model training, yet expert annotation is expensive and time-consuming. Active learning (AL) addresses this challenge by strategically selecting the most informative samples for the annotation purpose, but traditional methods solely rely on predictive uncertainty while ignoring whether models learn from clinically meaningful features a critical requirement for clinical deployment. We propose an explainability-guided active learning framework that integrates spatial attention alignment into a sample acquisition process. Our approach advocates for a dual-criterion selection strategy combining: (i) classification uncertainty to identify informative examples, and (ii) attention misalignment with radiologist-defined regions-of-interest (ROIs) to target samples where the model focuses on incorrect features. By measuring misalignment between Grad-CAM attention maps and expert annotations using Dice similarity, our acquisition function judiciously identifies samples that enhance both predictive performance and spatial interpretability. We evaluate the framework using three expert-annotated medical imaging datasets, namely, BraTS (MRI brain tumors), VinDr-CXR (chest X-rays), and SIIM-COVID-19 (chest X-rays). Using only 570 strategically selected samples, our explainability-guided approach consistently outperforms random sampling across all the datasets, achieving 77.22% accuracy on BraTS, 52.37% on VinDr-CXR, and 52.66% on SIIM-COVID. Grad-CAM visualizations confirm that the models trained by our dual-criterion selection focus on diagnostically relevant regions, demonstrating that incorporating explanation guidance into sample acquisition yields superior data efficiency while maintaining clinical interpretability.",
      "tags": [
        "papers",
        "safety",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.13308"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "dfb3f771b68016f6734de1112d9796771c27faaa0ac28678810a20105837d4d0",
      "title": "Arming Data Agents with Tribal Knowledge",
      "url": "https://arxiv.org/abs/2602.13521",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.13521v2 Announce Type: replace-cross Abstract: Natural language to SQL (NL2SQL) translation enables non-expert users to query relational databases through natural language. Recently, NL2SQL agents, powered by the reasoning capabilities of Large Language Models (LLMs), have significantly advanced NL2SQL translation. Nonetheless, NL2SQL agents still make mistakes when faced with large-scale real-world databases because they lack knowledge of how to correctly leverage the underlying data (e.g., knowledge about the intent of each column) and form misconceptions about the data when querying it, leading to errors. Prior work has studied generating facts about the database to provide more context to NL2SQL agents, but such approaches simply restate database contents without addressing the agent's misconceptions. In this paper, we propose Tk-Boost, a bolt-on framework for augmenting any NL2SQL agent with tribal knowledge: knowledge that corrects the agent's misconceptions in querying the database accumulated through experience using the database. To accumulate experience, Tk-Boost first asks the NL2SQL agent to answer a few queries on the database, identifies the agent's misconceptions by analyzing its mistakes on the database, and generates tribal knowledge to address them. To enable accurate retrieval, Tk-Boost indexes this knowledge with applicability conditions that specify the query features for which the knowledge is useful. When answering new queries, Tk-Boost uses this knowledge to provide feedback to the NL2SQL agent, resolving the agent's misconceptions during SQL generation, and thus improving the agent's accuracy. Extensive experiments across the BIRD and Spider 2.0 benchmarks with various NL2SQL agents shows Tk-Boost improves NL2SQL agents accuracy by up to 16.9% on Spider 2.0 and 13.7% on BIRD",
      "tags": [
        "papers",
        "agents",
        "reasoning",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.13521"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "2a49836d67837d94b9f970f1ab7ac46a563df018cb0b7557faaf9d1f04ace90d",
      "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
      "url": "https://arxiv.org/abs/2602.14299",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.14299v2 Announce Type: replace-cross Abstract: As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while the global average of semantic contents stabilizes rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop a stable structure and consensus due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.",
      "tags": [
        "papers",
        "agents",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.14299"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "05d0c2aaf10de0be002c2cf8113367ff139430663169e08aa87300d36614af0d",
      "title": "A Geometric Analysis of Small-sized Language Model Hallucinations",
      "url": "https://arxiv.org/abs/2602.14778",
      "sourceId": "arxiv_cs_ai",
      "sourceName": "arXiv — cs.AI",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.14778v2 Announce Type: replace-cross Abstract: Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings. This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%. Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.",
      "tags": [
        "papers",
        "agents",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.14778"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.AI"
        }
      ]
    },
    {
      "id": "32eae121d5e7019f458ac644563f7f8525171b468e8d2cc3598ff4825475ea4f",
      "title": "The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts",
      "url": "https://arxiv.org/abs/2602.15843",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15843v1 Announce Type: new Abstract: In \"Compress or Route?\" (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r >= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the \"perplexity paradox\" mechanism unvalidated, and provided no adaptive algorithm. This paper addresses all three gaps. First, we validate across six code benchmarks (HumanEval, MBPP, HumanEval+, MultiPL-E) and four reasoning benchmarks (GSM8K, MATH, ARC-Challenge, MMLU-STEM), confirming the compression threshold generalizes across languages and difficulties. Second, we conduct the first per-token perplexity analysis (n=723 tokens), revealing a \"perplexity paradox\": code syntax tokens are preserved (high perplexity) while numerical values in math problems are pruned despite being task-critical (low perplexity). Signature injection recovers +34 percentage points in pass rate (5.3% to 39.3%; Cohen's h=0.890). Third, we propose TAAC (Task-Aware Adaptive Compression), achieving 22% cost reduction with 96% quality preservation, outperforming fixed-ratio compression by 7%. MBPP validation (n=1,800 trials) confirms systematic variation: 3.6% at r=0.3 to 54.6% at r=1.0.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15843"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8e1aa0cc8d4f3edac2578736aeea522eae4a240f21349544fa84ad22587097b3",
      "title": "Language Model Representations for Efficient Few-Shot Tabular Classification",
      "url": "https://arxiv.org/abs/2602.15844",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15844v1 Announce Type: new Abstract: The Web is a rich source of structured data in the form of tables, from product catalogs and knowledge bases to scientific datasets. However, the heterogeneity of the structure and semantics of these tables makes it challenging to build a unified method that can effectively leverage the information they contain. Meanwhile, Large language models (LLMs) are becoming an increasingly integral component of web infrastructure for tasks like semantic search. This raises a crucial question: can we leverage these already-deployed LLMs to classify structured data in web-native tables (e.g., product catalogs, knowledge base exports, scientific data portals), avoiding the need for specialized models or extensive retraining? This work investigates a lightweight paradigm, $\\textbf{Ta}$ble $\\textbf{R}$epresentation with $\\textbf{L}$anguage Model~($\\textbf{TaRL}$), for few-shot tabular classification that directly utilizes semantic embeddings of individual table rows. We first show that naive application of these embeddings underperforms compared to specialized tabular models. We then demonstrate that their potentials can be unlocked with two key techniques: removing the common component from all embeddings and calibrating the softmax temperature. We show that a simple meta-learner, trained on handcrafted features, can learn to predict an appropriate temperature. This approach achieves performance comparable to state-of-the-art models in low-data regimes ($k \\leq 32$) of semantically-rich tables. Our findings demonstrate the viability of reusing existing LLM infrastructure for efficient semantics-driven pathway to reuse existing LLM infrastructure for Web table understanding.",
      "tags": [
        "papers",
        "language",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15844"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b0754c52b5862c2a2d07b2f56b6e4eebcae00e329aa90526b3c4280ace79925f",
      "title": "KD4MT: A Survey of Knowledge Distillation for Machine Translation",
      "url": "https://arxiv.org/abs/2602.15845",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15845v1 Announce Type: new Abstract: Knowledge Distillation (KD) as a research area has gained a lot of traction in recent years as a compression tool to address challenges related to ever-larger models in NLP. Remarkably, Machine Translation (MT) offers a much more nuanced take on this narrative: in MT, KD also functions as a general-purpose knowledge transfer mechanism that shapes supervision and translation quality as well as efficiency. This survey synthesizes KD for MT (KD4MT) across 105 papers (through October 1, 2025). We begin by introducing both MT and KD for non-experts, followed by an overview of the standard KD approaches relevant to MT applications. Subsequently, we categorize advances in the KD4MT literature based on (i) their methodological contributions and (ii) their practical applications. Our qualitative and quantitative analyses identify common trends in the field and highlight key research gaps as well as the absence of unified evaluation practice for KD methods in MT. We further provide practical guidelines for selecting a KD method in concrete settings and highlight potential risks associated with the application of KD to MT such as increased hallucination and bias amplification. Finally, we discuss the role of LLMs in re-shaping the KD4MT field. To support further research, we complement our survey with a publicly available database summarizing the main characteristics of the surveyed KD methods and a glossary of key terms.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15845"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "a603c56429a09e772c1b77f4914717bee116ebfd532d61061af42d49cca1372f",
      "title": "Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs",
      "url": "https://arxiv.org/abs/2602.15846",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15846v1 Announce Type: new Abstract: Decoder-only large language models achieve strong broad performance but are brittle to minor grammatical perturbations, undermining reliability for downstream reasoning. However, directly injecting explicit syntactic structure into an existing checkpoint can interfere with its pretrained competence. We introduce a checkpoint-compatible gated tree cross-attention (GTCA) branch that reads precomputed constituency chunk memory while leaving backbone architecture unchanged. Our design uses a token update mask and staged training to control the scope and timing of structural updates. Across benchmarks and Transformer backbones, GTCA strengthens syntactic robustness beyond continued-training baselines without compromising Multiple-Choice QA performance or commonsense reasoning, providing a practical checkpoint-compatible route to more syntax-robust decoder-only LLMs.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "compute",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15846"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "53434bb53a673249823443c86f9add05a07f4a9e5156d43cb1f379a31eddd1b2",
      "title": "Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models",
      "url": "https://arxiv.org/abs/2602.15847",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15847v1 Announce Type: new Abstract: Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by analysing the geometric relationships between Big Five personality steering directions. We study steering vectors extracted from two model families (LLaMA-3-8B and Mistral-8B) and apply a range of geometric conditioning schemes, from unconstrained directions to soft and hard orthonormalisation. Our results show that personality steering directions exhibit substantial geometric dependence: steering one trait consistently induces changes in others, even when linear overlap is explicitly removed. While hard orthonormalisation enforces geometric independence, it does not eliminate cross-trait behavioural effects and can reduce steering strength. These findings suggest that personality traits in LLMs occupy a slightly coupled subspace, limiting fully independent trait control.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15847"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "4f6a0df39c4f387bca450624d0c94954431dd4acc8682d92329bc3f2d14abd7a",
      "title": "Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling",
      "url": "https://arxiv.org/abs/2602.15848",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15848v1 Announce Type: new Abstract: This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against the gold-standard IPIP-50 questionnaire, while also measuring user-perceived accuracy. Results indicate moderate convergent validity (r=0.38-0.58), with Conscientiousness, Openness, and Neuroticism scores statistically equivalent between methods. Agreeableness and Extraversion showed significant differences, suggesting trait-specific calibration is needed. Notably, participants rated LLM-generated profiles as equally accurate as traditional questionnaire results. These findings suggest conversational AI offers a promising new approach to traditional psychometrics.",
      "tags": [
        "papers",
        "language",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15848"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b181f4b7a57fceaab62eb111c27ae24410bd95f0743ab3da7a0ff9472adbae91",
      "title": "Preference Optimization for Review Question Generation Improves Writing Quality",
      "url": "https://arxiv.org/abs/2602.15849",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15849v1 Announce Type: new Abstract: Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "policy",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15849"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "8422e21b00107b06c35e6b3db984c47e792e36c166cfccbcbc4a831c14c92115",
      "title": "Large Language Models for Assisting American College Applications",
      "url": "https://arxiv.org/abs/2602.15850",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15850v1 Announce Type: new Abstract: American college applications require students to navigate fragmented admissions policies, repetitive and conditional forms, and ambiguous questions that often demand cross-referencing multiple sources. We present EZCollegeApp, a large language model (LLM)-powered system that assists high-school students by structuring application forms, grounding suggested answers in authoritative admissions documents, and maintaining full human control over final responses. The system introduces a mapping-first paradigm that separates form understanding from answer generation, enabling consistent reasoning across heterogeneous application portals. EZCollegeApp integrates document ingestion from official admissions websites, retrieval-augmented question answering, and a human-in-the-loop chatbot interface that presents suggestions alongside application fields without automated submission. We describe the system architecture, data pipeline, internal representations, security and privacy measures, and evaluation through automated testing and human quality assessment. Our source code is released on GitHub (https://github.com/ezcollegeapp-public/ezcollegeapp-public) to facilitate the broader impact of this work.",
      "tags": [
        "papers",
        "language",
        "reasoning",
        "eval",
        "model",
        "release"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15850"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "2e39c04837e7da29c39e7c379c7e03d4968f8b82204ff0029a52c1c910497c3b",
      "title": "Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey",
      "url": "https://arxiv.org/abs/2602.15851",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15851v1 Announce Type: new Abstract: Applications of narrative theories using large language models (LLMs) deliver promising use-cases in automatic story generation and understanding tasks. Our survey examines how natural language processing (NLP) research engages with fields of narrative studies, and proposes a taxonomy for ongoing efforts that reflect established distinctions in narratology. We discover patterns in the following: narrative datasets and tasks, narrative theories and NLP pipeline and methodological trends in prompting and fine-tuning. We highlight how LLMs enable easy connections of NLP pipelines with abstract narrative concepts and opportunities for interdisciplinary collaboration. Challenges remain in attempts to work towards any unified definition or benchmark of narrative related tasks, making model comparison difficult. For future directions, instead of the pursuit of a single, generalised benchmark for 'narrative quality', we believe that progress benefits more from efforts that focus on the following: defining and improving theory-based metrics for individual narrative attributes to incrementally improve model performance; conducting large-scale, theory-driven literary/social/cultural analysis; and creating experiments where outputs can be used to validate or refine narrative theories. This work provides a contextual foundation for more systematic and theoretically informed narrative research in NLP by providing an overview to ongoing research efforts and the broader narrative studies landscape.",
      "tags": [
        "papers",
        "language",
        "eval",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15851"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "5c722698a83e3a9cfaa533235ed16ab6b6df7ee4d4d5c3dc555aa7d458548ffb",
      "title": "Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints",
      "url": "https://arxiv.org/abs/2602.15852",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15852v1 Announce Type: new Abstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, where documentation artifacts encode future clinical decisions and inflate apparent predictive performance. Such behavior poses substantial risks for real-world deployment, where overconfident or temporally invalid predictions can disrupt clinical workflows and compromise patient safety. This study focuses on system-level design choices required to build safe and deployable clinical NLP under temporal leakage constraints. We present a lightweight auditing pipeline that integrates interpretability into the model development process to identify and suppress leakage-prone signals prior to final training. Using next-day discharge prediction after elective spine surgery as a case study, we evaluate how auditing affects predictive behavior, calibration, and safety-relevant trade-offs. Results show that audited models exhibit more conservative and better-calibrated probability estimates, with reduced reliance on discharge-related lexical cues. These findings emphasize that deployment-ready clinical NLP systems should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance.",
      "tags": [
        "papers",
        "language",
        "safety",
        "training",
        "model"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15852"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "e872e53bb7d7e8e300e1d27443ac8a6c33d1e697b947ed86bd5c3694018faf50",
      "title": "A Lightweight Explainable Guardrail for Prompt Safety",
      "url": "https://arxiv.org/abs/2602.15853",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15853v1 Announce Type: new Abstract: We propose a lightweight explainable guardrail (LEG) method for the classification of unsafe prompts. LEG uses a multi-task learning architecture to jointly learn a prompt classifier and an explanation classifier, where the latter labels prompt words that explain the safe/unsafe overall decision. LEG is trained using synthetic data for explainability, which is generated using a novel strategy that counteracts the confirmation biases of LLMs. Lastly, LEG's training process uses a novel loss that captures global explanation signals and combines cross-entropy and focal losses with uncertainty-based weighting. LEG obtains equivalent or better performance than the state-of-the-art for both prompt classification and explainability, both in-domain and out-of-domain on three datasets, despite the fact that its model size is considerably smaller than current approaches. If accepted, we will release all models and the annotated dataset publicly.",
      "tags": [
        "papers",
        "language",
        "safety",
        "training",
        "model",
        "release",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15853"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    },
    {
      "id": "b9c1a9ff2194987c01ba7a2baefb98823ceb3afdc6131bc776eb4cdf835d9382",
      "title": "Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization",
      "url": "https://arxiv.org/abs/2602.15854",
      "sourceId": "arxiv_cs_cl",
      "sourceName": "arXiv — cs.CL",
      "publishedAt": "2026-02-19T05:00:00.000Z",
      "summary": "arXiv:2602.15854v1 Announce Type: new Abstract: Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierarchical reinforcement learning framework that decouples strategy planning from response generation via an Expert Agent and a Customer Service Agent. The Expert Agent optimizes multi-turn goal preferences at the dialogue-trajectory level, while the Customer Service Agent generates responses strictly aligned with the selected strategy. We evaluate GOPO on public benchmarks and e-commerce customer service datasets, and introduce Task-focused Sequential Engagement (TSE), a sequence-level metric derived from real e-commerce interaction data. On the Mgshop dataset, GOPO improves TSE by 7.7% and 10.3% over PPO and Memento, with consistent gains in sequence-level reward and generation quality. Furthermore, a 14B model trained with GOPO achieves 2.7% and 1.5% higher TSE than Qwen-235B and GPT-5.2, respectively. Ablation studies confirm the Expert Agent's critical role in long-horizon optimization. GOPO demonstrates consistent improvements across other datasets as well. This work establishes a new paradigm for task-oriented dialogue systems in commercial scenarios, with code and datasets to be made public.",
      "tags": [
        "papers",
        "language",
        "agents",
        "eval",
        "training",
        "model",
        "data"
      ],
      "citations": [
        {
          "label": "Original",
          "url": "https://arxiv.org/abs/2602.15854"
        },
        {
          "label": "Feed",
          "url": "https://rss.arxiv.org/rss/cs.CL"
        }
      ]
    }
  ]
}
